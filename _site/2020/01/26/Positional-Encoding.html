<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<link rel="icon" href="/assets/images/logo.png">
    
<title>Positional Encoding in NLP | Space Moon</title>
    
 
    
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+KR:300">

<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
    
<link href="/assets/css/screen.css" rel="stylesheet">
    
<link href="/assets/css/main.css" rel="stylesheet">
    
</head>
    

    

<body class="layout-post">

    
<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">
    
    <div class="container pr-0">    
    
    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="Space Moon">
    </a>
    <!-- End Logo -->
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    
    <div class="collapse navbar-collapse" id="navbarMediumish">
       
        <!-- Begin Menu -->
        
            <ul class="navbar-nav ml-auto">
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/inmoonlight/"><i class="fab fa-github"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.linkedin.com/in/mjihyung/"><i class="fab fa-linkedin"></i></a>
                </li>
                
                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="https://twitter.com/inmoonlight_kr"><i class="fab fa-twitter"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="https://www.instagram.com/jihyung.moon/"><i class="fab fa-instagram"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="/feed.xml"><i class="fa fa-rss"></i></a>
                </li>
                
            </ul>		
  
        <!-- End Menu -->

    </div>
        
    </div>
</nav>
<!-- End Navigation
================================================== -->
    
<div class="site-content">   
<div class="container">
    
<!-- Site Title
================================================== -->
<!-- <div class="mainheading">
    <h1 class="sitetitle">Space Moon</h1>
    <p class="lead">
         개인적이지만 사소하지 않은 이야기를 담고 싶습니다
    </p>
</div> -->

    
    
<!-- Content
================================================== --> 
<div class="main-content">
    <!-- Begin Article
================================================== -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Positional Encoding in NLP</title>
  <meta name="description" content="Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다.보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다.이번 글에서는 Con..." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="" />
  <meta property="og:title" content="Positional Encoding in NLP"/>
  
  <meta property="og:description" content="Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다.보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다.이번 글에서는 Con..." />
  
  <meta property="og:image" content="https://inmoonlight.github.io/assets/images/positional_encoding.png" />
  <meta property="og:url" content="https://inmoonlight.github.io/2020/01/26/Positional-Encoding" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2020-01-26T17:00:00+09:00">

  <link rel="canonical" href="https://inmoonlight.github.io/2020/01/26/Positional-Encoding"/>
  <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
</head>

  <body itemscope itemtype="http://schema.org/Article">
	<!-- header start -->


<!-- header end -->

	<main class="content" role="main">
	  <article class="post">
			<script type="text/javascript" async
				src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
			</script>
		
		<div class="article-image">
		  <div class="post-image-image" style="background-image: url(/assets/images/positional_encoding.png)">
			Article Image
		  </div>
		  <div class="post-image-image2" style="background-image: url(/assets/images/positional_encoding.png)">
			Article Image
		  </div>
		  <div class="post-meta">
			<h1 class="post-title">Positional Encoding in NLP</h1>
			<div class="cf post-meta-text">
        <time datetime="26-01-2020">Jan 26, 2020</time>
        | <span class="reading-time" title="Estimated read time">
    

    
        11 mins
    
</span> read
			  <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
			</div>
			<!-- <div style="text-align:center">
			  <a href="#topofpage" class="topofpage"><i class="fa fa-angle-down"></i></a>
			</div> -->
		  </div>
		</div>
		
		<section class="post-content-mediator">
		  <p>Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다.
보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다.
이번 글에서는 Convolutional Neural Network (CNN), End-to-End Memory Network (MemN2N), Transformer에서 sentence embedding을 위해 사용된 positional encoding에 대해 소개하려고 한다.</p>

<div class="breaker"></div>

<h3 id="table-of-contents">Table of Contents</h3>

<p><a href="#1-introduction">1. Introduction</a> <br />
<a href="#2-learned-positional-embeddings">2. Learned Positional Embeddings</a> <br />
<a href="#3-positional-encoding">3. Positional Encoding</a> <br />
    <a href="#30-the-intuition">3.0 Intuition</a> <br />
    <a href="#31-in-memn2n">3.1 In MemN2N</a> <br />
    <a href="#32-in-transformer">3.2 In Transformer</a> <br />
<a href="#4-conclusions">4. Conclusions</a><br />
<a href="#references">References</a></p>

<div class="breaker"></div>

<h2 id="1-introduction"><a href="#table-of-contents">1. Introduction</a></h2>

<p>일반적으로 NLP 모델은 각 문장을 구성하는 token을 one-hot vector가 아닌 distributed vector로 표현한다.
그 이유는 distributed representation이 1) 비슷한 의미지만 다른 lexical form을 가진 token을 더 잘 표현할 수 있기 때문이고, 2) embedding dimension을 감소시킬 수 있기 때문이다.</p>

<p>문장의 embedding은 문장을 이루는 각 token의 embedding을 조합하는 방식으로 얻어진다.
이 때 position에 대한 정보가 없다면 모델은 <code class="highlighter-rouge">handful of chocolate</code>과 <code class="highlighter-rouge">chocolate of handful</code>을 같은 의미로 인식하게 된다.
RNN은 모델 구조 자체에 time information이 녹아져 있다.
그래서 <code class="highlighter-rouge">handful --&gt; of --&gt; chocolate</code> 의 순서가 담긴 sentence embedding을 자연스럽게 얻을 수 있다.
반면 CNN이나 Attention 기반의 Transformer는 순서에 대한 정보를 강제해야 하고, 이 때 positional encoding이 사용된다.</p>

<p>Positional encoding (PE) 은 token embedding vector에 곱해지는 정보로, sentence에서 해당 token이 어디에 위치해 있는지를 나타낸다.
J개의 token <script type="math/tex">t_j \in \mathbb{R}^d</script> 으로 구성된 sentence <script type="math/tex">s = [t_1, t_2, ..., t_J]</script> 가 있다고 하자. 
PE <script type="math/tex">\in \mathbb{R}^{J \times d}</script> 의 row <script type="math/tex">j</script> 마다 다른 값을 가지도록 하여 문장 맨 처음의 <code class="highlighter-rouge">handful</code>과 맨 뒤의 <code class="highlighter-rouge">handful</code>을 다르게 인식하도록 한다.</p>

<p>PE를 구성하는 방식에는 크게 두 종류가 있다.
하나는 학습기반, 다른 하나는 position과 dimension을 입력으로 한 함수를 이용하는 방법이다.</p>

<div class="breaker"></div>

<h2 id="2-learned-positional-embeddings"><a href="#table-of-contents">2. Learned Positional Embeddings</a></h2>

<p>학습기반의 PE를 구성하는 방식은 Convolutional Sequence to Sequence Learning (ConvS2S)<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>에서 사용되었다.
평균 0, 표준편차 0.1을 따르는 normal distribution으로 initialize되고 학습을 통해 position 정보를 배우길 기대한다.</p>

<p>PE를 encoder와 decoder 모두에 사용한 경우, encoder에만 사용한 경우, decoder에만 사용한 경우, 아예 사용하지 않은 경우로 나누어 번역 task에 실험해보았을 때의 결과는 다음과 같다.</p>

<div style="text-align:center">
<img class="image" src="/assets/images/learned_pe_table.png" width="70%" />
</div>
<p><br />
BLEU를 기준으로 분석해보면 encoder에서의 PE역할이 decoder보다 조금 더 중요하다.
PE를 아예 쓰지 않을 때의 점수가 가장 낮지만 점수 차이를 생각해보면 모델 성능에는 크게 영향을 미치지 않는다고 해석해 볼 수 있다.</p>

<p>학습 기반이므로 학습 시 다루지 않았던 길이의 문장이 입력으로 들어온 경우, 외삽이 불가능하다는 단점이 있다.<sup id="fnref:6"><a href="#fn:6" class="footnote">2</a></sup></p>

<div class="breaker"></div>

<h2 id="3-function-based-positional-encoding"><a href="#table-of-contents">3. Function-based Positional Encoding</a><sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup></h2>

<p>함수 기반의 PE는 문장에서 몇 번째에 위치한 토큰인지, 토큰의 embedding dimension이 무엇인지를 정해주면 값이 정해진다.
이 때, 다른 위치의 정보가 같은 값으로 mapping되지 않아야 한다.
어떻게 구현할 수 있을까?</p>

<h3 id="30-the-intuition"><a href="#table-of-contents">3.0 The Intuition</a><sup id="fnref:3"><a href="#fn:3" class="footnote">4</a></sup></h3>

<p>0부터 15까지의 숫자를 2진법으로 나타내보자.</p>

<div style="text-align:center">
<img class="image" src="/assets/images/PE_intuition.png" width="60%" />
</div>
<p><br />
다른 색으로 구분지어 표현한 2진수의 자리수마다 다른 주기를 가지는 것을 볼 수 있다.
붉은색은 주기가 1이고, 노란색은 주기가 2, 초록색은 주기가 4, 파란색은 주기가 8이다.</p>

<p>위 예시에서의 자리수를 embedding dimension이라고 생각해보면 PE에도 같은 원리를 확장시켜볼 수 있다.</p>

<h3 id="31-in-memn2n"><a href="#table-of-contents">3.1 In MemN2N</a><sup id="fnref:4"><a href="#fn:4" class="footnote">5</a></sup></h3>

<p>End-to-End Memory Network (MemN2N)<sup id="fnref:4:1"><a href="#fn:4" class="footnote">5</a></sup>에서는 아래의 함수를 사용했다.</p>

<script type="math/tex; mode=display">PE_{k j}=(1- \frac{j}{J})-\frac{k}{d}(1- \frac{2j}{J})</script>

<script type="math/tex; mode=display">j \in {1, ..., J}</script>

<script type="math/tex; mode=display">k \in {1, ..., D}</script>

<p>임의의 문장 <code class="highlighter-rouge">The same representation is used for questions, memory inputs and memory outputs.</code>에 적용되는 PE를 시각화해보면 다음과 같다.<sup id="fnref:5"><a href="#fn:5" class="footnote">6</a></sup></p>

<div style="text-align:center">
<img class="image" src="/assets/images/PE_example_1.png" width="90%" />
</div>
<p><br /></p>

<p>여기서는 dimension에 관계없이 같은 주기를 가지지만 시작값이 전부 다르다.
결과적으로는 position마다 다른 vector를 곱하게 되어 position 정보를 전달할 수 있다.</p>

<p>다른 문장 길이를 가지는 경우에 대해서 적용해보면 어떨까?
이번에는 <code class="highlighter-rouge">We therefore propose a second representation that encodes the position of words within the sentence.</code>에 대해 시각해보았다.<sup id="fnref:5:1"><a href="#fn:5" class="footnote">6</a></sup></p>

<div style="text-align:center">
<img class="image" src="/assets/images/PE_example_2.png" width="90%" />
</div>
<p><br /></p>

<p>position이 늘어난만큼 position encoding 값의 변화도가 줄었다.
J는 문장마다 달라지므로 첫번째, 두번째의 절대적인 위치보다는 각 순서를 구분짓기 위한 목적에 치중하였다.</p>

<p>ConvS2S에서와는 달리 MemN2N에서 PE의 효과는 꽤나 컸던 것으로 보인다.</p>
<div style="text-align:center">
<img class="image" src="/assets/images/MemN2N_PE.png" width="90%" />
</div>
<p><br /></p>

<h3 id="32-in-transformer"><a href="#table-of-contents">3.2 In Transformer</a><sup id="fnref:6:1"><a href="#fn:6" class="footnote">2</a></sup></h3>

<p>Attention is all you need<sup id="fnref:6:2"><a href="#fn:6" class="footnote">2</a></sup>에서 사용된 PE는 <strong>주기</strong>함수로 유명한 sin 함수와 cos 함수를 기반으로 한다. (a.k.a, sinusoidal functions)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} 
P E_{(\text {pos, 2k} )} &=\sin \left(\text {pos} / 10000^{2 k / d}\right) \\ P E_{(\text {pos,2k+1})} &=\cos \left(\text {pos} / 10000^{2 k / d}\right) 
\end{aligned} %]]></script>

<p>잠시 고등학교 때 배운 수학을 떠올려보자. 
<script type="math/tex">sin(ax + b)</script> 의 주기는 <script type="math/tex">2\pi / |a|</script> 이다.
따라서 PE의 특정 position vector 값의 주기는 <script type="math/tex">2\pi \cdot 10000^{2 k / d}</script> 와 같다.</p>

<p>MemN2N에서의 PE와는 달리, position vector의 주기가 vector의 dimension마다 변화한다.
전체 벡터 크기(<script type="math/tex">d</script>)가 128이라고 가정할 때, k가 작을수록 주기가 짧고 k가 클수록 주기도 길어진다. (아래 그림 참고)</p>

<div style="text-align:center">
<img class="image" src="/assets/images/positional_encoding.png" width="90%" />
<figcaption class="caption">Image credit: https://kazemnejad.com/blog/transformer_architecture_positional_encoding</figcaption>
</div>
<p><br /></p>

<p>왜 Transformer에서는 MemN2N과 다르게 sinusoidal 함수를 썼을까?
논문에서 그 이유를 짧게 기술하고 있다.</p>

<blockquote>
  <p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <script type="math/tex">k</script>, <script type="math/tex">P E_{pos+k}</script> can be represented as a linear function of <script type="math/tex">P E_{pos}</script>.</p>
</blockquote>

<p>sinusodial 함수의 특징을 이용해 첫번째, 두번째마다 같은 position 정보를 주면서도 <script type="math/tex">n + k</script> 번째 vector가 <script type="math/tex">n</script> 번째 vector와 관계가 있을 때 이를 학습할 수 있는 여지를 남겨주기 위함이다.
(참고로 이에 대한 수학적인 증명은 <a href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/">이 article</a>에 기술되어 있다.)</p>

<p>또한 PE vector 간의 distance는 대칭적이고 거리에 따라 일정한 비율로 감소한다.
Transformer의 self-attention 연산에서 빛을 발하는 특징이다.</p>

<div style="text-align:center">
<img class="image" src="/assets/images/PE_pros_1.png" width="60%" />
<figcaption class="caption">Image credit: https://kazemnejad.com/blog/transformer_architecture_positional_encoding</figcaption>
</div>
<p><br /></p>

<div class="breaker"></div>

<h2 id="4-conclusions"><a href="#table-of-contents">4. Conclusions</a></h2>

<p>PE는 크게 학습을 통해 정해질 수 있고 미리 지정한 함수로 정해질 수도 있다.
학습을 통한 방식은 학습시 보지 않았던 새로운 길이가 등장했을 때 외삽이 불가능하지만 함수 기반의 PE는 가능하다.
함수도 어떤 함수를 쓰느냐에 따라 종류가 구분되는데, 절대적인 위치에 따라 같은 값을 가지면서도 상대적 위치의 관계도 학습할 수 있는 sin과 cos 기반의 함수가 가장 좋은 방법이라고 생각된다.</p>

<div class="breaker"></div>

<h2 id="references"><a href="#table-of-contents">References</a></h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://arxiv.org/abs/1705.03122">https://arxiv.org/abs/1705.03122</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a> <a href="#fnref:6" class="reversefootnote">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:6:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:2">
      <p>제가 만든 용어이므로 공식적으로 사용하면 곤란할 수 있습니다 ㅎㅎ <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://arxiv.org/abs/1503.08895">https://arxiv.org/abs/1503.08895</a> <a href="#fnref:4" class="reversefootnote">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p><a href="https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb">https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb</a> <a href="#fnref:5" class="reversefootnote">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

		</section>
		<footer class="post-footer">
			<!-- <div class="col-md-8 p-5 align-self-center text-center"> -->
			<ul class="tag-box inline">
					  
					
						 
							<a href="/tag/machine-learning">Machine-learning </a>
						 
							<a href="/tag/nlp">Nlp </a>
						 
							<a href="/tag/technique">Technique </a>
						 
							<a href="/tag/머신러닝">머신러닝 </a>
						 
							<a href="/tag/자연어처리">자연어처리 </a>
						
					
					
			</ul>
		  <section class="share">
				
			  	
						<a class="icon-twitter" href="http://twitter.com/share?text=Positional+Encoding+in+NLP&amp;url=https://inmoonlight.github.io/2020/01/26/Positional-Encoding"
				  		onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
						<i class="fa fa-twitter"></i><span class="hidden">twitter</span>
						</a>
			 		
				
			  	
						<a class="icon-facebook" href="https://www.facebook.com/sharer.php?t=Positional+Encoding+in+NLP&amp;u=https://inmoonlight.github.io/2020/01/26/Positional-Encoding"
				  		onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
						<i class="fa fa-facebook"></i><span class="hidden">facebook</span>
						</a>
			 		
				
		  </section>
		</footer>
	  </article>
	</main>
	<script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
   })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-131297969-1', 'auto');
ga('send', 'pageview');

</script>



  </body>
</html>
<!-- End Article
================================================== -->

  

<!-- Begin Comments
================================================== -->

	<div class="container">
		<div id="comments" class="row justify-content-center mb-5">
			<div class="col-md-10">              
				<section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'inmoonlight'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
                
			</div>
		</div>
	</div>

<!--End Comments
================================================== -->
</div>

<!-- Bottom Alert Bar
================================================== -->
<!-- <div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt=""> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>     -->
    
</div><!-- /.container>
    
<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
          
             
              <a href="/tag/멘토링">멘토링 (2)</a>
             
              <a href="/tag/mentoring">Mentoring (2)</a>
             
              <a href="/tag/essay">Essay (3)</a>
             
              <a href="/tag/에세이">에세이 (3)</a>
             
              <a href="/tag/machine-learning">Machine-learning (8)</a>
             
              <a href="/tag/book">Book (4)</a>
             
              <a href="/tag/summary">Summary (3)</a>
             
              <a href="/tag/머신러닝">머신러닝 (8)</a>
             
              <a href="/tag/책">책 (4)</a>
             
              <a href="/tag/요약">요약 (3)</a>
             
              <a href="/tag/연울림">연울림 (4)</a>
             
              <a href="/tag/데이터분석">데이터분석 (3)</a>
             
              <a href="/tag/data-analysis">Data-analysis (3)</a>
             
              <a href="/tag/사회">사회 (3)</a>
             
              <a href="/tag/society">Society (3)</a>
             
              <a href="/tag/뉴스댓글">뉴스댓글 (3)</a>
             
              <a href="/tag/news-comments">News-comments (3)</a>
             
              <a href="/tag/노르웨이">노르웨이 (1)</a>
             
              <a href="/tag/여행">여행 (1)</a>
             
              <a href="/tag/quantum-computer">Quantum-computer (1)</a>
             
              <a href="/tag/basic">Basic (1)</a>
             
              <a href="/tag/양자컴퓨터">양자컴퓨터 (1)</a>
             
              <a href="/tag/기초">기초 (1)</a>
             
              <a href="/tag/nlp">Nlp (5)</a>
             
              <a href="/tag/dataset">Dataset (1)</a>
             
              <a href="/tag/자연어처리">자연어처리 (5)</a>
             
              <a href="/tag/데이터셋">데이터셋 (1)</a>
             
              <a href="/tag/review">Review (1)</a>
             
              <a href="/tag/후기">후기 (1)</a>
             
              <a href="/tag/technique">Technique (1)</a>
             
              <a href="/tag/attention">Attention (1)</a>
             
              <a href="/tag/korean">Korean (1)</a>
             
              <a href="/tag/hate-speech">Hate-speech (1)</a>
             
              <a href="/tag/한국어">한국어 (1)</a>
             
              <a href="/tag/혐오발언">혐오발언 (1)</a>
             
              <a href="/tag/multilingual">Multilingual (1)</a>
             
              <a href="/tag/BERT">Bert (1)</a>
            
          
        

		</div>
	</div>
</div>
 


<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                 Copyright © 2020 Space Moon 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net">Mediumish Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

   
</div> <!-- /.site-content>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
    
<script src="/assets/js/jquery.min.js"></script>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
    
<script src="/assets/js/ie10-viewport-bug-workaround.js"></script>
    
<script src="/assets/js/mediumish.js"></script>
    
<script id="dsq-count-scr" src="//inmoonlight.disqus.com/count.js"></script>
</body>
</html>
