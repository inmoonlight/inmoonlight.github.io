<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>개인적이지만 사소하지 않은 이야기를 담고 싶습니다</description>
    <link>https://inmoonlight.github.io/</link>
    <atom:link href="https://inmoonlight.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 26 Jan 2020 20:29:14 +0900</pubDate>
    <lastBuildDate>Sun, 26 Jan 2020 20:29:14 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Positional Encoding</title>
        <description>&lt;p&gt;Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다.
보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다.
이번 글에서는 Convolutional Neural Network (CNN), End-to-End Memory Network (MemN2N), Transformer에서 sentence embedding을 위해 사용된 positional encoding에 대해 소개하려고 한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#1-introduction&quot;&gt;1. Introduction&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;#2-learned-positional-embeddings&quot;&gt;2. Learned Positional Embeddings&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;#3-positional-encoding&quot;&gt;3. Positional Encoding&lt;/a&gt; &lt;br /&gt;
    &lt;a href=&quot;#30-the-intuition&quot;&gt;3.0 Intuition&lt;/a&gt; &lt;br /&gt;
    &lt;a href=&quot;#31-in-memn2n&quot;&gt;3.1 In MemN2N&lt;/a&gt; &lt;br /&gt;
    &lt;a href=&quot;#32-in-transformer&quot;&gt;3.2 In Transformer&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;#4-conclusions&quot;&gt;4. Conclusions&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;1. Introduction&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;일반적으로 NLP 모델은 각 문장을 구성하는 token을 one-hot vector가 아닌 distributed vector로 표현한다.
그 이유는 distributed representation이 1) 비슷한 의미지만 다른 lexical form을 가진 token을 더 잘 표현할 수 있기 때문이고, 2) embedding dimension을 감소시킬 수 있기 때문이다.&lt;/p&gt;

&lt;p&gt;문장의 embedding은 문장을 이루는 각 token의 embedding을 조합하는 방식으로 얻어진다.
이 때 position에 대한 정보가 없다면 모델은 &lt;code class=&quot;highlighter-rouge&quot;&gt;handful of chocolate&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;chocolate of handful&lt;/code&gt;을 같은 의미로 인식하게 된다.
RNN은 모델 구조 자체에 time information이 녹아져 있다.
그래서 &lt;code class=&quot;highlighter-rouge&quot;&gt;handful --&amp;gt; of --&amp;gt; chocolate&lt;/code&gt; 의 순서가 담긴 sentence embedding을 자연스럽게 얻을 수 있다.
반면 CNN이나 Attention 기반의 Transformer는 순서에 대한 정보를 강제해야 하고, 이 때 positional encoding이 사용된다.&lt;/p&gt;

&lt;p&gt;Positional encoding (PE) 은 token embedding vector에 곱해지는 정보로, sentence에서 해당 token이 어디에 위치해 있는지를 나타낸다.
J개의 token &lt;script type=&quot;math/tex&quot;&gt;t_j \in \mathbb{R}^d&lt;/script&gt; 으로 구성된 sentence &lt;script type=&quot;math/tex&quot;&gt;s = [t_1, t_2, ..., t_J]&lt;/script&gt; 가 있다고 하자. 
PE &lt;script type=&quot;math/tex&quot;&gt;\in \mathbb{R}^{J \times d}&lt;/script&gt; 의 row &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; 마다 다른 값을 가지도록 하여 문장 맨 처음의 &lt;code class=&quot;highlighter-rouge&quot;&gt;handful&lt;/code&gt;과 맨 뒤의 &lt;code class=&quot;highlighter-rouge&quot;&gt;handful&lt;/code&gt;을 다르게 인식하도록 한다.&lt;/p&gt;

&lt;p&gt;PE를 구성하는 방식에는 크게 두 종류가 있다.
하나는 학습기반, 다른 하나는 position과 dimension을 입력으로 한 함수를 이용하는 방법이다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-learned-positional-embeddings&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;2. Learned Positional Embeddings&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;학습기반의 PE를 구성하는 방식은 Convolutional Sequence to Sequence Learning (ConvS2S)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;에서 사용되었다.
평균 0, 표준편차 0.1을 따르는 normal distribution으로 initialize되고 학습을 통해 position 정보를 배우길 기대한다.&lt;/p&gt;

&lt;p&gt;PE를 encoder와 decoder 모두에 사용한 경우, encoder에만 사용한 경우, decoder에만 사용한 경우, 아예 사용하지 않은 경우로 나누어 번역 task에 실험해보았을 때의 결과는 다음과 같다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/learned_pe_table.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
BLEU를 기준으로 분석해보면 encoder에서의 PE역할이 decoder보다 조금 더 중요하다.
PE를 아예 쓰지 않을 때의 점수가 가장 낮지만 점수 차이를 생각해보면 모델 성능에는 크게 영향을 미치지 않는다고 해석해 볼 수 있다.&lt;/p&gt;

&lt;p&gt;학습 기반이므로 학습 시 다루지 않았던 길이의 문장이 입력으로 들어온 경우, 외삽이 불가능하다는 단점이 있다.&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-function-based-positional-encoding&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;3. Function-based Positional Encoding&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;함수 기반의 PE는 문장에서 몇 번째에 위치한 토큰인지, 토큰의 embedding dimension이 무엇인지를 정해주면 값이 정해진다.
이 때, 다른 위치의 정보가 같은 값으로 mapping되지 않아야 한다.
어떻게 구현할 수 있을까?&lt;/p&gt;

&lt;h3 id=&quot;30-the-intuition&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;3.0 The Intuition&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;0부터 15까지의 숫자를 2진법으로 나타내보자.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/PE_intuition.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
다른 색으로 구분지어 표현한 2진수의 자리수마다 다른 주기를 가지는 것을 볼 수 있다.
붉은색은 주기가 1이고, 노란색은 주기가 2, 초록색은 주기가 4, 파란색은 주기가 8이다.&lt;/p&gt;

&lt;p&gt;위 예시에서의 자리수를 embedding dimension이라고 생각해보면 PE에도 같은 원리를 확장시켜볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;31-in-memn2n&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;3.1 In MemN2N&lt;/a&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;End-to-End Memory Network (MemN2N)&lt;sup id=&quot;fnref:4:1&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;에서는 아래의 함수를 사용했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;PE_{k j}=(1- \frac{j}{J})-\frac{k}{d}(1- \frac{2j}{J})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;j \in {1, ..., J}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k \in {1, ..., D}&lt;/script&gt;

&lt;p&gt;임의의 문장 &lt;code class=&quot;highlighter-rouge&quot;&gt;The same representation is used for questions, memory inputs and memory outputs.&lt;/code&gt;에 적용되는 PE를 시각화해보면 다음과 같다.&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/PE_example_1.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서는 dimension에 관계없이 같은 주기를 가지지만 시작값이 전부 다르다.
결과적으로는 position마다 다른 vector를 곱하게 되어 position 정보를 전달할 수 있다.&lt;/p&gt;

&lt;p&gt;다른 문장 길이를 가지는 경우에 대해서 적용해보면 어떨까?
이번에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;We therefore propose a second representation that encodes the position of words within the sentence.&lt;/code&gt;에 대해 시각해보았다.&lt;sup id=&quot;fnref:5:1&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/PE_example_2.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;position이 늘어난만큼 position encoding 값의 변화도가 줄었다.
J는 문장마다 달라지므로 첫번째, 두번째의 절대적인 위치보다는 각 순서를 구분짓기 위한 목적에 치중하였다.&lt;/p&gt;

&lt;p&gt;ConvS2S에서와는 달리 MemN2N에서 PE의 효과는 꽤나 컸던 것으로 보인다.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/MemN2N_PE.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-in-transformer&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;3.2 In Transformer&lt;/a&gt;&lt;sup id=&quot;fnref:6:1&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Attention is all you need&lt;sup id=&quot;fnref:6:2&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;에서 사용된 PE는 &lt;strong&gt;주기&lt;/strong&gt;함수로 유명한 sin 함수와 cos 함수를 기반으로 한다. (a.k.a, sinusoidal functions)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
P E_{(\text {pos, 2k} )} &amp;=\sin \left(\text {pos} / 10000^{2 k / d}\right) \\ P E_{(\text {pos,2k+1})} &amp;=\cos \left(\text {pos} / 10000^{2 k / d}\right) 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;잠시 고등학교 때 배운 수학을 떠올려보자. 
&lt;script type=&quot;math/tex&quot;&gt;sin(ax + b)&lt;/script&gt; 의 주기는 &lt;script type=&quot;math/tex&quot;&gt;2\pi / |a|&lt;/script&gt; 이다.
따라서 PE의 특정 position vector 값의 주기는 &lt;script type=&quot;math/tex&quot;&gt;2\pi \cdot 10000^{2 k / d}&lt;/script&gt; 와 같다.&lt;/p&gt;

&lt;p&gt;MemN2N에서의 PE와는 달리, position vector의 주기가 vector의 dimension마다 변화한다.
전체 벡터 크기(&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;)가 128이라고 가정할 때, k가 작을수록 주기가 짧고 k가 클수록 주기도 길어진다. (아래 그림 참고)&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/positional_encoding.png&quot; width=&quot;90%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Image credit: https://kazemnejad.com/blog/transformer_architecture_positional_encoding&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;왜 Transformer에서는 MemN2N과 다르게 sinusoidal 함수를 썼을까?
논문에서 그 이유를 짧게 기술하고 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;P E_{pos+k}&lt;/script&gt; can be represented as a linear function of &lt;script type=&quot;math/tex&quot;&gt;P E_{pos}&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;sinusodial 함수의 특징을 이용해 첫번째, 두번째마다 같은 position 정보를 주면서도 &lt;script type=&quot;math/tex&quot;&gt;n + k&lt;/script&gt; 번째 vector가 &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; 번째 vector와 관계가 있을 때 이를 학습할 수 있는 여지를 남겨주기 위함이다.
(참고로 이에 대한 수학적인 증명은 &lt;a href=&quot;https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/&quot;&gt;이 article&lt;/a&gt;에 기술되어 있다.)&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-conclusions&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;4. Conclusions&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;PE는 크게 학습을 통해 정해질 수 있고 미리 지정한 함수로 정해질 수도 있다.
학습을 통한 방식은 학습시 보지 않았던 새로운 길이가 등장했을 때 외삽이 불가능하지만 함수 기반의 PE는 가능하다.
함수도 어떤 함수를 쓰느냐에 따라 종류가 구분되는데, 절대적인 위치에 따라 같은 값을 가지면서도 상대적 위치의 관계도 학습할 수 있는 sin과 cos 기반의 함수가 가장 좋은 방법이라고 생각된다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;References&lt;/a&gt;&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.03122&quot;&gt;https://arxiv.org/abs/1705.03122&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:6:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:6:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;제가 만든 용어이므로 공식적으로 사용하면 곤란할 수 있습니다 ㅎㅎ &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://kazemnejad.com/blog/transformer_architecture_positional_encoding/&quot;&gt;https://kazemnejad.com/blog/transformer_architecture_positional_encoding/&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1503.08895&quot;&gt;https://arxiv.org/abs/1503.08895&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb&quot;&gt;https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 26 Jan 2020 17:00:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2020/01/26/Positional-Embedding</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2020/01/26/Positional-Embedding</guid>
        
        
        <category>machine-learning</category>
        
        <category>nlp</category>
        
        <category>technique</category>
        
        <category>머신러닝</category>
        
        <category>자연어처리</category>
        
      </item>
    
      <item>
        <title>은희경 - 빛의 과거</title>
        <description>&lt;p&gt;새해를 맞이하자마자 작년의 목표와 기대에는 없던 일이 일어났다.
오랜만에 뵌 분께 은희경 작가님의 “빛의 과거”라는 책을 선물받은 것이다.
그 동안 읽었던 텍스트라고는 오직 논문이었기에 선물받자마자 들었던 생각은,
“아, 내가 과연 책을 읽을 수 있을까?”
였다.&lt;/p&gt;

&lt;p&gt;그 와중에 작가님의 성함이 눈에 띄었다.
은희경 작가님…
왜 이렇게 익숙한 이름인가 했더니 개인적으로 존경하고 흠모하는 언니로부터 추천받았던 작가님이었다는 것이 떠올랐다. 
단지 이 작은 이유만으로 오랜만에 접하는 소설의 벽이 낮아지는 느낌이었다.&lt;/p&gt;

&lt;p&gt;이 책을 읽을 운명이었던 것인지, 마침 다음 날은 주말이었고 미용실에 오랜만에 가기 위해 예약을 잡아두었다.
자리에 앉자마자 책을 펼쳐들었다.
그리고 단숨에 그 소설의 세계에 몰입하게 되었다.&lt;/p&gt;

&lt;h3 id=&quot;책의-내용&quot;&gt;책의 내용&lt;/h3&gt;

&lt;p&gt;그래도 공개적인 곳에 쓰는 글이니만큼, 적어도 이 글에 어쩌다 접근하게 된 누군가를 위해 줄거리를 적어두어야 할 것 같다.&lt;/p&gt;

&lt;p&gt;주인공 ‘나’는 2017년 현재, 남편을 사별한 번역일을 하면서 살아가는 평범한 주부다.
1977년 대학생 시절 기숙사에서 처음으로 인연을 맺은 ‘친구’와 끊어질듯 끊어지지 않는 관계가 이어지고 그 ‘친구’와는 그렇게 친하지는 않지만 계속 연락을 하며 지낸다.
이 ‘친구’는 소설가다. 
작가가 꿈이었던 적은 없었지만 여러 직업을 거쳐 여기에 이르렀다.
그리고 본격적인 이야기는 그 친구의 책을 읽는 것으로 시작된다.
책은 함께 기억을 공유하고 있는 대학생 시절을 배경으로 한다.&lt;/p&gt;

&lt;h3 id=&quot;책의-구성&quot;&gt;책의 구성&lt;/h3&gt;

&lt;p&gt;한 편의 영화와 같은 구성이었다.
1977년과 2017년을 오가며 이야기가 진행된다.
처음에는 “흔한 구성”이라고 생각했지만 읽어나가다보니 시간을 오가지 않으면 불가능했을 이야기였다.&lt;/p&gt;

&lt;p&gt;‘2017년의 나’가 있었기 때문에 ‘1977년의 나’의 어리숙했던 부분을 어른의 시점에서 돌이켜 볼 수 있었고, 
‘1977년의 나’가 있었기 때문에 어렸을 시절의 나의 시점에서 어리숙했을 수 밖에 없었던 일을 설득력있게 전달할 수 있었다.&lt;/p&gt;

&lt;p&gt;그리고 1977년은 지금과는 달리 더 보수적인 시대였다.
‘1977년의 나’는 그 시대를 ‘여대생’의 시점에서 덤덤하게 이야기한다. 
‘2017년의 나’였다면 시니컬한 톤이 묻어날 수 밖에 없지 않았을까?&lt;/p&gt;

&lt;h3 id=&quot;마음에-닿았던-구절&quot;&gt;마음에 닿았던 구절&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;회사의 관례에 따라 여성 기혼자에게 주어지는 계약직 전환 서류를 내 책상 위에 갖다 놓은 것도 그녀였다. &lt;br /&gt;
- p.9&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마음에 닿았던 구절이라고 했지, 그 구절이 감동적일 것이라는 이야기는 하지 않았다 :P
‘정말 이랬다고?’ 라는 놀라움 때문에 바로 형광펜을 들이밀었다.
나는 이런 대우가 당연했었던 시절을 상상조차 할 수 없다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;끊어진 건 아니지만 밀착될 일도 없는, 간격이 불규칙한 점선 같은 관계였다. &lt;br /&gt;
- p.11&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;‘나’와 ‘친구’의 관계를 이야기하는 문장이다.
대학을 졸업하고 사회생활을 하는 사람들이라면 누구나 공감할 수 있는 관계일 것이다.
어떻게 이렇게 표현할 수 있을까 싶어서 밑줄을 그었다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그러나 그녀에게는 사람을 대할 때 미묘한 권력관계를 만드는 습성이 있었다. 끊임없이 자신을 중심으로 돌아가는 관계의 자장을 만들어내고 우월감과 피해 의식을 번갈아 써가며 그것을 정당화했다. 거기에는 증인이 필요했다. 결국 나로 하여금 위성처럼 그녀의 궤도를 따라 돌며 그녀라는 일방적이고 변덕스러운 광원을 반사하도록 만들어 버리는 것이다. &lt;br /&gt;
나는 나대로 소심함과 자기 합리화의 조합인 어정쩡한 온검함 뒤에 숨어 그녀의 그런 태도를 순순히 받아들이곤 했다. 열정은 단호한 구석이 있어서 금세 꺾이지만 친근함은 어느 정도 안이한 감정이라서 사소한 기억의 공유만으로도 쉽게 환기되었다. &lt;br /&gt;
- p.12&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;‘나’와 ‘친구’의 관계를 묘사한 내용이다. 
둘 사이의 미묘한 권력관계와 그 속에서 우월감을 느끼는 친구, 그리고 그 것이 느껴지지만 크게 동요하지 않고 순종적인 ‘나’의 모습이 참으로 세련되게 표현되었다.
“자신을 중심으로 돌아가는 관계의 자장”과 “소심함과 자기 합리화의 조합인 어정쩡한 온건함”.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;부분적으로나마 모범생 흉내를 내서 그 시스템에 순종했고 그 대가로 서울의 한 여자대학에 합격하여 고향과 부모로부터 벗어날 수 있었다. &lt;br /&gt;
- p.27&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;정말 나의 이야기였다. 
고등학생 시절의 나도 이 책의 화자처럼 소심했다.
시스템을 거스를 힘과 배짱이 없었기 때문에 그 안에서 내가 할 수 있는 최대한의 반항을 했다.
그 곳에서 벗어나고 시스템을 바꿀 힘을 갖기 위해 좋은 대학을 가는 것.
분명 이 것은 1977년의 문장일텐데, 어째서 2010년에도 똑같은 모습이었던걸까.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그 때까지 다름이란 걸 전혀 겪어보지 못했냐고 묻는다면 그렇지는 않다. 
12년간이나 중단 없이 지긋지긋했던 초중등학교 생활 속에서도 타인과 부딪힐 기회는 얼마든지 있었다. 
그러나 내가 그 시절 겪없던 것은 다름이라기보다 수직적인 위계와 시비였다. 
그때그때 적용되는 일관성 없는 규율이 있었고, 없으면 교사나 반장이나 힘센 애들이 만들었다. 
남과 다른 것이 그대로 결격사유가 되는 단체 생활에서 내가 누군지 따위를 고민할 기회는 아무에게도 주어지지 않았다. &lt;br /&gt;
-p. 27&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 페이지 속의 많은 문장에서 머물렀다.
나도 책의 주인공처럼 고등학교를 탈출하고 대학교에 합격을 하자마자 기숙사 생활을 했다.
6명이서 함께 사는 방이었고 한 방은 두 명이 함께 썼다.
합격과 입학이라는 들뜬 마음도 잠시, 나와 ‘다른’ 누군가와 시간과 장소를 공유하는 것의 어려움이 크게 다가왔었다.
그 때는 이유를 몰랐지만 여기에 서술된 것처럼, 내가 고등학교 시절까지 경험했던 삶 속에서 진정한 ‘다름’에 대해 이해하는 시간은 전혀 없었기 때문이었던 것은 아니었을까.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;혼자라는 건 어떤 공간을 혼자 차지하는게 아니라 타인의 시선에서 벗어나 익명으로 존재하는 시간을 뜻하는 거였다. &lt;br /&gt;
-p. 84&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;단 한 번도 혼자 보내는 시간을 이렇게 정의해보지 않았었다.
사람들 속에 있으면서도 혼자라고 느끼는 이유는 그 들에게 난 철저히 ‘익명’의 누군가이기 때문이다.
이 글을 읽은 이후, 지하철에 있을 때마다 괜히 피식되게 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;나는 내 앞의 문을 열지 못하고 번번이 과거의 나로 굴러떨어지곤 했다. &lt;br /&gt;
-p. 86&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;왜 자꾸 나의 대학생활이 떠오르는 것일까.
1977년이든, 2011년이든 청춘의 고민은 비슷했다. 
그리고 여기에 적진 않았지만 행복의 순간도 비슷했다.
사람을 둘러싼 정치적, 사회적, 경제적, 기술적 상황은 너무나도 달라졌지만 사람은 결국 똑같았다.
나를 행복하게 만드는 선택과 행복을 느끼는 방법만 달라졌다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;첫인상 역시 두번째와 크게 다르지 않았다. &lt;br /&gt;
-p. 287&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 문장은 반드시 두번 읽어야 한다. 
무심결에 넘기면 평범한 문장이지만 전혀 그렇지 않다.
어떻게 첫인상이 두번째 인상 이후에 느껴질 수 있단 말인가?!
이 문장은 첫번째 만남을 기억하지 못한 채로 두번째 만남을 첫번째라고 생각했기 때문에 가능했다.
작가님의 재치에 빵- 터졌다.&lt;/p&gt;

&lt;h3 id=&quot;개인적인-감상&quot;&gt;개인적인 감상&lt;/h3&gt;

&lt;p&gt;작가님의 문체와 삶에서 느끼는 감각에 모두 공명할 수 있었다.
내가 느꼈던 “그 감정”을 거부감없이 재치있고 한편으로는 날카로운 문장으로 군더더기 없이 표현한다.&lt;/p&gt;

&lt;p&gt;군더더기 없음은 자칫하면 82년생 김지영처럼 어떤 메시지를 품을 수 있는 배경의 소설을 담백하게 만든다.
책을 읽으면서 나는 1977년을 관찰하고 있지 않았다.
그 시대 속에 머물러 있으며 소설 속의 “나”가 겪어나가는 감정에 공감하기 위해 시대적 배경을 이해하려고 했다.
내가 77년대에 부자로 살았더라면, 똑똑한 남자였다면, 똑똑한 여자였다면, 소심한 남자였다면, 적극적인 여자였다면 어땠을까.
나의 기질과 나의 주어진 환경에 의해 2020년, 지금의 나와 다른 모습이었겠지.&lt;/p&gt;

&lt;p&gt;책 속에서 사회가 남자에게 부여하는 모습은 능력있고 가족을 책임지고 이끌어나가는 가부장적인 것이었다.
반대로 여성은 자원이 부족하다면 언제든 가장 먼저 포기를 강요받은 대상이었고 남자의 내조를 위한 모습을 요구받았다.
불현듯 보수적인 할머니와 보수적인 집안에서 가족의 기대를 한몸에 받았던 아버지가, 그리고 이 보수적인 집안으로 시집와서 많은 것을 포기했던 어머니가 떠올랐다.&lt;/p&gt;

&lt;p&gt;논문과는 다르게 소설은 “그래서 앞으로 무엇을 하면 좋을까?”에 대한 생각을 남기기보다는 내가 경험했지만 의식하지 않았던 회색 지대 속의 감정과 기억을 재구성하고 새롭게 의미를 붙여준다.
그래서 과거 속으로 탐험하는 기분이 들었고 특히나 대학생 시절이 많이 떠올랐다.
2020년은 내가 한국나이로 30대가 된 해이기도 하다. 
개인적으로는 20대의 나를 돌이켜보기에 너무나도 좋았던 책이었다.
그 때 무엇을 어떻게 해야할지 몰랐던, 나만의 다름은 인지하지 못하고 다르다는 것이 두렵기만 했던 내가 지금은 이렇게 달라졌구나.&lt;/p&gt;

&lt;p&gt;짜식 많이 컸구나. &lt;br /&gt;
수고했다.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 22:00:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung</guid>
        
        
        <category>book</category>
        
        <category>review</category>
        
        <category>essay</category>
        
        <category>책</category>
        
        <category>후기</category>
        
        <category>에세이</category>
        
      </item>
    
      <item>
        <title>General Language Understanding Evaluation (GLUE) benchmark</title>
        <description>&lt;p&gt;General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다.
ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, &lt;a href=&quot;https://openreview.net/pdf?id=rJ4km2R5t7&quot;&gt;전자&lt;/a&gt;는 설명이 상세하고 &lt;a href=&quot;https://www.aclweb.org/anthology/W18-5446.pdf&quot;&gt;후자&lt;/a&gt;는 요약되어 있다. 
이 글은 가장 최근(2019.2.22)에 업데이트된 &lt;a href=&quot;https://arxiv.org/pdf/1804.07461.pdf&quot;&gt;arXiv에 있는 논문&lt;/a&gt;을 기반으로 작성되었다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;1. GLUE overall&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#corpus-of-linguistic-acceptability-cola&quot;&gt;Corpus of Linguistic Acceptability (CoLA)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#stanford-sentiment-treebank-sst-2&quot;&gt;Stanford Sentiment Treebank (SST-2)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#microsoft-research-paraphrase-corpus-mrpc&quot;&gt;Microsoft Research Paraphrase Corpus (MRPC)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#quora-question-pairs-qqp&quot;&gt;Quora Question Pairs (QQP)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#semantic-textual-similarity-benchmark-sts-b&quot;&gt;Semantic Textual Similarity Benchmark (STS-B)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#multi-genre-nli-corpus-mnli&quot;&gt;Multi-Genre NLI corpus (MNLI)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#the-recognizing-textual-entailment-rte&quot;&gt;The Recognizing Textual Entailment (RTE)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#the-stanford-question-answering-nli-qnli&quot;&gt;The Stanford Question Answering NLI (QNLI)&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#the-winograd-schema-challenge-nli-wnli&quot;&gt;The Winograd Schema Challenge NLI (WNLI)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#2-download&quot;&gt;2. Download&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#3-leaderboard&quot;&gt;3. Leaderboard&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-glue-overall&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;1. GLUE overall&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;GLUE는 총 9개의 task로 구성되었으며 각 task는 언어의 특정한 성질을 평가하기 위한 목적으로 만들어졌고, 최종 점수는 각 task 별 점수의 평균 값을 가져간다.
task는 크게 3가지 - Single-Sentence Tasks (CoLA, SST-2), Similarity and Paraphrase Tasks (MRPC, QQP, STS-B), Inference Tasks (MNLI, RTE, QNLI, WNLI) - 로 구분할 수 있다.
세부 task에 대해 살펴보기 전에 전반적인 task의 특징을 아래의 표에 정리했다.
원 논문에 정리되어 있는 것을 바탕으로 재구성하였고 직접 다운로드 받은 데이터를 기준으로 측정했기 때문에 corpus의 size가 다를 수 있다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;data&lt;/th&gt;
      &lt;th&gt;|train|&lt;/th&gt;
      &lt;th&gt;|dev|&lt;/th&gt;
      &lt;th&gt;|test|&lt;/th&gt;
      &lt;th&gt;domain&lt;/th&gt;
      &lt;th&gt;input&lt;/th&gt;
      &lt;th&gt;task&lt;/th&gt;
      &lt;th&gt;metrics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#corpus-of-linguistic-acceptability-cola&quot;&gt;Corpus of Linguistic Acceptability (CoLA)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;8.5k&lt;/td&gt;
      &lt;td&gt;1.0k&lt;/td&gt;
      &lt;td&gt;1.2k&lt;/td&gt;
      &lt;td&gt;linguistics literature&lt;/td&gt;
      &lt;td&gt;single-sentence&lt;/td&gt;
      &lt;td&gt;- grammatical acceptability &lt;br /&gt; - binary classification &lt;br /&gt; (grammatical / ungrammatical)&lt;/td&gt;
      &lt;td&gt;Matthews correlation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#stanford-sentiment-treebank-sst-2&quot;&gt;Stanford Sentiment Treebank (SST-2)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;67k&lt;/td&gt;
      &lt;td&gt;872&lt;/td&gt;
      &lt;td&gt;1.8k&lt;/td&gt;
      &lt;td&gt;movie reviews&lt;/td&gt;
      &lt;td&gt;single-sentence&lt;/td&gt;
      &lt;td&gt;- sentiment &lt;br /&gt; - binary classification &lt;br /&gt; (positive / negative)&lt;/td&gt;
      &lt;td&gt;acc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#microsoft-research-paraphrase-corpus-mrpc&quot;&gt;Microsoft Research Paraphrase Corpus (MRPC)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;3.7k&lt;/td&gt;
      &lt;td&gt;408&lt;/td&gt;
      &lt;td&gt;1.7k&lt;/td&gt;
      &lt;td&gt;news&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;paraphrase&lt;/td&gt;
      &lt;td&gt;acc./F1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#quora-question-pairs-qqp&quot;&gt;Quora Question Pairs (QQP)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;364k&lt;/td&gt;
      &lt;td&gt;40k&lt;/td&gt;
      &lt;td&gt;391k&lt;/td&gt;
      &lt;td&gt;social QA questions&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;paraphrase&lt;/td&gt;
      &lt;td&gt;acc./F1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#semantic-textual-similarity-benchmark-sts-b&quot;&gt;Semantic Textual Similarity Benchmark (STS-B)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5.8k&lt;/td&gt;
      &lt;td&gt;1.5k&lt;/td&gt;
      &lt;td&gt;1.4k&lt;/td&gt;
      &lt;td&gt;news &lt;br /&gt; caption &lt;br /&gt; forum&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;- sentence similarity &lt;br /&gt; - regression&lt;/td&gt;
      &lt;td&gt;Pearson / Spearman corr.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#multi-genre-nli-corpus-mnli&quot;&gt;Multi-Genre NLI corpus (MNLI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;393k&lt;/td&gt;
      &lt;td&gt;20k&lt;/td&gt;
      &lt;td&gt;20k&lt;/td&gt;
      &lt;td&gt;fiction &lt;br /&gt; face-to-face &lt;br /&gt; government &lt;br /&gt; letters &lt;br /&gt; 9/11 &lt;br /&gt; oxford university press (oup) &lt;br /&gt; slate&lt;br /&gt; telephone &lt;br /&gt; travel &lt;br /&gt; verbatim&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;ternary classification &lt;br /&gt; (entailment / contradiction / neutral)&lt;/td&gt;
      &lt;td&gt;matched acc. / mismatched acc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#the-recognizing-textual-entailment-rte&quot;&gt;The Recognizing Textual Entailment (RTE)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2.5k&lt;/td&gt;
      &lt;td&gt;276&lt;/td&gt;
      &lt;td&gt;3.0k&lt;/td&gt;
      &lt;td&gt;news &lt;br /&gt; wikipedia&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;binary classification &lt;br /&gt; (entailment / not_entailment)&lt;/td&gt;
      &lt;td&gt;acc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#the-stanford-question-answering-nli-qnli&quot;&gt;The Stanford Question Answering NLI (QNLI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;105k&lt;/td&gt;
      &lt;td&gt;5.5k&lt;/td&gt;
      &lt;td&gt;5.5k&lt;/td&gt;
      &lt;td&gt;wikipedia&lt;/td&gt;
      &lt;td&gt;two sentences (question, sentence)&lt;/td&gt;
      &lt;td&gt;binary classification &lt;br /&gt; (entailment / not_entailment)&lt;/td&gt;
      &lt;td&gt;acc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#the-winograd-schema-challenge-nli-wnli&quot;&gt;The Winograd Schema Challenge NLI (WNLI)&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;634&lt;/td&gt;
      &lt;td&gt;71&lt;/td&gt;
      &lt;td&gt;146&lt;/td&gt;
      &lt;td&gt;fiction books&lt;/td&gt;
      &lt;td&gt;two sentences&lt;/td&gt;
      &lt;td&gt;binary classification &lt;br /&gt; (entailment / not_entailment)&lt;/td&gt;
      &lt;td&gt;acc.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;corpus-of-linguistic-acceptability-cola&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Corpus of Linguistic Acceptability (CoLA)&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;공개된 언어학 문헌(publised liguistics literature)에서 추출된 약 21k 문장들로 구성되어 있다.
이 문장들은 문법적으로 옳은지, 그른지가 표기되어 있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1 They drank the pub dry.
0 * They drank the pub.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;문법적으로 옳고 그름을 판단하는 기준은 다양하다. 
아래의 표는 corpus를 제작하면서 기준에서 포함된 것들과 제외된 것들을 나타낸다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/cola_dataset_description.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Included&lt;/strong&gt;
&lt;br /&gt; 
&lt;strong&gt;(a) Morphological Violation&lt;/strong&gt;: “should leave” 가 올바른 표현이지만 “should leaving”으로 작성되었다. 동사의 형태(verbal inflection)가 맞지 않는 경우에 해당한다.
&lt;br /&gt;
&lt;strong&gt;(b) Syntactic Violation&lt;/strong&gt;: “What did Bill buy?” 혹은 “Bill bought potatoes and _” 이 되어야 한다. 통사 구조가 틀린 경우에 해당한다.
&lt;br /&gt;
&lt;strong&gt;(c) Semantic Violation&lt;/strong&gt;: 의미적으로 말이 되지 않는 문장에 해당한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Excluded&lt;/strong&gt;
&lt;br /&gt;
&lt;strong&gt;(d) Pragmatic Anomalies&lt;/strong&gt;: grammar와 상관없는 외부 지식이 필요하므로 제외되었다.
&lt;br /&gt;
&lt;strong&gt;(e) Unavailable Meanings&lt;/strong&gt;: 문장만보고는 판단이 애매하므로 제외되었다.
&lt;br /&gt;
&lt;strong&gt;(f) Prescriptive Rules&lt;/strong&gt;: 사람도 누군가의 가르침없이는 터득하기 어려운 rule이기 때문에 제외되었다.
&lt;br /&gt;
&lt;strong&gt;(g) Nonce Words&lt;/strong&gt;: “arrivable”과 같이 typical word-level NLP 모델의 vocab에는 등장하지 않는 단어가 포함된 경우이다. NLP 모델의 scope이 아니라고 판단되어 제외되었다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;testset-and-metrics&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;testset은 In-Domain과 Out-of-Domain으로 구성되어 있다. 
In-Domain은 training set이 추출된 source와 같은 source에서, Out-of-Domain은 training set이 추출되지 &lt;em&gt;않은&lt;/em&gt; source에서 구성되었다. 
GLUE는 원래 구분된 두 testset을 하나로 합쳐 단일 testset을 구축하였고 총 1,160 문장이다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/cola_by_source.png&quot; width=&quot;40%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 task의 평가는 unbalanced binary classification task에서 사용되는 Matthews correlation으로 한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;stanford-sentiment-treebank-sst-2&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Stanford Sentiment Treebank (SST-2)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rottentomatoes.com&lt;/code&gt;의 영화 리뷰 corpus로 구성되었으며 AMT(Amazon Mechanical Turk)를 통해 리뷰의 sentiment가 labeling 되었다.
1은 긍정, 0은 부정을 나타낸다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;that loves its characters and communicates something rather beautiful about human nature 1
on the worst revenge-of-the-nerds clichés the filmmakers could dredge up 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-1&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;일반적인 binary classification 문제로 accuracy를 통해 평가한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;microsoft-research-paraphrase-corpus-mrpc&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Microsoft Research Paraphrase Corpus (MRPC)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;MRPC는 온라인 뉴스에서 추출된 문장들로 구성되었으며 2개의 문장이 의미적으로 같은지 다른지를 평가하는 task이다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . 
On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .
1

Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion . 
Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .
0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-2&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;testset이 label이 불균등(68% positive,  32% negative)하므로 accuracy와 F1 score를 metric으로 한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;quora-question-pairs-qqp&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Quora Question Pairs (QQP)&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;https://www.quora.com/&lt;/code&gt;의 질문들로 구성되었으며, 두 개의 질문이 의미상 같은지 다른지가 표기되어있다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How do you start a bakery?
How can one start bakery business?
1

What are natural numbers?
What is a least natural number?
0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-3&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;MRPC와 마찬가지로 불균등(37% positive, 63% negative)하므로 accuracy와 F1 score가 metric으로 활용된다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;semantic-textual-similarity-benchmark-sts-b&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Semantic Textual Similarity Benchmark (STS-B)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;문장의 유사도는 번역, 요약, 문장 생성, QA, 대화 모델링 등등 다양한 NLP 분야에서 중요하게 다뤄진다. 
STS shared task는 모델이 문장들의 유사도를 얼마나 잘 파악하는지를 평가하기 위해 등장하였고, 2012년부터 2017년까지 매년 개최되었으며 그 때마다 다른 dataset이 사용되었다.
이 때문에 각 연도의 데이터셋을 적절히 조합한 common evaluation set으로 STS-B가 소개되었다.&lt;/p&gt;

&lt;p&gt;이 전의 task와는 다르게 STS는 regression task이다. 
human annotator들은 두 문장의 의미적인 유사도를 1~5점으로 평가하였고 모델은 score를 예측해야한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A plane is taking off.  An air plane is taking off.     5.000
Three men are playing chess.    Two men are playing chess.      2.600
A man is smoking.       A man is skating.       0.500
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-4&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;Regression task이므로 human label과의 Pearson correlation으로 평가된다.&lt;/p&gt;

&lt;!-- collection of sentence pairs drawn from news headlines video and image captions and NLI data. --&gt;

&lt;!-- 
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/stsb_dataset_description.png&quot; width=&quot;50%&quot;&gt;
&lt;/div&gt;
&lt;br&gt; --&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;multi-genre-nli-corpus-mnli&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;Multi-Genre NLI corpus (MNLI)&lt;/a&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;MNLI는 SNLI(Stanford NLI) dataset의 단점을 개선시킨 데이터셋이다.
SNLI는 image caption으로만 구성되었기 때문에 장면을 표현하는 짧고 간단한 문장이 많고 NLU(Natural Language Understanding) task와 무관한 단어들이 많이 등장한다.
그래서 NLU task의 benchmark로 사용되기는 어렵기 때문에 다양한 도메인(논문에서는 genre라고 표현)의 조합인 MNLI benchmark dataset이 등장하였다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/mnli_dataset_description.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 표에서 나와있듯이 MNLI는 총 10개의 Genre로 구성되었다.
Fiction을 제외한 9개의 domain은 Open American National Corpus에서 추출되었고 Fiction은 fiction literature에서 가져왔으며 mystery, humor, sci-fi 등 그 안에서도 다양한 장르로 구성되었다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;OANC data constitutes the following nine genres: transcriptions from the Charlotte Narrative and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s–early 2000s (LETTERS); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE) written between 1996–2000; transcriptions from University of Pennsylvania’s Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;For our tenth genre, FICTION, we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery (The Mysterious Affair at Styles, 3 Christie, 1921; The Secret Adversary, 4 Christie, 1922; Murder in the Gun Room, 5 Piper, 1953), humor (Password Incorrect, 6 Name, 2008), western (Rebel Spurs, 7 Norton, 1962), science fiction (Seven Swords, 8 Shea, 2008; Living History,9  Essex, 2016; The Sky Is Falling, 10 Del Rey, 1973; Youth, 11 Asimov, May 1952), and adventure (Captain Blood, 12 Sabatini, 1922).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;선별된 문장을 premise로 두고 human annotator들이 premise와 같은 결론을 도출하는 문장(entailment), 반대되는 문장(contradiction), 두 경우 모두 아닌 문장(neutral)을 생성하고 label을 단다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;How do you know? All this is their information again.   
This information belongs to them.      
entailment

Vrenna and I both fought him and he nearly took us.     
Neither Vrenna nor myself have ever fought him.      
contradiction

There was nothing like that emotion now.        
There are few emotions that come close.      
neutral
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-5&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;testset은 CoLA처럼 matched(in-domain)와 mismatched(cross-domain)로 구성되었다. 
mismatched에는 9/11, FACE-TO-FACE, LETTERS, OUP, VERBATIM처럼 training set에는 없는 domain이 포함되어 있다. (위의 표 참고)
각각의 경우를 나누어서 accuracy로 평가한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-recognizing-textual-entailment-rte&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;The Recognizing Textual Entailment (RTE)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;RTE도 STS처럼 RTE1부터 RTE7까지의 데이터셋에서 만들어졌다. 
구체적으로는 RTE1, RTE2, RTE3, RTE5로 구성되었고, 나머지 데이터셋 중 RTE4는 공개되지 않아서, RTE6와 7은 NLI task로는 부적합해서 제외했다고 한다. 
취합하는 과정에서 일부는 세 개의 class, 일부는 두 개의 class로 labeling이 되어있어 이를 일괄적으로 두 개의 class(entailment, not_entailment)로 구분지었다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Swansea striker Lee Trundle has negotiated a lucrative image-rights deal with the League One club.      
Lee Trundle is in business with the League One club.    
entailment

No Weapons of Mass Destruction Found in Iraq Yet.       
Weapons of Mass Destruction Found in Iraq.      
not_entailment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-6&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;일반적인 binary classification 문제이므로 accuracy로 측정한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-stanford-question-answering-nli-qnli&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;The Stanford Question Answering NLI (QNLI)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Stanford에서 구축한 Machine Comprehension 목적의 QA Dataset, a.k.a SQuAD,을 NLI task에 맞게 변형한 데이터셋이다.
SQuAD는 wikipedia에서 paragraph를 가져와서 annotator들이 적절한 질문을 던지는데 이에 대한 답을 paragraph 내에 있는 문장, 구, 단어로 답할 수 있게 구성되었다.
QNLI는 질문과 paragraph 내의 한 문장을 비교하여 이 둘이 entailment되었는지 아닌지를 판단하도록 바뀌었다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;What two things does Popper argue Tarski's theory involves in an evaluation of truth?   
He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.   
entailment

Who was elected as the Watch Tower Society's president in January of 1917?      
His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner.       
not_entailment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-7&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;일반적인 binary classification 문제이므로 accuracy로 측정한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;the-winograd-schema-challenge-nli-wnli&quot;&gt;&lt;a href=&quot;#1-glue-overall&quot;&gt;The Winograd Schema Challenge NLI (WNLI)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;이 데이터셋도 entailment를 평가하는 목적으로 만들어졌다.
original sentence와 이 문장에서 대명사를 일반명사로 치환한 문장 사이의 entailment가 있는지 없는지가 label로 달려있다.
아래 예시의 첫 번째 문장에서 “it” had a hole의 it이 “The carrot”으로 바뀐 문장이 두 번째 문장이고 이 두 문장의 관계가 entailment 되어 있으므로 label 1이 달린다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I stuck a pin through a carrot. When I pulled the pin out, it had a hole.       
The carrot had a hole.  
1

John was jogging through the park when he saw a man juggling watermelons. He was very impressive.       
John was very impressive.       
0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;testset-and-metrics-8&quot;&gt;testset and metrics&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://gluebenchmark.com/faq&quot;&gt;GLUE FAQ&lt;/a&gt;의 12번 문항에는 WNLI에서 이상한 결과를 얻을 수 있는 이유가 적혀있다.
같은 문장이 포함된 다른 example 끼리는 반대의 label이 달려있는데 이 때문에 training set에 overfit된 모델은 dev set에서 성능이 매우 나쁠 수 있다는 것이다.
실제로 &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt;는 이 이유로 WNLI의 성능은 report 하지 않았다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-download&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;2. Download&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/nyu-mll/jiant/blob/master/scripts/download_glue_data.py&quot;&gt;링크&lt;/a&gt;에 있는 python script를 다운로드한 이후 실행시키면 된다.
지정한 dir에 전체 task를 받을 수도 있고 일부 task만 받을 수도 있다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python download_glue_data.py &lt;span class=&quot;nt&quot;&gt;--data_dir&lt;/span&gt; data &lt;span class=&quot;nt&quot;&gt;--tasks&lt;/span&gt; all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-leaderboard&quot;&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;3. Leaderboard&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;여태까지 제출한 모델의 성능은 &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;&gt;GLUE leaderboard&lt;/a&gt;에 정리되어있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/glue_leaderboard.png&quot; width=&quot;95%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;2019.12.22 기준 top 3&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Leaderboard에는 순기능과 역기능이 모두 공존하지만, 아직까지는 순기능이 더 많다고 생각한다.
상대적으로 공정하게 비교할 수 있는 데이터셋이고 덕분에 다양한 Language Model이 주목받을 수 있었기 때문이다.
너무 낡아버리기 전에 새로운 데이터셋이 나와야한다고도 생각했는데, Neurips 2019에 “Stickier Benchmark”라는 부제와 함께 &lt;a href=&quot;http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems&quot;&gt;SuperGLUE&lt;/a&gt;가 등장했다!!&lt;/p&gt;

&lt;p&gt;이로 인해 열릴 새로운 LM들의 등장을 기대해본다 :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.12471.pdf&quot;&gt;https://arxiv.org/pdf/1805.12471.pdf&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs&quot;&gt;https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/N18-1101.pdf&quot;&gt;https://www.aclweb.org/anthology/N18-1101.pdf&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 22 Dec 2019 22:22:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/12/22/GLUE-benchmark</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/12/22/GLUE-benchmark</guid>
        
        
        <category>machine-learning</category>
        
        <category>nlp</category>
        
        <category>dataset</category>
        
        <category>머신러닝</category>
        
        <category>자연어처리</category>
        
        <category>데이터셋</category>
        
      </item>
    
      <item>
        <title>수학으로 이해하는 양자컴퓨터의 기초</title>
        <description>&lt;p&gt;최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은 전세계적으로 큰 화제였다.
물 들어올 때 노 저으라고 하지 않았던가, 이 때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을 넘나들며 관련 지식을 습득해보았다.&lt;/p&gt;

&lt;p&gt;아마 나와 같이 관련 기사나 여러 블로그 글, 유튜브 등을 찾아본 사람들이라면 어렵지 않게 아래의 정보는 얻었을 것이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;n개의 qbit은 bit와 달리 &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt;의 state를 표현할 수 있다.&lt;/li&gt;
  &lt;li&gt;superposition이란 동시에 0과 1의 상태를 띠는 성질로, 병렬연산이 가능해져서 고전컴퓨터에 비해 계산 속도의 이점이 생긴다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;텍스트만 보면 “아 그렇구나.” 싶은 내용들이다. 
이해가 된 것일까 싶었지만 스스로에게 세 질문을 던졌을 때 답하지 못하는 것을 보며 제대로 이해하지 못했음을 인지했다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;    &lt;strong&gt;Q1.&lt;/strong&gt; n개의 bit로도 &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt;을 표현할 수 있는거 아닌가? 3개의 bit가 000, 001, 010, 011, 100, 101, 110, 111 이렇게 8개의 상태를 표현할 수 있으니까. &lt;br /&gt;
    &lt;strong&gt;Q2.&lt;/strong&gt; 양자 세계는 불확정성 원리에 지배받는다고 하는데, 대체 양자컴퓨터로 어떻게 연산하고 있는 것이며, 이 성질이 어떻게 계산 비용을 감소시킬 수 있는걸까? &lt;br /&gt;
    &lt;strong&gt;Q3.&lt;/strong&gt; Entanglement는 qbit들이 어떻게 된 상태인거지?&lt;/p&gt;

&lt;p&gt;이 질문들에 제대로 답하기 위해서는 수학이 필요하다는 생각이 들었다.
4차원 이상의 공간을 제대로 시각화하지 못하듯이 양자 세계를 자연어로 표현한다는 것 자체가 말이 되지 않는 것 같았기 때문이다.
그래서 수학으로 설명된 자료를 찾으려고 부던히 애를 썼고, 끝내 “내 수준에 맞는” (= 이 글을 읽을 모두가 다 이해할 수 있는) 수학으로 설명된 자료를 찾았다.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/F_Riqjdh2oM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&quot;https://speakerdeck.com/ahelwer/quantum-computing-for-computer-scientists&quot;&gt;발표자료&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이 영상을 보는 것을 추천하지만 무려 한시간이 넘는지라 글로도 정리를 해보았다. 아래에 기술된 내용은 내 방식대로 위의 영상과 자료를 재구성한 것이다.&lt;/p&gt;

&lt;p&gt;사실 이 자료를 다 보더라도 양자컴퓨터에 대해 많은 것을 알았다고 보긴 어렵다.
python을 처음 접한 사람이 &lt;code class=&quot;highlighter-rouge&quot;&gt;print('Hello World!')&lt;/code&gt;를 성공했다고 해서 python을 잘 알았다고 하기 어려운 것처럼.
그리고 딥러닝에 관심있는 사람이 tutorial을 따라해보며 CNN을 돌려봤다고 해서 딥러닝을 잘 알았다고 하기 어려운 것처럼.&lt;/p&gt;

&lt;p&gt;그렇지만 양자 세계에 한 번은 &lt;code class=&quot;highlighter-rouge&quot;&gt;'Hello World!'&lt;/code&gt;를 날려봐야 하지 않을까?&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#1-introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#2-basics&quot;&gt;2. Basics&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#21-qubit--qbit&quot;&gt;2.1 Qubit / Qbit&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#22-superposition&quot;&gt;2.2 Superposition&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#23-tensor-product&quot;&gt;2.3 Tensor product&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#24-1-bit-operations&quot;&gt;2.4 1-bit operations&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#25-cnot-one-of-the-2-bit-operations&quot;&gt;2.5 CNOT (one of the 2-bit operations)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#3-the-deutsch-jozsa-problem-&quot;&gt;3. The Deutsch-Jozsa problem&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#31-classical-computer&quot;&gt;3.1 Classical computer&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#32-quantum-computer&quot;&gt;3.2 Quantum computer&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#4-entanglement&quot;&gt;4. Entanglement&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#5-conclusion&quot;&gt;5. Conclusion&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;The Deutsch-Jozsa problem 이라는 아주 간단한 문제를 통해 양자컴퓨터가 고전컴퓨터에 비해 어떻게 연산 속도에서 이점을 보이는지 알아보려고 한다.
이 과정을 이해하기 위해 양자컴퓨터가 연산하는 방법에 대해 소개할 것이며 matrix 연산과 기초적인 논리회로에 대한 내용을 짚고 넘어갈 것이다.&lt;/p&gt;

&lt;p&gt;추가로, entanglement에 대한 간단한 설명이 있다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-basics&quot;&gt;2. Basics&lt;/h2&gt;

&lt;h3 id=&quot;21-qubit--qbit&quot;&gt;2.1 Qubit / Qbit&lt;/h3&gt;

&lt;p&gt;Qubit 혹은 Qbit은 양자컴퓨터 계산의 기본적인 단위이다. 조금이라도 양자컴퓨터에 대해 알아본 사람들이라면 qbit은 지겹도록 보고 들었을 것이다.&lt;/p&gt;

&lt;p&gt;Qbit은 언제나 다음의 조건을 만족시킨다. &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A qbit, represented by 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
 \alpha \\
 \beta
 \end{pmatrix}&lt;/script&gt;
 where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; are complex numbers must be constrained by the equation 
&lt;script type=&quot;math/tex&quot;&gt;||\alpha||^2 + ||\beta||^2 = 1&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;따라서 아래의 예시들은 qbit에 해당된다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
 \end{pmatrix}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
\frac{1}{2} \\
\frac{\sqrt{3}}{2}
 \end{pmatrix}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
 1 \\
 0
 \end{pmatrix}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
 0 \\
 -1
 \end{pmatrix}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;그리고 이 모든 벡터들의 basis가 되는 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
 1 \\
 0
 \end{pmatrix}&lt;/script&gt;
 과 
 &lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
 0 \\
 1
 \end{pmatrix}&lt;/script&gt;
은 각각 &lt;script type=&quot;math/tex&quot;&gt;\mid 0\rangle&lt;/script&gt; 과 &lt;script type=&quot;math/tex&quot;&gt;\mid 1\rangle&lt;/script&gt; 이라는 특별한 기호로 정의한다.&lt;/p&gt;

&lt;h3 id=&quot;22-superposition&quot;&gt;2.2 Superposition&lt;/h3&gt;

&lt;p&gt;양자컴퓨터의 qbit을 설명할 때 빠지지 않는 성질이다. 
“동시에 0과 1을 가진다.”는 문장으로 자주 설명되지만 이보다는 슈뢰딩거의 고양이 느낌이 물씬 나는 “When we measure a qbit, it collapses to an actual value of 0 or 1.” 이라는 문장이 더 좋은 설명인 것 같다.&lt;/p&gt;

&lt;p&gt;위에서 qbit이라고 언급했던 벡터 하나를 예시로 들어보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{pmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
 \end{pmatrix}&lt;/script&gt;

&lt;p&gt;이 qbit은 &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; 혹은 &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; 로 collapse될 확률이 (
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2} = || \frac{1}{\sqrt{2}} || ^2&lt;/script&gt;
) 이다.&lt;/p&gt;

&lt;p&gt;감사하게도 &lt;a href=&quot;https://quantum-computing.ibm.com/&quot;&gt;IBM은 자사의 양자컴퓨터를 사용할 수 있는 API&lt;/a&gt;를 만들어 놓았다. 
여기서 이 qbit을 만들고 1024번 관측해보면 0과 1이 50%씩 나오는 것을 확인할 수 있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/qubit_1_2_example.png&quot; height=&quot;200px&quot; style=&quot;margin-right:20px&quot; /&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/qubit_1_2_result.png&quot; height=&quot;200px&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;(왼) H gate를 통해 |0&amp;gt; 은 예시로 든 qbit으로 바뀐다. (이 내용은 밑에서 다룬다.) (오) 1024번 관측한 결과다. 50% 확률로 0과 1을 나타낸다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
\frac{1}{2} \\
\frac{\sqrt{3}}{2}
 \end{pmatrix}&lt;/script&gt;
은 &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; 으로 collapse 될 확률이 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{4}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; 로 collapse 될 확률이 &lt;script type=&quot;math/tex&quot;&gt;\frac{3}{4}&lt;/script&gt;인 qbit이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;|0\rangle&lt;/script&gt; 
은 0으로만 collapse 한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/qubit_0_example.png&quot; height=&quot;200px&quot; style=&quot;margin-right:20px&quot; /&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/qubit_0_result.png&quot; height=&quot;200px&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;(왼) |0&amp;gt; (오) 1024번 관측한 결과로, 100% 0으로 관측된다. &lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-tensor-product&quot;&gt;2.3 Tensor product&lt;/h3&gt;

&lt;p&gt;여러 개의 qbit을 나타내기 위해 Tensor product 개념이 필요하다.
수학적으로 엄밀한 표현은 아니지만, n개의 qbit 연산을 표현하기 위해서는 아래의 표기 방식을 따르는 것이 좋다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{x_0}{x_1} \otimes \binom{y_0}{y_1} 
= \begin{pmatrix} x_0  \binom{y_0}{y_1} \\ 
{x_1  \binom{y_0}{y_1}} \end{pmatrix} 
= \begin{pmatrix} x_0 y_0 \\ x_0 y_1 \\ x_1 y_0 \\ x_1 y_1 \end{pmatrix}&lt;/script&gt;

&lt;p&gt;이를 응용하면 2개, 3개의 qbit도 벡터처럼 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|01\rangle = \binom{1}{0} \otimes \binom{0}{1} = \begin{pmatrix}
  0 \\ 1 \\ 0 \\ 0
\end{pmatrix}
\hspace{10pt}
|100\rangle = \binom{0}{1} \otimes \binom{1}{0} \otimes \binom{1}{0} = \begin{pmatrix}
  0\\ 0\\0\\0 \\ 1 \\ 0 \\ 0 \\ 0
\end{pmatrix}&lt;/script&gt;

&lt;p&gt;이와 같이 tensor product의 결과로 표현된 벡터는 product state라고 한다.
여기서 우리는 n개 qbit의 product state 크기가 &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt; 이라는 것을 알 수 있다.
만약 
&lt;script type=&quot;math/tex&quot;&gt;\binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} \otimes \binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} = 
\begin{pmatrix}
 \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2}
\end{pmatrix}&lt;/script&gt;
의 multiple qbits 이 있다면 &lt;script type=&quot;math/tex&quot;&gt;\mid 00\rangle&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mid 01\rangle&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mid 10\rangle&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\mid 11\rangle&lt;/script&gt; 으로 collapse될 확률이 모두 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{4}&lt;/script&gt;이므로 동시에 4개의 state를 표현할 수 있게 된다. 
즉, qbit이 bit와는 다르게 &lt;script type=&quot;math/tex&quot;&gt;2^n&lt;/script&gt; 개의 state를 표현할 수 있다고 한 것은 &lt;em&gt;동시에&lt;/em&gt; 가질 수 있는 최대 state 관점에서 비교한 것이다.
bit는 절대로 동시에 2개 이상의 state를 가질 수 없으므로 한 번에 계산할 수 있는 정보는 1개 뿐이다.&lt;/p&gt;

&lt;p&gt;또한 product state는, 뒤의 entanglement와 구분되는 중요한 성질로, &lt;strong&gt;독립적인 state들로 factorize가 가능&lt;/strong&gt;하다는 점이 있다.&lt;/p&gt;

&lt;p&gt;Multiple qbits의 product state 또한 single qbit과 같은 성질을 만족시킨다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{a}{b} \otimes \binom{c}{d} = 
\begin{pmatrix}
  ac \\ ad \\ bc \\ bd
\end{pmatrix}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;24-1-bit-operations&quot;&gt;2.4 1-bit operations&lt;/h3&gt;

&lt;p&gt;1-bit에서 가능한 연산은 Identity, Negation, Constant-0, Constant-1의 총 4가지가 있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/1bit_operations.png&quot; width=&quot;40%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;1-bit 연산의 4 종류&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;각각의 연산은 matrix로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\text{Identity} = \begin{pmatrix}
1 &amp; 0 \\ 
0 &amp; 1
\end{pmatrix} %]]&gt;&lt;/script&gt;, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\text{Negation} = \begin{pmatrix}
0 &amp; 1 \\ 
1 &amp; 0
\end{pmatrix} %]]&gt;&lt;/script&gt;, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\text{Contant-0} = \begin{pmatrix}
1 &amp; 1 \\ 
0 &amp; 0
\end{pmatrix} %]]&gt;&lt;/script&gt;, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\text{Contant-1} = \begin{pmatrix}
0 &amp; 0 \\ 
1 &amp; 1
\end{pmatrix} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/1bit_matrix.png&quot; width=&quot;70%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;1-bit 연산의 matrix 표현&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;25-cnot-one-of-the-2-bit-operations&quot;&gt;2.5 CNOT (one of the 2-bit operations)&lt;/h3&gt;

&lt;p&gt;CNOT 연산은 control bit와 target bit로 구성된 2-bit가 있을 때 control bit가 0이면 target bit를 바꾸지 않고, control bit가 1일 때 target bit를 바꾸는 연산이다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/CNOT_operation.png&quot; width=&quot;20%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;CNOT&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;마찬가지로 이 연산도 matrix로 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
C = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\ 
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 1 \\ 
0 &amp; 0 &amp; 1 &amp; 0 \\ 
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/CNOT_examples.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;2-qbits에 적용한 CNOT 예시&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;#24-1-bit-operations&quot;&gt;2.4&lt;/a&gt;와 &lt;a href=&quot;#25-cnot-one-of-the-2-bit-operations&quot;&gt;2.5&lt;/a&gt;에서 operation들을 matrix화 한 것에 주목하자. 
확률이 지배하는 양자 세계에서 deterministic한 연산을 하기 위해서는 matrix를 관측하지 않은 qbit에 곱하는 것이 유일한 방법이기 때문이다.
아래의 예시에서 우리가 확신할 수 있는 정보는 qbit이 0과 1로 관측될 확률이 반대가 되었다는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
0 &amp; 1 \\ 
1 &amp; 0
\end{pmatrix} \begin{pmatrix}
  \frac{1}{2} \\ \frac{\sqrt{3}}{2} 
\end{pmatrix} = \begin{pmatrix}
  \frac{\sqrt{3}}{2} \\ \frac{1}{2}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;항상 0 혹은 1로 관측되는 &lt;script type=&quot;math/tex&quot;&gt;\mid0\rangle&lt;/script&gt;이나 &lt;script type=&quot;math/tex&quot;&gt;\mid1\rangle&lt;/script&gt;을 쓰면 matrix 연산을 고집하지 않아도 되지만 이런 qbit만 사용할거라면 고전컴퓨터를 쓰면 그만이다.
굳이 0K 가까이 되는 험악한 조건을 유지해가며 계산할 필요가 없다.&lt;/p&gt;

&lt;p&gt;그래서 matrix 연산은 양자 컴퓨팅에서 굉장히 중요하다. 여기에는 한가지 추가조건이 있는데, 반드시 연산에 사용되는 matrix는 &lt;strong&gt;reversible&lt;/strong&gt;해야한다는 것이다.
따라서 앞서 본 1-bit 연산 중 Constant-1과 Constant-0를 계산하기 위해서는 단순 matrix를 곱하는 것 외의 다른 방법이 필요하다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-the-deutsch-jozsa-problem-&quot;&gt;3. The Deutsch-Jozsa problem &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;이 문제는 양자컴퓨터가 고전컴퓨터에 비해 계산적인 이점을 가지는 아주 간단한 (&lt;del&gt;&lt;em&gt;동시에 쓸데없는&lt;/em&gt;&lt;/del&gt;) 문제다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1-bit를 입력받아서 1-bit를 내뱉는 어떤 함수가 있다고 하자. 
만약 이 함수가 Constant(Contant-0, Constant-1)인지, 아니면 Variable(Identity, Negation)인지 알기 위해서는 최소 몇 번의 query를 날려야 할까?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;31-classical-computer&quot;&gt;3.1 Classical computer&lt;/h3&gt;

&lt;p&gt;고전컴퓨터에서는 0과 1을 입력해야하므로 총 두 번의 연산이 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;32-quantum-computer&quot;&gt;3.2 Quantum computer&lt;/h3&gt;

&lt;p&gt;예상했듯이 정답은 한 번이다. 왜인지 알기위해서는 추가적인 개념이 필요하다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;--hadamard-gate&quot;&gt;- Hadamard gate&lt;/h4&gt;

&lt;p&gt;앞서 언급된 적 있는 H gate이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H = \begin{pmatrix}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Hadamard gate는 0- 혹은 1-qbit을 받아서 0과 1을 같은 확률로 가지는 qbit으로 바꿔준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H\mid0\rangle = \begin{pmatrix}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
  1 \\ 0
\end{pmatrix} = \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H\mid1\rangle = \begin{pmatrix}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
  0 \\ 1
\end{pmatrix} = \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Hadamard gate는 또 다른 중요한 성질이 있다. 0과 1을 같은 확률로 가지는 qbit을 다시 0- 과 1-qbit으로 돌려보낸다는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix} = \begin{pmatrix}
  1 \\ 0
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
  \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
  \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}
\end{pmatrix} \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}
\end{pmatrix} = \begin{pmatrix}
  0 \\ 1
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;--x-gate&quot;&gt;- X gate&lt;/h4&gt;

&lt;p&gt;X gate는 qbit의 위 아래를 바꿔준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X = 
\begin{pmatrix}
   0 &amp; 1 \\
   1 &amp; 0
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
   0 &amp; 1 \\
   1 &amp; 0
\end{pmatrix} 
\begin{pmatrix}
  0 \\ 1
\end{pmatrix}
= \begin{pmatrix}
  1 \\ 0
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
   0 &amp; 1 \\
   1 &amp; 0
\end{pmatrix} 
\begin{pmatrix}
  \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{pmatrix}
= \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;H gate와 X gate 연산을 이해하기 쉽게 표현하면 아래의 그림이 된다. 붉은색이 X gate, 노란색이 H gate 연산의 방향이다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/gates-visualization.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  1 \\ 0
\end{pmatrix}&lt;/script&gt;
에 X - H - X - H - X gate를 씌운 결과는 그림으로 보면 더 쉽게 이해된다. 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  1 \\ 0
\end{pmatrix}&lt;/script&gt;
에서 출발해 각 gate가 연산되는 방향으로 화살표를 움직이면 마지막에 도달하는 곳이 결과값이 된다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_logic_operation_example.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;--non-reversible-matrix&quot;&gt;- non-reversible matrix&lt;/h4&gt;

&lt;p&gt;앞서 양자컴퓨터는 non-reversible한 matrix 를 곱하는 연산은 불가능하다고 했다.
1-bit 연산 중에서 Constant-0과 Constant-1은 non-reversible하다. 그래서 양자 컴퓨팅에서는 2개의 qbit을 사용한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_non_reversible.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 그림의 input과 output notation을 보면 “응?” 이라는 생각이 절로 들 것이다. 영상에서도 사람들이 대체 왜 “Output”이 Input 쪽에 가 있는지에 대해 끊임없이 묻는다.
아쉽게도 강연자는 속시원하게 답변을 해주지 않아서 그런가보다 하고 넘어갔는데 다시 보니 이해가 된 부분이 있어 글로 설명해보려고 한다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 실제 1-bit 연산의 input과 output을 나타낸다. 
그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output&lt;/code&gt;은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 BB 이후에 있기 때문에 BB 이전에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 1-bit 연산의 input과 output이 되도록 넣어주는, 양자컴퓨터 연산 방식 때문에 필요한 input들이다.&lt;/p&gt;

&lt;p&gt;이 약속에 따라서 양자컴퓨터가 1-bit 연산을 어떻게 수행하는지 아래의 예시를 통해 좀 더 이해해보자.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_constant_0.png&quot; height=&quot;200px&quot; width=&quot;300px&quot; style=&quot;margin-right:20px; margin-bottom:10px&quot; /&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_constant_1.png&quot; height=&quot;200px&quot; width=&quot;300px&quot; style=&quot;margin-bottom:10px&quot; /&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_identity.png&quot; height=&quot;200px&quot; width=&quot;300px&quot; style=&quot;margin-right:20px; margin-bottom:10px&quot; /&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_negation.png&quot; height=&quot;200px&quot; width=&quot;300px&quot; style=&quot;margin-bottom:10px&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;왼쪽 위 부터 차례대로 Constant-0, Constant-1, Identity, Negation. Identity와 Negation에 있는 이상한 operation은 CNOT이다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Constant-0은 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;일 때와 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;일 때 모두 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;이어야 한다. 
어떤 gate도 없는 왼쪽 위 그림의 회로에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input&lt;/code&gt;에 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt; 혹은 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;을 대입해보면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 Constant-0의 관계를 가지는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;Indentity는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;일 때는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;이고 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;일 때는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;인 함수다. 
왼쪽 아래 그림의 회로는 CNOT gate를 표현하고 있다. 색이 채워진 원이 control bit 쪽을 나타내고 그렇지 않은 쪽 원은 target bit를 나타낸다.
&lt;code class=&quot;highlighter-rouge&quot;&gt;Input&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt; 이면 control bit가 0이므로 target bit도 그대로 유지한다. 그래서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;도 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;도 &lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;이 된다.
&lt;code class=&quot;highlighter-rouge&quot;&gt;Input&lt;/code&gt;이 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt; 이면 control bit가 1이므로 target bit가 바뀐다. 그래서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Input'&lt;/code&gt;도 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Output'&lt;/code&gt;도 &lt;script type=&quot;math/tex&quot;&gt;\mid 1 \rangle&lt;/script&gt;이 된다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;그럼 다시 The Deutsch-Jozsa problem로 돌아가서, 양자컴퓨터에서는 어떻게 한 번에 구할 수 있을까? 정답은 아래의 그림이 설명해준다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_one_query.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;양자컴퓨터가 한 번에 문제를 푸는 법&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
이 연산대로라면 BB가 Constant(Contant-0, Constant-1)이었을 경우, 측정 결과가 &lt;script type=&quot;math/tex&quot;&gt;\mid11\rangle&lt;/script&gt;이고, Variable(Identity, Negation)이었을 경우에는 &lt;script type=&quot;math/tex&quot;&gt;\mid01\rangle&lt;/script&gt;이 된다.&lt;/p&gt;

&lt;p&gt;BB의 경우의 수를 따져가며 이해해보자.&lt;/p&gt;

&lt;h4 id=&quot;preprocessing-bb-입력-직전까지의-연산&quot;&gt;preprocessing (BB 입력 직전까지의 연산)&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_preprocessing.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;BB에 들어가기 전 input (&lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;) 과 output qbit (&lt;script type=&quot;math/tex&quot;&gt;\mid 0 \rangle&lt;/script&gt;) 모두 X와 H gate를 거쳐서
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  \frac{-1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
가 된다.&lt;/p&gt;

&lt;h4 id=&quot;case-1-bb가-constant-0-이었을-경우&quot;&gt;case 1) BB가 Constant-0 이었을 경우&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;
Constant-0은 input과 output에 어떤 gate도 씌우지 않는다. 
따라서 BB가 Constant-0이었을 때 Input과 Output은 H gate만 통과한 이후 관측된다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/const_0.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Constant-0&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_bb_const_0.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;BB가 Constant-0인 경우 Input'과 Output'&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;case-2-bb가-contstant-1-이었을-경우&quot;&gt;case 2) BB가 Contstant-1 이었을 경우&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;
Constant-1은 output에만 X gate를 적용한다. 
따라서 BB가 Constant-1이었을 때는 Output에 X gate가 추가되고, 이후 Input과 Output 모두에 H gate가 적용된다.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/const_1.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Constant-1&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_bb_const_1.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;BB가 Constant-1인 경우 Input'과 Output'&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;case-3-bb가-identity-이었을-경우&quot;&gt;case 3) BB가 Identity 이었을 경우&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;
Identity는 CNOT gate를 통해 연산된다.
앞에서 CNOT 연산은 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\ 
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 1 \\ 
0 &amp; 0 &amp; 1 &amp; 0 \\ 
\end{pmatrix} %]]&gt;&lt;/script&gt;
을 곱하는 것과 같다고 설명했다.
Preprocessing을 거친 Input과 Output은 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  \frac{-1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
이므로 CNOT연산은 아래와 같이 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
C \begin{pmatrix}
  \begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{-1}{\sqrt{2}}
  \end{pmatrix} \otimes
    \begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{-1}{\sqrt{2}}
  \end{pmatrix}
\end{pmatrix} = C \begin{pmatrix}
  \frac{1}{2} \\
  \frac{-1}{2} \\
  \frac{-1}{2} \\
  \frac{1}{2}
\end{pmatrix} = \frac{1}{2} \begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 \\
  0 &amp; 0 &amp; 1 &amp; 0 \\
\end{pmatrix} \begin{pmatrix}
  1 \\
  -1 \\
  -1 \\
  1
\end{pmatrix} = \frac{1}{2} \begin{pmatrix}
  1 \\
  -1 \\
  1 \\
  -1
\end{pmatrix} = \begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{1}{\sqrt{2}}
  \end{pmatrix} \otimes
    \begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{-1}{\sqrt{2}}
  \end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;즉 Input은 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{-1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
에서 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt; 
로 바뀌고 Output은 그대로 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{-1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
이 된다.
이 상태에서 H gate가 각각 적용되어 최종 결과는 &lt;script type=&quot;math/tex&quot;&gt;\mid 01 \rangle&lt;/script&gt;이 된다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/identity.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Identity&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_bb_identity.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;BB가 Identity인 경우 Input'과 Output'&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;case-4-bb가-negation-이었을-경우&quot;&gt;case 4) BB가 Negation 이었을 경우&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt; 
Negation은 Indentity의 결과 중 Output에만 X gate가 추가되는 연산이다. 
따라서 아래 그림처럼 연산이 이루어지고 Identity와 마찬가지로 최종 결과는 &lt;script type=&quot;math/tex&quot;&gt;\mid 01 \rangle&lt;/script&gt;이 된다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/negation.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Negation&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/quantum_bb_negation.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;BB가 Negation인 경우 Input'과 Output'&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;정리하면, 양자컴퓨터에서는 특정 설계 상황에서 고정된 BB input에 대한 BB output을 “한 번”만 관측하면 BB가 Constant인지 Variable인지 확인할 수 있다는 것이다!&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-entanglement&quot;&gt;4. Entanglement&lt;/h2&gt;

&lt;p&gt;Entanglement는 지금까지의 흐름에서는 동떨어진 이야기지만 양자컴퓨터에서 항상 소개되는 내용이기 때문에 추가하였다.&lt;/p&gt;

&lt;p&gt;앞서 qbit과 product state의 성질을 수학적으로 나타낸 것처럼 entanglement도 수학적인 성질로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  0 \\
  0 \\
  \frac{1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
는 entangle된 qbit인데, 그 모양새가 product state와 닮아있다. 하지만 product state와는 중요한 성질에서 차이를 보인다.
위에서 설명했듯이 product state는 개별적인 qbit으로 factorize된다. 하지만 entanlged qbit은 개별적인 qbit으로 factorize 되지 않는다. 
(If the product state of two qbits &lt;strong&gt;cannot be factored&lt;/strong&gt;, they are said to be &lt;strong&gt;entanlged&lt;/strong&gt;.)
이 때문에 entangled qbit은 차원이 늘어난 하나의 qbit으로 볼 수 있으며 일부를 관측했을 때 나머지 일부의 상태가 유추된다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  0 \\
  0 \\
  \frac{1}{\sqrt{2}}
\end{pmatrix}&lt;/script&gt;
이 entangle 되었음을 증명하는 것은 간단하다. 
&lt;script type=&quot;math/tex&quot;&gt;\begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  0 \\
  0 \\
  \frac{1}{\sqrt{2}}
\end{pmatrix} = \begin{pmatrix}
  a \\
  b
\end{pmatrix} \otimes \begin{pmatrix}
  c \\
  d
\end{pmatrix}&lt;/script&gt;
를 만족하는 &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;는 존재하지 않기 때문에 이는 entanlge되어 있는 qbit이다.&lt;/p&gt;

&lt;p&gt;Entanlged qbit은 CNOT과 H gate를 통해 쉽게 생성할 수 있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/entanlge.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;Entangled qbit&lt;/figcaption&gt;
&lt;/div&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
CH_1
\begin{pmatrix}
  \begin{pmatrix}
    1 \\
    0
  \end{pmatrix} \otimes \begin{pmatrix}
    1 \\
    0
  \end{pmatrix}
\end{pmatrix} = C \begin{pmatrix}
  \begin{pmatrix}
    \frac{1}{\sqrt{2}} \\
    \frac{1}{\sqrt{2}} 
  \end{pmatrix} \otimes \begin{pmatrix}
    1 \\ 0
  \end{pmatrix}
\end{pmatrix} = 
\begin{pmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 \\ 
0 &amp; 1 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0 &amp; 1 \\ 
0 &amp; 0 &amp; 1 &amp; 0 \\ 
\end{pmatrix} \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  0 \\
  \frac{1}{\sqrt{2}} \\
  0 
\end{pmatrix} = \begin{pmatrix}
  \frac{1}{\sqrt{2}} \\
  0 \\
  0 \\
  \frac{1}{\sqrt{2}}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;만약 이후에 이런 게이트의 조합을 본다면 곧바로 ‘entanlge 되었군!’ 이라고 생각하면 된다 :)&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt;

&lt;p&gt;개인적으로 이 영상을 본 이후, 속이 뻥 뚫리는 기분이 들었다. 아직 matrix로 표현되는 qbit이 물리적으로 어떤 모습인지, gate들이 물리적으로 어떻게 qbit에 적용되는지는 모르지만
(이건 실제 양자컴퓨터를 눈으로 보면 이해가 되지 않을까) 이 정도라도 양자컴퓨터와 고전컴퓨터의 연산과정에서의 차이를 구체적으로 알 수 있었기 때문에 만족할 수 있었다.&lt;/p&gt;

&lt;p&gt;양자컴퓨터의 연산 과정을 이해하고나니 양자 우월성은 그냥 달성되는 것은 아니었으며, 잘 설계된 gate가 뒷받침되었을 때 가능한 것임을 깨닫게 되기도 했다.&lt;/p&gt;

&lt;p&gt;이 정도면 양자 세계에 &lt;code class=&quot;highlighter-rouge&quot;&gt;'Hello World!'&lt;/code&gt;를 했다고 볼 수 있지 않을까?&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 07 Nov 2019 00:07:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/11/07/Basics-of-Quantum-Computings</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/11/07/Basics-of-Quantum-Computings</guid>
        
        
        <category>quantum-computer</category>
        
        <category>basic</category>
        
        <category>양자컴퓨터</category>
        
        <category>기초</category>
        
      </item>
    
      <item>
        <title>Naver News Comment Analysis (3)</title>
        <description>&lt;p&gt;이번 글은 &lt;strong&gt;뉴스 댓글 분석&lt;/strong&gt; 3편이다. 지난 글에서와 마찬가지로 궁금한 점이나 추가 분석 요청은 댓글로 남겨주면 좋을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은 불가능하다. 
그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은 내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까?&lt;/p&gt;

&lt;p&gt;네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을 상위에 랭크시키고 있다. 
각각의 알고리즘이 가진 결함도 문제지만, 과연 정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할 의견일까?
특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱 획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어 무의식적으로 다양한 사고에 대한 가능성을 차단받는다.&lt;/p&gt;

&lt;p&gt;그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sorting algorithm들을 제안한다.
reddit과 yelp 등에서 사용하고 있는 알고리즘을 비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고 비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#0-previous-work&quot;&gt;0. Previous Work&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#1-introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#2-naver-news-comment-sorting-system&quot;&gt;2. Naver News Comment Sorting System&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#21-sorting-algorithms&quot;&gt;2.1 Sorting Algorithms&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#22-limitations&quot;&gt;2.2 Limitations&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#3-reddit-comment-sorting-algorithms&quot;&gt;3. Reddit Comment Sorting Algorithms&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#31-best--&quot;&gt;3.1 Best&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#32-controversial-&quot;&gt;3.2 Controversial&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#4-new-sorting-algorithms&quot;&gt;4. New Sorting Algorithms&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#5-conclusions&quot;&gt;5. Conclusions&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#6-future-works&quot;&gt;6. Future works&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#appendix-a-wilson-score&quot;&gt;Appendix A: Wilson score&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;0-previous-work&quot;&gt;0. Previous Work&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/2019/07/25/Naver-News-Comments-Analysis-(1)&quot;&gt;Naver News Comment Analysis (1)&lt;/a&gt;에서는 수집한 뉴스와 댓글 데이터의 전체 통계량을 보며 간단히 데이터를 이해해보았다. &lt;br /&gt;
&lt;a href=&quot;/2019/08/03/Naver-News-Comments-Analysis-(2)&quot;&gt;Naver News Comment Analysis (2)&lt;/a&gt;에서는 어뷰저를 찾기 위해 네이버 뉴스에 빈번하게 순위권에 오르는 유저 중에서 비정상적으로 빈번하게 오른 유저를 분석하였다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;모두가 다 알고 있는 사실이지만, 어뷰저는 존재한다. 드루킹과 지난 분석 글에서 나온 결론으로도 뒷받침될 수 있지만 트위터에 &lt;code class=&quot;highlighter-rouge&quot;&gt;m.news.naver.com/comment&lt;/code&gt; 라고 검색하기만 해도 아래와 같이 댓글 조작의 흔적을 쉽게 발견할 수 있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/twitter.png&quot; width=&quot;60%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;https://twitter.com/search?q=m.news.naver.com%2Fcomment&amp;amp;src=recent_search_click&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇듯 쉽게 어뷰저의 존재를 찾을 수 있음에도 네이버가 어뷰저를 잡지 않는 이유는 그 일이 생각처럼 쉬운 일이 아니기 때문이다.&lt;/p&gt;

&lt;p&gt;n초 안에 여러번 공감과 비공감을 지속적으로 받은 댓글은 어뷰징의 결과로 의심한다. 그 댓글을 지워야 할까? 만약 댓글을 쓴 유저가 어뷰저가 아니었다면 문제가 될 수 있다.&lt;/p&gt;

&lt;p&gt;사후 분석을 통해 어뷰저로 의심되는 댓글의 내용을 지우는 방법은 어떨까? 뉴스라는 매체의 특성 상 시간이 지난 기사는 사람들이 관심있게 보지 않는다. 그러므로 이 방법은 어뷰저를 막는다고 볼 수 없다.&lt;/p&gt;

&lt;p&gt;분석을 통해 어뷰저라고 강하게 의심되는 유저를 차단한다고 하더라도 새로운 패턴으로 어뷰징을 하는 유저들이 생겨날 것이다. 어뷰저의 기준을 세우는 것은 어려운 반면 새로운 방식으로 어뷰징을 하는 것은 좀 더 쉽기 때문에 이렇게 물고 물리는 싸움은 어뷰저에게 유리하다.&lt;/p&gt;

&lt;p&gt;그렇다면 어뷰저를 차단하는 것에만 집중하지 말고, 어뷰징은 내버려두되 그 효과를 완화시키는 방법은 어떨까? &lt;strong&gt;지금 네이버 뉴스 댓글 랭킹 방식은 그것이 미치는 영향력에 비해 너무 간단하고 단편적이다.&lt;/strong&gt; 구글의 검색 랭킹이 신뢰도를 가지고 있는 이유는 상위에 랭크된 글이 ‘조작’을 통해 만들어지지 않았다는 점 때문일 것이다. 그 이유는 정보가 되는 글에 대한 정보량, 품질 기준이 보다 엄격하고 단편적인 면으로만 순위를 매기지 않기 때문이다. 만약 구글 랭킹이 웹문서의 클릭수로만 되어 있었다면 어땠을까? 많은 기업들이 본인의 홈페이지를 상위에 랭크시키기 위해 많은 조작이 일어났을 것이다.&lt;/p&gt;

&lt;p&gt;그래서 이번 글에서는 그렇게 간단하다고는 볼 수 없는 다른 랭킹 algorithm에 대해 소개해보려고 한다. 현재 네이버 뉴스 댓글 랭킹 방식 중 순공감순, 공감비율순, 답글순의 한계점을 살펴보고 reddit과 yelp에서 신뢰도있게 쓰이는 best 랭킹과 새로운 관점의 controversial 랭킹 algorithm을 소개한다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-naver-news-comment-sorting-system&quot;&gt;2. Naver News Comment Sorting System&lt;/h2&gt;

&lt;h3 id=&quot;21-sorting-algorithms&quot;&gt;2.1 Sorting Algorithms&lt;/h3&gt;

&lt;p&gt;2019년 9월 기준, 총 5개의 정렬방으로 서비스되고 있다. 드루킹 논란 이후 댓글 제공 여부와 정렬방식을 언론사가 선택하는 방식으로 바뀌었다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;순공감순&lt;/strong&gt;: 공감 - 비공감 &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;공감비율순:&lt;/strong&gt; 공감 / (공감 + 비공감)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;답글순&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;최신순&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;과거순&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 중, 댓글에 대한 사용자의 인터랙션(공감, 비공감, 답글)으로 순위를 매기는 순공감순, 공감비율순, 답글순에 대한 문제점을 하나씩 짚어보고자 한다.&lt;/p&gt;

&lt;h3 id=&quot;22-limitations&quot;&gt;2.2 Limitations&lt;/h3&gt;

&lt;h4 id=&quot;순공감순&quot;&gt;순공감순&lt;/h4&gt;

&lt;p&gt;순공감순은 우리의 직관과 벗어나는 랭킹이라는 점에서 한계가 있다. 우리는 절대적인 공감 수치보다, 공감비율로 댓글의 신뢰도를 평가한다.&lt;/p&gt;

&lt;p&gt;아래의 사례는 네이버 뉴스 댓글&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;의 실제 예시이다. 첫번째 댓글은 순공감 344개(= 455 - 111) 로, 300개(= 316 - 16)의 순공감을 지니는 두번째 댓글보다 더 높은 순위에 자리한다. 하지만 각각의 댓글의 공감비율은 80.4%(= 455 / (455 + 11)) 로, 두번째 댓글의 공감비율인 95.2% (= 316 / (316 + 16)) 보다 작다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/sgg_limit.png&quot; width=&quot;70%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;paid****: 순공감 344, 공감비율 80.4% | adam****: 순공감 300, 공감비율 95.2%&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;공감비율순&quot;&gt;공감비율순&lt;/h4&gt;

&lt;p&gt;앞서 설명한 것처럼 공감비율순이 좀 더 우리의 직관과 유사한 척도이다. 하지만 공감비율순은 전체 공감, 비공감 수가 적을 때 문제가 된다.&lt;/p&gt;

&lt;p&gt;소수의 사람들에게만 노출된 댓글은 공감과 비공감의 개수가 모두 적어 100% 라는 공감비율이 쉽게 만들어지는 반면, 여러 명에게 노출된 댓글은 하나의 비공감만 달리더라도 그보다 낮은 공감비율을 지니게 되는 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;아래의 네이버 뉴스 댓글 예시&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;에서 공감수가 20, 비공감수가 0인 댓글이 비공감을 전혀 받지 않아 공감비율 100%가 되어 더 많은 사람들이 읽고 공감을 표한 공감수 1021, 비공감수 58인 댓글보다 더 상단에 위치한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/ggratio_limit.png&quot; width=&quot;70%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;euic****, qkrs**** 공감비율 100% 공감+비공감수 20 | hang**** 공감비율 94.6% 공감+비공감수 1,079&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;답글순&quot;&gt;답글순&lt;/h4&gt;

&lt;p&gt;여러 개의 답글이 달리는 댓글은 주로 일찍 남겨진 댓글 중에 인신공격이나 뉴스 외 주제에 대한 댓글인 경우가 많다. 댓글 공간에서는 명확한 내용으로 구성된 댓글에 대해서는 대댓글 보다도 공감 혹은 비공감으로 본인의 주장을 표시하는 것이 일반적이다. 그러나 감정적으로 쓰여진 댓글은 그 댓글에 자극을 받은 다른 사용자의 답글로 이어지고 되므로 답글 개수를 기준으로 댓글을 정렬하면 뉴스 내용과는 무관한 자극적인 댓글들이 우선적으로 노출된다.&lt;/p&gt;

&lt;p&gt;또한 일찍 쓰여진 댓글일수록 더 많은 사람들에게 노출될 가능성이 있으므로 대부분 뉴스 작성 시점과 가까운 댓글이 상위에 랭크된다.&lt;/p&gt;

&lt;p&gt;랭킹 algorithm으로 보기에는 정렬 기준이 controllable하지 않으며 댓글의 유익한 속성이 높게 평가되어 정렬되는 랭킹이라고 볼 수 없다.&lt;/p&gt;

&lt;p&gt;아래의 네이버 뉴스 댓글 예시&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;를 보면 vote 수가 많지 않아도, 공감수가 전혀 없고 비공감만 받더라도 top 10에 위치할 수 있다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/replyCount_limit.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-reddit-comment-sorting-algorithms&quot;&gt;3. Reddit Comment Sorting Algorithms&lt;/h2&gt;

&lt;p&gt;댓글이 활발하게 생성되는 플랫폼은 비단 네이버 뉴스 뿐만은 아니다. 네이버 쇼핑, 네이버 호텔, 망고 플레이트,  reddit, stackoverflow, yelp, amazon 등의 다양한 플랫폼에서 수집되며 플랫폼에서는 다시 이 데이터를 가공하여 사용자에게 유익한 정보를 제공한다.&lt;/p&gt;

&lt;p&gt;그 중에서도 reddit의 랭킹 시스템이 앞서 비판했던 순공감순, 공감비율순의 한계를 극복한 sorting algorithm을 제공하고 있기에 자세히 살펴보려고 한다. reddit의 랭킹 방식에는 best, top, new, controversial, old, q&amp;amp;a가 있다. top이 순공감순, new가 최신순, old가 과거순이다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/reddit_sorting.png&quot; width=&quot;25%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;reddit의 sorting algorithms&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-best--&quot;&gt;3.1 Best &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Best ranking은 Wilson score&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;로 정렬한 것으로, 공감비율순의 단점으로 언급되었던, 전체 vote수가 적은 상황을 smoothing시켜준 algorithm이다. reddit뿐 아니라 yelp에서도 사용한다고 한다&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Wilson score는 주어진 positive와 negative vote가 binomial distribution을 따른다고 가정했을 때, positive 발생 확률을 95% 신뢰구간의 최소값으로 추정한 값이다.&lt;/p&gt;

&lt;p&gt;동전 뒤집기 상황에서 앞면을 positive, 뒷면을 negative라고 하자. n번 던진 후 앞면이 나올 확률(&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;)을 추정할 때 n이 충분히 큰 경우 central limit theorem에 의해 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;는 normal distribution을 따른다. 따라서 95%의 신뢰도로 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;를 추정하여 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;의 최소값, 최대값을 구할 수 있고 이 때 최소값이 Wilson score가 된다. 자세한 수식은 &lt;a href=&quot;#appendix-a-wilson-score&quot;&gt;Appendix A&lt;/a&gt;에 정리해두었다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^- = max\{0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 + 4n\hat{p}(1-\hat{p})}}{2(n+z^2)}\}=\text{wilson score}&lt;/script&gt;

&lt;p&gt;위의 식을 함수로 구현하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# ref: http://www.evanmiller.org/how-not-to-sort-by-average-rating.html
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.96&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 95% confidence level
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;denominator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;numerator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numerator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;denominator&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ZeroDivisionError&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아래의 예시는 네이버 뉴스 댓글에 Best ranking algorithm을 적용해본 결과이다. 
공감비율순 정렬이었다면 “&lt;em&gt;원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요&lt;/em&gt;“는 1000개 이상의 vote를 가진 “&lt;em&gt;법대로 해라 법은 만인 앞에 평등하다&lt;/em&gt;“는 댓글을 제치고 상위에 랭크되었을 것이다. 
하지만 Best 정렬방식에서는 vote 수가 적은 경우 약간의 penalty를 받기 때문에 하위에 랭크되었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;«MB ‘정치보복’ 반발에 문무일 총장 “법적 절차대로 하겠다”»&lt;/strong&gt; &lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;comments&lt;/th&gt;
      &lt;th&gt;공감수&lt;/th&gt;
      &lt;th&gt;비공감수&lt;/th&gt;
      &lt;th&gt;best score&lt;/th&gt;
      &lt;th&gt;공감비율&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;법대로 해라 법은 만인 앞에 평등하다&lt;/td&gt;
      &lt;td&gt;1091&lt;/td&gt;
      &lt;td&gt;55&lt;/td&gt;
      &lt;td&gt;0.938&lt;/td&gt;
      &lt;td&gt;0.952006980803&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;법대로 하면 사형인데 !!&lt;/td&gt;
      &lt;td&gt;562&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;0.936&lt;/td&gt;
      &lt;td&gt;0.935108153078&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;제발 법대로만 해주세요. 그래도 나라를 지옥으로 만든 죄는 물을 법도 없다. 이 악마야!!!&lt;/td&gt;
      &lt;td&gt;252&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0.933&lt;/td&gt;
      &lt;td&gt;0.947368421053&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;지금까지 반발하고 나서 살아남은 넘을 못봤다.&lt;/td&gt;
      &lt;td&gt;565&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
      &lt;td&gt;0.933&lt;/td&gt;
      &lt;td&gt;0.936981757877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;혓바닥몇번 낼름거릴까나했더니 찔렸나보네ㅎㅎ&lt;/td&gt;
      &lt;td&gt;595&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0.932&lt;/td&gt;
      &lt;td&gt;0.941455696203&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;본인이 구린짓을 했으니까 먼저 발광하는거겠지..&lt;/td&gt;
      &lt;td&gt;686&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
      &lt;td&gt;0.931&lt;/td&gt;
      &lt;td&gt;0.941015089163&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;법대로 하는 것보다 더 정의로운 절차는 세상에 없다&lt;/td&gt;
      &lt;td&gt;4146&lt;/td&gt;
      &lt;td&gt;317&lt;/td&gt;
      &lt;td&gt;0.926&lt;/td&gt;
      &lt;td&gt;0.928971543805&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;당연히 법대로 하셔야죠&lt;/td&gt;
      &lt;td&gt;296&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0.921&lt;/td&gt;
      &lt;td&gt;0.954838709677&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0.921&lt;/td&gt;
      &lt;td&gt;0.95873015873&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;법대로 합시다&lt;/td&gt;
      &lt;td&gt;919&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;0.92&lt;/td&gt;
      &lt;td&gt;0.947422680412&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
기본적으로 공감수가 많은 댓글을 상위에 랭크시키는 알고리즘이기 때문에 어뷰징 작업으로 공감수가 부풀려진 댓글이 top 10 밖으로 밀려나지는 못한다. 하지만 vote수가 적더라도 경향성을 파악해 댓글을 정렬시키기 때문에 단순한 순공감이나 공감비율순으로는 하위권에 있던 댓글이 상위권에 위치할 기회를 증가시켰다.&lt;/p&gt;

&lt;p&gt;어뷰저 입장에서는 쉽게 계산할 수 있는 정렬방식이 아니기 때문에 조작이 어려워질 것이다. 어뷰징을 할 때 고의로 공감과 비공감을 섞어서 해당 댓글을 상위에 랭크시키는데, Best 정렬이라면 “적당”한 비율을 맞추기 까다로워질 것이다.&lt;/p&gt;

&lt;h3 id=&quot;32-controversial-&quot;&gt;3.2 Controversial &lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;controversial은 이름 그대로, 공감과 비공감이 팽팽하게 맞서는 댓글을 상위에 위치시키려는 알고리즘이다. 단순히 팽팽하기만 하면 공감과 비공감이 1:1인 상황과 10:10인 상황이 같다고 생각할 수 있기에 vote수도 sorting algorithm에 포함시켜서 10:10이 1:1인 상황보다 더 controversial할 수 있도록 만들어졌다.&lt;/p&gt;

&lt;p&gt;아래의 식에서 upvote는 공감을, downvote는 비공감을 의미한다. upvote와 downvote의 차이가 같아서 분모가 같아진 경우에는 그 크기가 큰 쪽이 높고, vote의 크기가 같은 경우에는 차이가 작은 쪽이 높다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{controversial} = \frac{match * log(match + 1)}{|\#upvote - \#downvote| + 1},\text{ where }match=min(\#upvote, \#downvote)&lt;/script&gt;

&lt;p&gt;python으로 구현한 식이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;controversial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;좀 더 직관적인 이해를 돕기 위해 가공한 아래의 예시를 보자.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;upvote&lt;/th&gt;
      &lt;th&gt;downvote&lt;/th&gt;
      &lt;th&gt;controversial score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1001&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;3454.38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;999&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;3450.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;461.52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;230.76&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;700&lt;/td&gt;
      &lt;td&gt;15.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;14.89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;14.89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.35&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
upvote, downvote의 비율이 비슷한 댓글 순서로 정렬되고, 그 비율 내에서는 vote 수가 큰 댓글이 더 위에 놓이게 된다.&lt;/p&gt;

&lt;p&gt;controversial algorithm을 네이버 뉴스 댓글에 적용해보았다. 예상대로 공감과 비공감 수치가 비슷하면서도 vote수가 많은 댓글이 가장 먼저 보인다. vote수가 작은 이유는 이미 순공감 노출로 인해 vote를 받을 기회를 박탈당한 댓글들이기 때문이다.&lt;/p&gt;

&lt;p&gt;수치와는 무관하게 top 10 댓글의 내용은 얼마나 controversial하게 구성되어 있는지 정성적으로 평가해보았다. 보도자료에 대한 찬성은 &lt;span style=&quot;color:blue&quot;&gt;푸른색&lt;/span&gt;, 반대는 &lt;span style=&quot;color:red&quot;&gt;붉은색&lt;/span&gt; , 애매한 문장은 표기하지 않았다. controversial하다면 뉴스 기사의 주제에 대해 찬성과 반대가 골고루 섞여있어야 할 것이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;«아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합)»&lt;/strong&gt; &lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;comments&lt;/th&gt;
      &lt;th&gt;공감수&lt;/th&gt;
      &lt;th&gt;비공감수&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 1&lt;/td&gt;
      &lt;td&gt;대통령 각하, ‘사드 문제’ 갖고 거품무는 중국에도 내정 간섭이라고 거침 없이 말씀해주세요&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 2&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;이제는 한미일군사훈련을 해야 한다.&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 3&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;근데 왜 중국한테는 대놓고 내정간섭 받는거죠, 대통령님? 치욕스러웠던 조선시대가 그리운건가요?&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 4&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;봐라 ㅋㅋㅋ \n한미연합훈련 연기하지?\n미군 철수 얘기나온다 백퍼 ㅋㅋㅋㅋ \n미군철수하면 베트남꼴 나는거야 ㅋㅋㅋ \n개돼지들아 정신 좀 차리자&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;33&lt;/td&gt;
      &lt;td&gt;31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 5&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;아베만도못한 문통;                                  &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 6&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;문재인 아가라 닥쳐라. 사드도 내정문제인데 중국한테는 끽소리 못 하던 색히가 어디서 주둥아리 씨부리노.  &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 7&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;ㅋㅋㅋㅋㅋㅋ 곧 양념단와서 또 평화올림픽 울부짖겠네.         &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;66&lt;/td&gt;
      &lt;td&gt;57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 8&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;미국이 한국을 버려야 할 듯.\n답이 없네.  &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 9&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;미국을 대변하는거다.\n아베는 국익을 최우선으로 하는거지\n일본은 싫지만 아베가 똑똑하지않는냐.\n생각좀하고 살자.  &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 10&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;얼마나 답답하면 저런말을 할지 생각 안해보셨나요?? 북에서 원하는 대로 흘러가네요. 앞으로 한미군사훈련 연기뿐만 아니라 축소되고 없어지고 난리나겠네&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;분명 공감수와 비공감수는 controversial하지만 대부분이 당시의 여론과 반대대는 내용으로 치우쳐있다. 정성적으로 controversial한 댓글은 공감: 비공감이 1:1이 아닌 좀더 공감 비율이 높은 비율을 가진다는 사실을 유추해볼 수 있다.&lt;/p&gt;

&lt;p&gt;공감비율과 비슷하게 controversial도 vote수가 많은 경우에 불리해진다. controversial의 분모는 upvote와 downvote의 차이값인데, vote수가 많을수록 한두개차이를 유지하기가 어려워진다. 공감 66, 비공감 57을 가진 댓글이 공감 10, 비공감 10보다 아래에 놓인다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-new-sorting-algorithms&quot;&gt;4. New Sorting Algorithms&lt;/h2&gt;

&lt;p&gt;reddit ranking algorithm 중에서 controversial의 문제점을 해결한 새로운 controversial algorithm과 비공감이 많은 의견도 노출하는 best anti 정렬방식을 제안하고자 한다.&lt;/p&gt;

&lt;h3 id=&quot;41-new-controversial&quot;&gt;4.1 New controversial&lt;/h3&gt;

&lt;p&gt;앞서 지적했듯이 controversial은 공감: 비공감의 비율 재조정과 vote 수가 많은 경우 분모값의 기준을 완화시켜야하는 이슈가 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;공감 : 비공감&lt;/strong&gt;&lt;br /&gt;
정성적으로 확인해보았을 때 공감: 비공감 = 6.5 : 3.5 정도에서 기사 내용에 대한 찬성과 반대의 댓글이 골고루 등장하였다. 때문에 new controversial에서 upvote와 downvote의 값을 조정해주어야 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;vote수가 많은 경우&lt;/strong&gt;&lt;br /&gt;
이 문제는 공감비율순과 비슷했다. upvote와 downvote의 절대치에 의존하기보다 wilson score로 도출된 값을 upvote와 downvote로 대체하면 vote수가 많고 적음을 고려하면서도 0과 1 사이의 값을 가지게 되어 upvote와 downvote의 차이에 대한 효과가 완화된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;변경된 내용을 정리하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;controversial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;downvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upvote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;«아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합)»&lt;/strong&gt; &lt;sup id=&quot;fnref:11:1&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;comments&lt;/th&gt;
      &lt;th&gt;공감수&lt;/th&gt;
      &lt;th&gt;비공감수&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 11&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;아베한테 대하듯 똑같이 김정은하고 북한, 중국한테도 당당하게 나와라!&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 12&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;개~~새끼 아베 한테는 그렇게 당당하면서 김정은한테는 왜 그렇게 꼬리를 내린다냐? 핵이 무섭긴 무서운가 보다&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 13&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:blue&quot;&gt;한미 동맹도 좋다 그러나 우리 나라 스스로 강한 나라가 되어야 한다. 문대통형 수고 많으십니다 !!&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 14&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:blue&quot;&gt;아베에게 일침을 놔주신\n 문 대통령님 지지 합니다.\n아베 나대지 마시오&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 15&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:blue&quot;&gt;쪽바리 추종자들 많네!! 특히 벌레 틀딱들~~&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 16&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:blue&quot;&gt;반대로 우리나라가 일본보고 자위대 훈련하는거 보고 참견하면 일본이 가많이 있겠냐?\n이 벌레들아! 비판을 하려면 국내 내정에 간섭하는 아베를 비판해야지 아베를 두둔하냐? 이 스레기들아…&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 17&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;아베가 옳은말했네 지금이라고 김정은 참수 한미연합훈련을 시작하라 빨갱이한테 이 나라를 줄 수 없다 &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 18&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;대한민국은 다시 한번 망해봐야 정신차리지..\n말로는 안된다.&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 19&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:blue&quot;&gt;일본이 우방이란애들 멍청한거 아니냐 일본애들도 그렇게 생각안하는데 왜 니혼자 망상해 찐따새끼인가ㅋㅋㅋㅋㅋㅋ&lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 20&lt;/td&gt;
      &lt;td&gt;&lt;span style=&quot;color:red&quot;&gt;문재인씨 당신의 국적은 어디입니까? 다스 실소유주를 밝히는 것보다 훨씬 더 중요한 문제입니다. &lt;/span&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
공감 비율을 조금 높여주었을 때 기사 내용에 찬성하는 댓글과 반대하는 댓글이 top 10에 골고루 섞이게 되었다. 또 wilson score로 변환한 상태에서 비율을 조정해주게되어 vote수가 높은 경우에 up과 down의 차이에 덜 민감해질 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;42-best-anti&quot;&gt;4.2 Best-Anti&lt;/h3&gt;

&lt;p&gt;꼭 공감수가 많은 것만 괜찮은 의견이라고 볼 수 있을까? 비공감수가 많은 의견 또한 반대 진영의 입장을 대변하는 좋은 의견이라고도 볼 수 있지 않을까?&lt;/p&gt;

&lt;p&gt;네이버 뉴스 댓글은 대부분 당시의 여론에 따라 분위기가 흘러간다. 순공감순이든 공감비율순이든 한가지 주장을 다른 방식으로 표현하고 있는 댓글들이 top 10이 된다. 이를 보는 대중은 한쪽의 영향만 받게 되어 생각이 더욱 치우쳐진다.&lt;/p&gt;

&lt;p&gt;정치적 다양성을 수용하는 것은 의견의 객관성을 유지하는데에 도움이 된다. 그런 의미에서 당시 여론과 반대되는 내용의 댓글 또한 보여주는 것은 댓글에 영향을 받을 다른 사용자를 위해서도, 플랫폼의 중립성을 담보하기 위해서도 중요하다고 생각한다.&lt;/p&gt;

&lt;p&gt;Best-Anti는 negative vote에 대한 Wilson score를 구한 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{neg}^- = max\{0, \frac{2n(1-\hat{p}) + z^2 - z\sqrt{z^2 + 4n\hat{p}(1-\hat{p})}}{2(n+z^2)}\}&lt;/script&gt;

&lt;p&gt;python 구현식은 다음과 같다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;best_anti&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;up&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.96&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 95% confidence level
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;denominator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;numerator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_up&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_down&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numerator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;denominator&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ZeroDivisionError&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;«아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합)»&lt;/strong&gt; &lt;sup id=&quot;fnref:11:2&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;comments&lt;/th&gt;
      &lt;th&gt;공감수&lt;/th&gt;
      &lt;th&gt;비공감수&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 21&lt;/td&gt;
      &lt;td&gt;평화협정후 미군철수 바랍니다&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 22&lt;/td&gt;
      &lt;td&gt;홍발정씨..트럼프도 좌파 빨갱이죠??&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 23&lt;/td&gt;
      &lt;td&gt;늙다리 미치광이는 빠져 줄래!\n우리끼리 자주통일좀 하자!&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 24&lt;/td&gt;
      &lt;td&gt;자국당은 사형감 많던데… 미국철수 애기했다고 파면? 자국당 5월에는 문정인으로 놀고먹겠군~!&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 25&lt;/td&gt;
      &lt;td&gt;봐라. 지도자 하나가 이렇게나 세상을 바꿀 수 있다. 물론 촛불 들고, 직접민주주의를 구현한 국민 또한 위대하지. 지방선거 때 투표 잘 하자.&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 26&lt;/td&gt;
      &lt;td&gt;아직도. 미국이 인계철선이라믿고 50년대 사고방식이 존재하는구나 군사력 세계10위안에들고 1-1붙어도 안지니 너무 미군철수로 여론전말고 참신한거없어요 ? 자한당분들?&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 27&lt;/td&gt;
      &lt;td&gt;극우 자한당은 미국도 빨갱이란다 제비가 왔다고 봄은 아니람서 ㅋㅋㅋ&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 28&lt;/td&gt;
      &lt;td&gt;원샷-빅딜!&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 29&lt;/td&gt;
      &lt;td&gt;자한당분들께서 트럼프도 좌파래요..&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 30&lt;/td&gt;
      &lt;td&gt;잊지마세요 지금도 북한은 세계 최악의 인권유린 국가입니다 이시간에도 북한 주민들은 김정은한테 총살당하거나 아오지탄광으로 끌려가고 있습니다 북한 여성들은 김정은의 성노예가 되고 있구요 대한한공 조현민의 갑질 화가나죠 미투운동으로 드러난 권력자들의 성폭력 정말 싫습니다 그런데 이것보다 수백배는 더심한 갑질과 성폭력을 일삼는게 북한 김정은입니다&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;5-conclusions&quot;&gt;5. Conclusions&lt;/h2&gt;

&lt;p&gt;현재의 네이버 뉴스 댓글 정렬방식은 공감수가 높은 댓글을 위주로 보여주고 있고, 기준 또한 쉽다. 조작에 들어가는 비용 대비 얻을 수 있는 효과가 큰 상황에서 조작으로 인해 이익을 볼 집단은 당연히 어뷰징을 할 수 밖에 없다. 그리고 이미 조직적인 세력이 되어버린 어뷰저들은 완벽히 차단할 수 없다. 때문에 어뷰징을 해결할 수 있는 가장 좋은 방법은 현재의 정렬 방식의 단점을 극복하면서 자연스럽게 기준이 복잡해지게 만드는 것과 사람들이 조작된 의견에 크게 흔들리지 않을 수 있도록 다양한 의견을 보여주는 것이다.&lt;/p&gt;

&lt;p&gt;현재의 네이버 정렬 방식 중 순공감순과 공감비율순이 가지는 한계는 reddit에서 사용하고 있는 best 정렬방식으로 해결된다. 공감수에 가중치를 둔 정렬방식 외에 공감수와 비공감수가 비슷한 댓글에 가중치를 두는 방식, 비공감수에 가중치를 두는 방식을 제안하였다.&lt;/p&gt;

&lt;p&gt;한 쪽의 의견만 듣는 것은 언제나 편향된 결과를 야기한다고 생각한다. 한 쪽이 명백히 잘못한 것처럼 보도될 때, 그 반대의 의견에도 귀를 기울일 수 있는 플랫폼이 되길 바란다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;6-future-works&quot;&gt;6. Future works&lt;/h2&gt;

&lt;p&gt;지금까지는 댓글의 contents보다는 댓글에 부과된 공감, 비공감의 interaction 데이터로 문제점과 해결방식을 제안했다. controversial로 의견의 다양성을 추구했지만 text를 보지 않았기 때문에 의견의 다양성을 간접적으로 보장하기엔 불안정할 수 있다.&lt;/p&gt;

&lt;p&gt;쇼핑 리뷰에서 가격, 내구성, 디자인 등 다양한 측면을 보여주듯이 정치적 의견도 기사에서 언급된 중요한 단어들에 대한 사람들의 반응을 보는 방식도 생각해보면 좋을 것 같다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-a-wilson-score&quot;&gt;Appendix A: Wilson score&lt;/h2&gt;

&lt;p&gt;사실 본문에서 기술한 내용은 일반적인 Normal approximation interval이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p = \hat{p} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}&lt;/script&gt;

&lt;p&gt;여기서 &lt;script type=&quot;math/tex&quot;&gt;\hat{p}&lt;/script&gt;은 Bernoulli process의 성공확률을 의미한다.&lt;/p&gt;

&lt;p&gt;Wilson score는 confidence interval을 &lt;script type=&quot;math/tex&quot;&gt;\hat{p}&lt;/script&gt;가 아닌 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;로 추정한 score interval의 최소값이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p = \hat{p} \pm z\sqrt{\frac{p(1-p)}{n}}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;에 대해 정리하여 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;에 대한 2차방정식을 만든다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+\frac{z^2}{n})p^2 - (2\hat{p}+\frac{z^2}{n})p + \hat{p}^2 = 0&lt;/script&gt;

&lt;p&gt;근의 공식을 사용해 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;를 구한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p = \frac{2n\hat{p} + z^2 \pm z\sqrt{z^2 + 4n\hat{p}(1-\hat{p})}}{2(n+z^2)}&lt;/script&gt;

&lt;p&gt;Wilson score는 &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;의 lower bound이므로 - 에 대해 정리하면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^- = max\{0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 + 4n\hat{p}(1-\hat{p})}}{2(n+z^2)}\}=\text{wilson score}&lt;/script&gt;

&lt;p&gt;95%의 신뢰도로 고정하는 경우 z에 1.96을 대입할 수 있다. 그리고 이 경우 본문의 python 함수에서 구현한 &lt;code class=&quot;highlighter-rouge&quot;&gt;best&lt;/code&gt;가 된다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;2017년 11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의 호감순처럼 호감도를 ‘공감-비공감’으로 계산하게 되었다. (https://namu.wiki/w/네이버뉴스) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://news.naver.com/main/ranking/read.nhn?mid=etc&amp;amp;sid1=111&amp;amp;rankingType=popular_day&amp;amp;oid=081&amp;amp;aid=0003030446&amp;amp;date=20190922&amp;amp;type=1&amp;amp;rankingSeq=1&amp;amp;rankingSectionId=100&amp;amp;m_view=1&amp;amp;includeAllCount=true&amp;amp;m_url=%2Fcomment%2Fall.nhn%3FserviceId%3Dnews%26gno%3Dnews081%2C0003030446%26sort%3Dlikability &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;https://news.naver.com/main/read.nhn?mode=LSD&amp;amp;mid=shm&amp;amp;sid1=102&amp;amp;oid=008&amp;amp;aid=0004274961 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;https://news.naver.com/main/ranking/read.nhn?mid=etc&amp;amp;sid1=111&amp;amp;rankingType=popular_day&amp;amp;oid=023&amp;amp;aid=0003475126&amp;amp;date=20190922&amp;amp;type=1&amp;amp;rankingSeq=2&amp;amp;rankingSectionId=102&amp;amp;m_view=1&amp;amp;includeAllCount=true&amp;amp;m_url=%2Fcomment%2Fall.nhn%3FserviceId%3Dnews%26gno%3Dnews023%2C0003475126%26sort%3Dlikability &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system/ &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;https://news.naver.com/main/ranking/read.nhn?rankingType=popular_day&amp;amp;oid=001&amp;amp;aid=0009819820&amp;amp;date=20180117&amp;amp;type=1&amp;amp;rankingSectionId=100&amp;amp;rankingSeq=27 &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot;&gt;
      &lt;p&gt;https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;https://news.naver.com/main/ranking/read.nhn?rankingType=popular_day&amp;amp;oid=001&amp;amp;aid=0009878303&amp;amp;date=20180210&amp;amp;type=1&amp;amp;rankingSectionId=100&amp;amp;rankingSeq=6 &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:11:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:11:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 17:25:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/09/23/Naver-News-Comments-Analysis-(3)</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/09/23/Naver-News-Comments-Analysis-(3)</guid>
        
        
        <category>데이터분석</category>
        
        <category>data-analysis</category>
        
        <category>사회</category>
        
        <category>society</category>
        
        <category>뉴스댓글</category>
        
        <category>news-comments</category>
        
      </item>
    
      <item>
        <title>노르웨이에서의 나홀로 여행 1: 피오르드, 브라운 치즈</title>
        <description>&lt;p&gt;북유럽은 오로라를 보고 싶어서 겨울에 갈 곳으로 내심 정해두고 있었는데, 그럼에도 불구하고 한 여름에 노르웨이를 행선지로 정했던 까닭은 &lt;strong&gt;피오르드(Fjord)&lt;/strong&gt; 였다. 
웅장한 자연을 보길 좋아하는 편인데다가 (오로라만 봐도 알 수 있다) 덥고 습한 여름에서, 그리고 틀에 박힌듯한 답답한 삶에서 잠시 벗어나고 싶었다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/fjords.png&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;노르웨이의 피오르드들&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
좀 더 찾아보니 노르웨이는 피오르드의 천국이었다. 
대부분의 관광객이 찾는 피오르드이자 가장 길고 깊은 &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 송네피오르드(Sognefjord), 영화 «겨울왕국»의 배경이자 그 아름다움으로 UNESCO 세계 유산에 지정된 내로이피오르드(Nærøyfjord), 베르겐(Bergen)에서 플럼(Flåm)으로 가는 길에 송네피오르드를 거쳐 지나는 Aurlandsfjord &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 등등.
“피오르드”가 빙하로 만들어진 좁고 깊은 만을 뜻하다보니 노르웨이 곳곳에 피오르드로 끝나는 이름이 많았다. 
나는 이번 여행에서 베르겐을 들르지 않고 플럼에서만 머무르기로 했기 때문에 &lt;a href=&quot;https://www.fjordsafari.com/activities/may-september/heritage-taste-fjordsafari&quot;&gt;Aurlandsfjord에서 내로이피오르드(Nærøyfjord)를 거쳐 운드레달(Undredal)에서 브라운 치즈를 체험할 수 있는 투어&lt;/a&gt;를 신청했다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/fjord_guide.JPG&quot; width=&quot;65%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;열심히 설명 중인 가이드&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
ferry 투어와는 다르게 RIB(Rigid Inflated Boat) 투어는 야생의 투어에 가까웠다. 
보트의 운전을 담당하는 가이드는 때때로 ferry가 가르고 간 물결 위를 지나며 스릴감을 주기도 하고, 피오르드 협곡에 사는 작은 고래를 만나면 근처에서 구경을 시켜주기도 했다.
이에 더해 그 지역에 대한 친절하고 자세한 설명도 함께 들을 수 있어서 노르웨이가 더 가깝게 느껴졌다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/naeroyfjord.jpg&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;내로이피오르드(Nærøyfjord)와 저 멀리 보이는 구드방겐(Gudvangen)&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
피오르드는 정말 아름다웠다. 특히 내로이피오르드(Nærøyfjord)는 협곡의 곡선이 섬세했고 조화로웠다. 
아름다움과 실용성은 반비례한다고 생각해왔는데, 피오르드도 마찬가지였다.
보기에는 아름답고 경이롭지만, 그 곳에서 살아가기 위해서는 비옥한 평지에서 살아가는 사람들에 비해 많은 노력이 필요했다.
특히 “식”을 해결하는 것이 가장 어려운 문제였다. 
노르웨이는 경작할 수 있는 토지의 비율이 작다.  (정확히 얼마나?)
특히 피오르드 근처는 그 비율의 1/10이다.
이런 척박함 덕분에 석유의 발견으로 지금과 같이 부유한 국가가 되기 전에는 많은 사람들이 미국으로 이민을 갔다고 한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/undradel.jpg&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;아렌델(Arendelle)의 배경이 되었다고 하는 운드라델(Undradel)&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
남은 사람들은 농업대신 낙농업으로 생계를 유지해 나갔다. 
이 곳의 낙농업은 다른 곳에 비해 특이한 점이 있었다. 
하나는 염소 치즈가 주를 이룬다는 점, 다른 하나는 브라운 치즈라는 독특한 치즈가 발전되었다는 점.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/goats.jpg&quot; width=&quot;65%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;험준한 피오르드를 알아서 잘 타는 염소떼들&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
염소 치즈가 소 치즈보다 발달한 이유는 염소가 산을 잘 타기 때문이다.
추운 지방이다 보니 여름과 겨울에 풀을 먹을 수 있는 면적이 눈에 띄게 달라진다.
눈이 녹을 시기에는 그 기회를 십분 활용해 좀 더 위에서 풀을 먹는 것이 이득이다.
다행히 염소의 천적은 이 추위를 견디며 살 수 없어서 염소 떼를 풀어두기만 하면 알아서 산을 올라 제일 맛있는 풀을 알아서 뜯어먹고 안전히 마을로 귀가한다고 한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/brown_cheese.jpg&quot; width=&quot;65%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;브라운 치즈의 모습. 색이 정말 갈색이다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
브라운 치즈는 우리가 흔히 아는 화이트 치즈의 잔여물로 만든 독특한 치즈다. 
아무리 낙농업이 발달했다고 하더라도 그 양이 충분하지 않았기 때문에 사람들은 일반 화이트 치즈를 만들고 남은 것도 식량으로 활용해야 했다.
잔여물은 “유청”이라고 불리는 것인데 이를 충분히 졸이면 갈색으로 변하고 카라멜처럼 단 맛이 난다.
여기에 얼마 되지 않는 소의 젖으로 만든 크림을 1:1 비율로 섞어주면 브라운 치즈가 된다.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/cows.jpg&quot; width=&quot;65%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;힙하다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
화이트 치즈를 만들 때 지방이 많이 빠져나가다보니 브라운 치즈는 지방이 덜 함유되어 있다.
브라운 치즈는 노르웨이에서 많이 파는 크래커같은 빵에 버터를 발라 그 위에 얹고, 그 지방의 잼을 발라먹는다.
마트에서 파는 브라운 치즈는 인공적인 향이 가미되어 있어 좀 더 단 맛이 많이 난다.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/cheese_jam.jpg&quot; width=&quot;65%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;운드레달(Undredal)에서 제공했던 잼(blueberry, cloudberry, lingonberry)과 다양한 숙성도의 염소치즈, 브라운 치즈, 빵&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
내로이피오르드의 끝에는 구드방겐(Gudvangen)이라는 마을이 있었고, 운드레달(Undredal)과는 달리 바이킹 족의 전통을 간직하고 있었다.
집의 지붕은 풀이 덮어져있었고 &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, 어린 아이들이 (특히 남자) 낯선 이를 따라가지 못하도록 “트롤이 너를 잡아먹는다!”며 겁을 줬다고 한다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/gudvangen.jpg&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;구드방겐(Gudvangen) 마을의 전경&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
놀랍게도 이 트롤은 노르웨이의 유명한 극작가 헨릭크 입센의 소설 «페르귄트»에 처음으로 등장했다고 한다.
주인공 페르 귄트가 산 속에서 판타지스러운 여정을 할 때 초록 옷을 입은 여자가 나타나 그를 유혹했는데 알고보니 트롤의 딸이었다고 한다. 
그 이후 트롤은 판타지 소설 속에서 괴기스럽게 발전되기도 하고, «겨울왕국»에 등장하는 트롤처럼 귀여운 모습으로도 발전했다.&lt;/p&gt;

&lt;p&gt;생각보다 노르웨이 사람들의 입센 사랑은 엄청났다. 
릴리함메르(Lillehammer)라는 지역에서는 매년 8월 초마다 «페르귄트(Peer Gynt)» 페스티벌을 연다. 
노르웨이의 자연을 배경으로 야외에서 하는 연극이 가장 인기가 많다.
그리그의 «페르 귄트 모음곡»으로만 접했던 페르 귄트의 이야기가 궁금해서, 그리고 그들 고유의 축제가 궁금해서, 나도 한 번 연극 티켓을 구매했다.
이에 대한 내용은 다음 글에서 소개하겠다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;적어도 나의 가이드에 따르면, 가장 긴 피오르드는 사실 Greenland에 있는 피오르드라고 한다. 하지만 사람이 살지 않기 때문에 &lt;em&gt;안&lt;/em&gt; 쳐준다고… &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;한국어로 어떻게 표기하는 것이 옳은지 몰라 영어로 남겨두었다. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;나의 가이드에 따르면, 풀을 덮은 이유는 지붕을 더 단단히 엮기 위함과 아름다움(?!) 때문이라고 한다. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 09 Aug 2019 22:26:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/08/09/Travel-to-Norway-fjord-brown-cheese</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/08/09/Travel-to-Norway-fjord-brown-cheese</guid>
        
        
        <category>노르웨이</category>
        
        <category>여행</category>
        
      </item>
    
      <item>
        <title>Naver News Comment Analysis (2)</title>
        <description>&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;NOTICE: 앞으로 소개될 내용은 NAVER와 무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은 중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을 알립니다.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이번 글은 &lt;strong&gt;뉴스 댓글 분석&lt;/strong&gt; 2편이다. 지난 글에서와 마찬가지로 궁금한 점이나 추가 분석 요청은 댓글로 남겨주면 좋을 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를 확인해보았다.
여기서 말하는 어뷰저의 criteria는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.&lt;/li&gt;
  &lt;li&gt;발생하기 어려운 패턴을 보여야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서 369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고 269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#0-previous-work&quot;&gt;0. Previous Work&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#1-abuser-who-are-you&quot;&gt;1. Abuser, who are you?&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#11-introduction&quot;&gt;1.1 Introduction&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#12-abuser-criteria&quot;&gt;1.2 Abuser Criteria&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#13-data-preprocessing&quot;&gt;1.3 Data preprocessing&lt;/a&gt;&lt;br /&gt;
    &lt;a href=&quot;#14-analysis&quot;&gt;1.4 Analysis&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#2-conclusions&quot;&gt;2. Conclusions&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#appendix-a-gaussian-mixture-model-fitting&quot;&gt;Appendix A: Gaussian Mixture Model Fitting&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;0-previous-work&quot;&gt;0. Previous Work&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/2019/07/25/Naver-News-Comments-Analysis-(1)&quot;&gt;Naver News Comment Analysis (1)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-abuser-who-are-you&quot;&gt;1. Abuser, who are you?&lt;/h2&gt;

&lt;h3 id=&quot;11-introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;“정말 2016년 4분기부터 정말 댓글 조작을 했던 사용자들이 있었을까?”&lt;/strong&gt; 라는 단순한 의문과 궁금증에서 분석을 시작하게 되었다. 다만, 데이터에 &lt;공감&gt; &lt;비공감&gt; 을 눌렀던 interaction 정보가 누락되어 있기에 (이 데이터는 네이버 뉴스 측에서 제공해주지 않는 이상 얻을 수 없다) 적어도 댓글을 한 번이라도 남겼던 사용자에 대해서만 어뷰저로 의심해 볼 수 있었다.&lt;/비공감&gt;&lt;/공감&gt;&lt;/p&gt;

&lt;p&gt;label이 없는 상황에서 어뷰저를 특정짓는 것과 그 사용자가 어뷰저임을 다른 사람에게 설득하는 것은 어려운 일이다. 또한 무죄추정의 원칙에 의거해 댓글 작성자를 함부로 어뷰저라고 단정지을 수도 없었다. 그래서 이번 분석에서는, “모든 작성자는 어뷰저가 아니다.” 라는 가정을 기반으로 특정 패턴이 등장할 확률을 계산해서 &lt;strong&gt;어뷰저였을 가능성&lt;/strong&gt;을 간접적으로 추측하는 방식을 취했다.&lt;/p&gt;

&lt;p&gt;이 과정에서 누군가는 그 정도 확률로는 어뷰저라고 단정짓기 어렵다고 판단할 수도 있고, 아닐 수도 있다. 또 추가적인 분석 결과가 있다면 어뷰저 가능성이 더 높아질 수도 있다. 후자라면 언제든 댓글로 추가 분석할 내용을 요청했으면 하는 바람이다.&lt;/p&gt;

&lt;h3 id=&quot;12-abuser-criteria&quot;&gt;1.2 Abuser Criteria&lt;/h3&gt;

&lt;p&gt;어뷰저는 어떤 존재일까? 이에 답하기 앞서, 어뷰징의 목적과 어뷰징이 문제가 되는 상황에 대해 먼저 정리해보았다.&lt;/p&gt;

&lt;h4 id=&quot;어뷰징의-목적&quot;&gt;어뷰징의 목적&lt;/h4&gt;
&lt;p&gt;어뷰저들의 목표는 네이버의 댓글 정렬 기준에 맞추어 10위권 내에 드는 것이다. 네이버 뉴스의 UI 상, top 10 내에 들면 그 기사를 읽는 누구나 쉽게 그 댓글의 내용에 접하게 되기 때문이다. 그리고 그 내용이 대중을 대표한다고 생각하기 때문에 쉽게 타인의 생각에 영향을 미칠 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;어뷰징이-문제가-되는-상황&quot;&gt;어뷰징이 문제가 되는 상황&lt;/h4&gt;
&lt;p&gt;어뷰징이 문제가 되었던 이유는 공정하고 자연스러운 방식으로 집계되었다고 믿었던 top 10 댓글이 실제로는 어떤 세력에 의해 의도를 가지고 조작되었기 때문이었다. top 댓글이 특정 집단에 의해 조작되었다면, 그것들이 과연 네이버 뉴스 플랫폼에 참여하는 사용자들의 생각을 대표하는 댓글이라고 볼 수 있을까?&lt;/p&gt;

&lt;p&gt;그래서, 이 글에서 이야기 할 어뷰저의 criteria는 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;어뷰징의 목적을 달성해야 한다. 즉, 타인의 생각에 영향을 미칠 수 있도록 &lt;strong&gt;작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다&lt;/strong&gt;. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다. &lt;del&gt;&lt;em&gt;(안습)&lt;/em&gt;&lt;/del&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;자연적으로 발생하기 어려운, &lt;strong&gt;확률이 낮은 패턴이 등장&lt;/strong&gt;해야 한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;13-data-preprocessing&quot;&gt;1.3 Data preprocessing&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;Previous work에서 사용했던 데이터&lt;/a&gt;에서 추가로 필터링이 필요했다. 크롤링한 댓글 데이터에 hashing 된 아이디가 포함된 것이 2015년 12월 이후였기 때문이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사용한 댓글 데이터 기간: 2015.12.08 ~ 2018.05.25&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;14-analysis&quot;&gt;1.4 Analysis&lt;/h3&gt;

&lt;p&gt;먼저, &lt;strong&gt;정치&lt;/strong&gt; 분야에서 댓글이 top 10 내에 들었던 횟수를 작성자 별로 집계한 후, 횟수가 높은 순서대로 정렬하였을 때의 추이를 살펴보았다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/politics_top_user.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;stdev&lt;/th&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;th&gt;med&lt;/th&gt;
      &lt;th&gt;min&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2.240442&lt;/td&gt;
      &lt;td&gt;4.607621&lt;/td&gt;
      &lt;td&gt;369&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
대부분의 작성자는 1~2번 정도 댓글이 top 10 내에 드는 반면, 일부 사용자들은 100번 이상 순위권 내에 든다. 이 그래프만 본다면 자주 top 10에 드는 사용자들 모두가 의심스러울 수 있지만 이런 skewed graph는 대부분의 사회과학 데이터에서 발견되므로 이들을 어뷰저로 속단하긴 이르다. 검증을 위해 다른 섹션(사회, 경제, 문화, IT, 세계)에 대해서도 마찬가지 방법으로 그래프를 그려보았다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/paretto.png&quot; width=&quot;100%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;갓 파레토...&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;자주 순위권 내에 드는 댓글을 작성한 사용자를 &lt;strong&gt;&lt;em&gt;top user&lt;/em&gt;&lt;/strong&gt; 라고 했을 때, 다른 분야에서도 top user는 쉽게 찾아볼 수 있었다. 어쩌면 이들은 (어뷰저가 아닌 이상) 네이버 뉴스 플랫폼에서 높은 “공감수-비공감수”를 받을 수 있는 전략이 학습된 것은 아닐까? 기사가 나오고 얼마 지나지 않아 댓글을 남기거나, 그 당시 분위기에 맞는 댓글의 내용을 남기거나, 사실로 보여지는 데이터와 함께 댓글을 작성하거나 하는 등 자신만의 전략이 있을 것이다.&lt;/p&gt;

&lt;p&gt;그러나 이 전략들이 100%의 확률로(=항상) 통하지는 않았을 것이다. 때로는 일찍 댓글을 작성했음에도 뒤늦게 작성한 댓글이 폭발적인 공감을 이끌어내서 top 10에 들지 못했을 수도 있고, 당시의 전반적인 분위기에 탑승하는 댓글을 남겼음에도 다른 댓글 중에 두드러지지 못해 공감을 받지 못했을 수도 있다.&lt;/p&gt;

&lt;p&gt;top user 간에 일반적인 top 10 성공률이 존재할 것이고 이는 normal distribution을 따른다는 가설을 바탕으로 “top user가 작성한 전체 댓글 수 대비 top 10에 들었던 댓글 수(=top 10 성공률)”를 계산해보았다.&lt;/p&gt;

&lt;h4 id=&quot;정치-top-users&quot;&gt;정치 top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;user 1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;369&lt;/td&gt;
      &lt;td&gt;386&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;95.60&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 2&lt;/td&gt;
      &lt;td&gt;339&lt;/td&gt;
      &lt;td&gt;380&lt;/td&gt;
      &lt;td&gt;89.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;user 3&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;269&lt;/td&gt;
      &lt;td&gt;289&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;93.08&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 4&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;610&lt;/td&gt;
      &lt;td&gt;29.18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 5&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;1090&lt;/td&gt;
      &lt;td&gt;16.33&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 6&lt;/td&gt;
      &lt;td&gt;175&lt;/td&gt;
      &lt;td&gt;424&lt;/td&gt;
      &lt;td&gt;41.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 7&lt;/td&gt;
      &lt;td&gt;174&lt;/td&gt;
      &lt;td&gt;818&lt;/td&gt;
      &lt;td&gt;21.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 8&lt;/td&gt;
      &lt;td&gt;155&lt;/td&gt;
      &lt;td&gt;316&lt;/td&gt;
      &lt;td&gt;49.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 9&lt;/td&gt;
      &lt;td&gt;143&lt;/td&gt;
      &lt;td&gt;950&lt;/td&gt;
      &lt;td&gt;15.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 10&lt;/td&gt;
      &lt;td&gt;141&lt;/td&gt;
      &lt;td&gt;583&lt;/td&gt;
      &lt;td&gt;24.19&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;경제-top-users&quot;&gt;경제 top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 11&lt;/td&gt;
      &lt;td&gt;289&lt;/td&gt;
      &lt;td&gt;1185&lt;/td&gt;
      &lt;td&gt;24.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 12&lt;/td&gt;
      &lt;td&gt;226&lt;/td&gt;
      &lt;td&gt;2935&lt;/td&gt;
      &lt;td&gt;7.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 13&lt;/td&gt;
      &lt;td&gt;219&lt;/td&gt;
      &lt;td&gt;1656&lt;/td&gt;
      &lt;td&gt;13.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 14&lt;/td&gt;
      &lt;td&gt;183&lt;/td&gt;
      &lt;td&gt;1636&lt;/td&gt;
      &lt;td&gt;11.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 15&lt;/td&gt;
      &lt;td&gt;173&lt;/td&gt;
      &lt;td&gt;1378&lt;/td&gt;
      &lt;td&gt;12.55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 16&lt;/td&gt;
      &lt;td&gt;161&lt;/td&gt;
      &lt;td&gt;989&lt;/td&gt;
      &lt;td&gt;16.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 17&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;654&lt;/td&gt;
      &lt;td&gt;24.46&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 18&lt;/td&gt;
      &lt;td&gt;157&lt;/td&gt;
      &lt;td&gt;2589&lt;/td&gt;
      &lt;td&gt;6.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 19&lt;/td&gt;
      &lt;td&gt;139&lt;/td&gt;
      &lt;td&gt;1514&lt;/td&gt;
      &lt;td&gt;9.18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 20&lt;/td&gt;
      &lt;td&gt;127&lt;/td&gt;
      &lt;td&gt;742&lt;/td&gt;
      &lt;td&gt;17.12&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;사회-top-users&quot;&gt;사회 top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 21&lt;/td&gt;
      &lt;td&gt;366&lt;/td&gt;
      &lt;td&gt;953&lt;/td&gt;
      &lt;td&gt;38.41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 22&lt;/td&gt;
      &lt;td&gt;308&lt;/td&gt;
      &lt;td&gt;1636&lt;/td&gt;
      &lt;td&gt;18.83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 23&lt;/td&gt;
      &lt;td&gt;271&lt;/td&gt;
      &lt;td&gt;935&lt;/td&gt;
      &lt;td&gt;28.98&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 24&lt;/td&gt;
      &lt;td&gt;241&lt;/td&gt;
      &lt;td&gt;1254&lt;/td&gt;
      &lt;td&gt;19.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 25&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;1656&lt;/td&gt;
      &lt;td&gt;14.07&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 26&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;328&lt;/td&gt;
      &lt;td&gt;62.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 27&lt;/td&gt;
      &lt;td&gt;191&lt;/td&gt;
      &lt;td&gt;719&lt;/td&gt;
      &lt;td&gt;26.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 28&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;625&lt;/td&gt;
      &lt;td&gt;26.88&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 29&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
      &lt;td&gt;1190&lt;/td&gt;
      &lt;td&gt;12.52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 30&lt;/td&gt;
      &lt;td&gt;148&lt;/td&gt;
      &lt;td&gt;1489&lt;/td&gt;
      &lt;td&gt;9.94&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;문화-top-users&quot;&gt;문화 top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 31&lt;/td&gt;
      &lt;td&gt;373&lt;/td&gt;
      &lt;td&gt;1636&lt;/td&gt;
      &lt;td&gt;22.80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 32&lt;/td&gt;
      &lt;td&gt;367&lt;/td&gt;
      &lt;td&gt;935&lt;/td&gt;
      &lt;td&gt;39.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 33&lt;/td&gt;
      &lt;td&gt;301&lt;/td&gt;
      &lt;td&gt;1417&lt;/td&gt;
      &lt;td&gt;21.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 34&lt;/td&gt;
      &lt;td&gt;243&lt;/td&gt;
      &lt;td&gt;890&lt;/td&gt;
      &lt;td&gt;27.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 35&lt;/td&gt;
      &lt;td&gt;220&lt;/td&gt;
      &lt;td&gt;1656&lt;/td&gt;
      &lt;td&gt;13.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 36&lt;/td&gt;
      &lt;td&gt;188&lt;/td&gt;
      &lt;td&gt;1943&lt;/td&gt;
      &lt;td&gt;9.68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 37&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;3245&lt;/td&gt;
      &lt;td&gt;5.49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 38&lt;/td&gt;
      &lt;td&gt;172&lt;/td&gt;
      &lt;td&gt;2738&lt;/td&gt;
      &lt;td&gt;6.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 39&lt;/td&gt;
      &lt;td&gt;164&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;82.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 40&lt;/td&gt;
      &lt;td&gt;151&lt;/td&gt;
      &lt;td&gt;719&lt;/td&gt;
      &lt;td&gt;21.00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;it-top-users&quot;&gt;IT top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 41&lt;/td&gt;
      &lt;td&gt;714&lt;/td&gt;
      &lt;td&gt;3123&lt;/td&gt;
      &lt;td&gt;22.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 42&lt;/td&gt;
      &lt;td&gt;572&lt;/td&gt;
      &lt;td&gt;3886&lt;/td&gt;
      &lt;td&gt;14.72&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 43&lt;/td&gt;
      &lt;td&gt;442&lt;/td&gt;
      &lt;td&gt;2287&lt;/td&gt;
      &lt;td&gt;19.33&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 44&lt;/td&gt;
      &lt;td&gt;399&lt;/td&gt;
      &lt;td&gt;1468&lt;/td&gt;
      &lt;td&gt;27.18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 45&lt;/td&gt;
      &lt;td&gt;231&lt;/td&gt;
      &lt;td&gt;810&lt;/td&gt;
      &lt;td&gt;28.52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 46&lt;/td&gt;
      &lt;td&gt;234&lt;/td&gt;
      &lt;td&gt;3010&lt;/td&gt;
      &lt;td&gt;7.77&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 47&lt;/td&gt;
      &lt;td&gt;275&lt;/td&gt;
      &lt;td&gt;1622&lt;/td&gt;
      &lt;td&gt;16.95&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 48&lt;/td&gt;
      &lt;td&gt;317&lt;/td&gt;
      &lt;td&gt;1493&lt;/td&gt;
      &lt;td&gt;21.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 49&lt;/td&gt;
      &lt;td&gt;346&lt;/td&gt;
      &lt;td&gt;2349&lt;/td&gt;
      &lt;td&gt;14.73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 50&lt;/td&gt;
      &lt;td&gt;364&lt;/td&gt;
      &lt;td&gt;1185&lt;/td&gt;
      &lt;td&gt;30.72&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;세계-top-users&quot;&gt;세계 top users&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;top comment #&lt;/th&gt;
      &lt;th&gt;total comment #&lt;/th&gt;
      &lt;th&gt;top 10 성공률 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 51&lt;/td&gt;
      &lt;td&gt;237&lt;/td&gt;
      &lt;td&gt;1636&lt;/td&gt;
      &lt;td&gt;14.49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 52&lt;/td&gt;
      &lt;td&gt;214&lt;/td&gt;
      &lt;td&gt;1432&lt;/td&gt;
      &lt;td&gt;14.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 53&lt;/td&gt;
      &lt;td&gt;145&lt;/td&gt;
      &lt;td&gt;615&lt;/td&gt;
      &lt;td&gt;23.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 54&lt;/td&gt;
      &lt;td&gt;148&lt;/td&gt;
      &lt;td&gt;1709&lt;/td&gt;
      &lt;td&gt;8.66&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 55&lt;/td&gt;
      &lt;td&gt;155&lt;/td&gt;
      &lt;td&gt;611&lt;/td&gt;
      &lt;td&gt;25.37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 56&lt;/td&gt;
      &lt;td&gt;156&lt;/td&gt;
      &lt;td&gt;1076&lt;/td&gt;
      &lt;td&gt;14.50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 57&lt;/td&gt;
      &lt;td&gt;162&lt;/td&gt;
      &lt;td&gt;864&lt;/td&gt;
      &lt;td&gt;18.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 58&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;2778&lt;/td&gt;
      &lt;td&gt;5.94&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 59&lt;/td&gt;
      &lt;td&gt;165&lt;/td&gt;
      &lt;td&gt;1254&lt;/td&gt;
      &lt;td&gt;13.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 60&lt;/td&gt;
      &lt;td&gt;175&lt;/td&gt;
      &lt;td&gt;575&lt;/td&gt;
      &lt;td&gt;30.43&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;top user의 top 10 성공률을 확률 변수 X라고 했을 때의 histogram과 모든 유저가 전략을 바탕으로 활동하는 그룹이라고 가정했을 때 Gaussian distribution으로 추정한 확률 분포이다. (Gaussian Mixture Model로 distribution fitting한 결과는 &lt;a href=&quot;#appendix-A-Gaussian-Mixture-Model-Fitting&quot;&gt;Appendix A&lt;/a&gt; 참고)&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/abuser-gaussian.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;mean: 25.0, std: 20.5&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;정치 섹션에서만 유일하게 &lt;code class=&quot;highlighter-rouge&quot;&gt;ratio &amp;gt; 90%&lt;/code&gt; 인 top user(&lt;code class=&quot;highlighter-rouge&quot;&gt;user 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;user 3&lt;/code&gt;)가 존재했으며 이들의 top 10 전략 성공률은 다른 top user 대비 발생하기 어려울 정도로 (0.0053%, 0.0079%) 높다고 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;숫자 이면의 패턴을 보기 위해 전체 정치면 기사들의 댓글 수와 &lt;code class=&quot;highlighter-rouge&quot;&gt;user 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;user 3&lt;/code&gt;의 전체 댓글 수, top 10 내에 든 댓글 수를 그래프로 시각화 해보았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;c.f. 2016.3 ~ 2018.5 까지 굵직한 이슈들 &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;이세돌 vs. 알파고 (2016.3)&lt;/li&gt;
      &lt;li&gt;옥시 (2016.4)&lt;/li&gt;
      &lt;li&gt;최순실 태블릿 pc (2016.10)&lt;/li&gt;
      &lt;li&gt;박근혜 탄핵 소추안 (2016.12)&lt;/li&gt;
      &lt;li&gt;사드배치 / 박근혜 수감 / 세월호 인양(2017.3)&lt;/li&gt;
      &lt;li&gt;19대 대통령 선거 (문재인 당선) (2017.5)&lt;/li&gt;
      &lt;li&gt;이대목동 신생아 사망 (2017.12)&lt;/li&gt;
      &lt;li&gt;평창 동계 올림픽 (2018.2)&lt;/li&gt;
      &lt;li&gt;이명박 수감 (2018.3)&lt;/li&gt;
      &lt;li&gt;드루킹 (2018.4)&lt;/li&gt;
      &lt;li&gt;남북1차정상회담 @판문점 (2018.4)&lt;/li&gt;
      &lt;li&gt;남북2차정상회담 (2018.5)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;141-user-1-ratio-96&quot;&gt;1.4.1 &lt;code class=&quot;highlighter-rouge&quot;&gt;user 1&lt;/code&gt; (ratio: 96%)&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/user1.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:darkgray&quot;&gt;회색 line&lt;/span&gt;이 정치면 기사 댓글, &lt;span style=&quot;color:blue&quot;&gt;파란색 line&lt;/span&gt;이 작성자가 쓴 전체 댓글 수, &lt;span style=&quot;color:green&quot;&gt;초록색 line&lt;/span&gt;이 작성자가 쓴 댓글 중 top 10 내에 들었던 댓글 수를 나타낸다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;user 1&lt;/code&gt;이 주로 활동했던 시기는, 최순실 태블릿 pc 사건, 박근혜 탄핵 및 19대 대통령 선거, 평창 동계올림픽 및 MB 다스 사건과 맞물려 있었다. 댓글 내용을 시기 별로 뜯어보면, 다음과 같다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;article date&lt;/th&gt;
      &lt;th&gt;user top comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;朴대통령, ‘29일까지 대면조사’ 檢 요청에 사흘째 묵묵부답&lt;/td&gt;
      &lt;td&gt;2016-11-25 15:12:00&lt;/td&gt;
      &lt;td&gt;대통령인 자가 자신의 관할하에 있는 검찰을 부정한다면 곧 국가도 부정하겠다는 의미다. 이런 대통령은 더이상 대한민국 대통령이 아니다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;차은택 변호인 “차씨, 최순실 지시로 김기춘 실장 공관서 면담”&lt;/td&gt;
      &lt;td&gt;2016-11-27 16:04:00&lt;/td&gt;
      &lt;td&gt;김기춘의 진두지휘하에 박근혜 정권의 모든 불법들이 자행되었다. 정말 악마같은 인간이다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;변호인 “차은택, 崔 지시로 김기춘 만나…우병우 장모와 골프도”(종합)&lt;/td&gt;
      &lt;td&gt;2016-11-27 16:46:00&lt;/td&gt;
      &lt;td&gt;김기춘의 진두지휘하에 박근혜 정권의 불법들이 자행되었다. 구속수사해서 감옥에서 못나오게 만들어야 한다. 공작정치부터 공안탄압 정경유착의 죄를 물어야 한다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;청문회장도 지배한 ‘촛불민심’…與野 ‘재벌 봐주기’ 없었다&lt;/td&gt;
      &lt;td&gt;2016-12-06 12:34:00&lt;/td&gt;
      &lt;td&gt;뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까… 강요죄는 확실할듯…&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;재벌 총수들 “청와대 거절 어려워”…하나같이 대가성 부인&lt;/td&gt;
      &lt;td&gt;2016-12-06 12:45:00&lt;/td&gt;
      &lt;td&gt;뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까… 강요죄는 확실할듯…&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;박한철 前소장 한표, ‘캐스팅보트’ 될뻔한 아슬아슬 상황 나올까&lt;/td&gt;
      &lt;td&gt;2017-03-04 08:00:00&lt;/td&gt;
      &lt;td&gt;탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;85시간 재판, 속기록 3000쪽…탄핵심판 이번주 결론날까&lt;/td&gt;
      &lt;td&gt;2017-03-05 09:00:00&lt;/td&gt;
      &lt;td&gt;탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[리얼미터] 다자 文 42.6% vs 安 37.2%…양자 文 47.6% vs 安 43.3%&lt;/td&gt;
      &lt;td&gt;2017-04-10 09:15:00&lt;/td&gt;
      &lt;td&gt;여론몰이에 흔들릴 국면이 아니다. 무쏘의 뿔처럼 나아가면 야합은 흩어지고 굳건함이 승리할 것이다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文 “김부겸 동지 미안하다…꼭 국민통합 해내겠다”&lt;/td&gt;
      &lt;td&gt;2017-04-22 08:01:00&lt;/td&gt;
      &lt;td&gt;김부겸의 진심이 느껴지고 그를 위로하고 뜻을 같이 하는 문재인의 진심도 느껴진다. 남자들에게 이런 동지애는 죽음도 불사하게 만드는 마력과도 깉다. 조~오타!!!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文 대통령 “내게 반대하라” 파격적 수석회의 시동(상보)&lt;/td&gt;
      &lt;td&gt;2017-05-25 12:56:00&lt;/td&gt;
      &lt;td&gt;요새 대통령의 행동과 지시사항을 보면 정말 준비된 겸손한 사람이란게 진솔하게 느껴진다. 대한민국 국민인게 자랑스럽고 행복해진다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文대통령 “사드 임시배치, 현재 정부가 취할 수 있는 최선의 조치”(종합)&lt;/td&gt;
      &lt;td&gt;2017-09-08 21:13:00&lt;/td&gt;
      &lt;td&gt;국가의 지도자는 자신의 굳은 신념까지도 국가와 국민을 위해 잠시 접어야할 용기가 필요할 때가있다. 그 지도자라고 왜 자신의 신념을 꺾음에 자괴감과 고민이 없겠는가? 그는 자신을 지지하는 사람들만의 지도자가 아니라 대한민국의 지도자이기 때문이다. 그의 고뇌찬 결단을 위로하며 지켜보고 힘을 실어주고 싶다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;與 “안철수, 나라다운 나라 만드는 일 폄훼 말라”&lt;/td&gt;
      &lt;td&gt;2017-11-04 16:10:00&lt;/td&gt;
      &lt;td&gt;명버기 구하기에 혈안이 된 명바기 아바타!!!&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;김정은 위원장 “이른 시일내 만날 용의”…문 대통령에 방북 요청(종합)&lt;/td&gt;
      &lt;td&gt;2018-02-10 15:56:00&lt;/td&gt;
      &lt;td&gt;남북 정상회담애서 허심탄회하게 모든 할 말 다해서 기필코 한반도 비핵화와 평화를 이루어야 한다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표&lt;/td&gt;
      &lt;td&gt;2018-03-09 09:11:00&lt;/td&gt;
      &lt;td&gt;한반도 평화가 세계 평화다. 이런 평화 모드가 얼마만인가…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;같은 내용의 댓글이 top 에 오른 것도 확인할 수 있었다.
    &lt;ul&gt;
      &lt;li&gt;다른 섹션에 같은 댓글을 남기고도 top에 오른 적도 있다. (아래 표 참고)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;title&lt;/th&gt;
          &lt;th&gt;section&lt;/th&gt;
          &lt;th&gt;article date&lt;/th&gt;
          &lt;th&gt;top user comments&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[현장영상] 박근혜 前 대통령 법원으로 출발&lt;/td&gt;
          &lt;td&gt;society&lt;/td&gt;
          &lt;td&gt;2017-03-30 10:18:00&lt;/td&gt;
          &lt;td&gt;최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;구속 갈림길에 선 박근혜 ‘웅변 대신 침묵’ 선택&lt;/td&gt;
          &lt;td&gt;politics&lt;/td&gt;
          &lt;td&gt;2017-03-30 10:22:00&lt;/td&gt;
          &lt;td&gt;최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;142-user-3-ratio-93&quot;&gt;1.4.2 &lt;code class=&quot;highlighter-rouge&quot;&gt;user 3&lt;/code&gt; (ratio: 93%)&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/user3.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;user 3&lt;/code&gt; 의 주 활동 시기는 사드배치, 평창 동계올림픽 및 북미회담과 맞물려있었다. 댓글을 자세히 보면 아래와 같다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;article date&lt;/th&gt;
      &lt;th&gt;user top comments&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;정상회담 돌발 변수는 ‘사드’…청 “모든 가능성 준비”&lt;/td&gt;
      &lt;td&gt;2017-06-25 20:20:00&lt;/td&gt;
      &lt;td&gt;국익과국가안보가최우선입니다~~~~&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;송영무 “사드, 비준 아닌 국회 검증…고액연봉·음주운전 송구”(종합)&lt;/td&gt;
      &lt;td&gt;2017-06-28 12:18:00&lt;/td&gt;
      &lt;td&gt;이유미녹취록에맛짱구치고놀아난언론은×&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;이유미-이준서 중 한 명은 거짓말…윗선 수사 불가피&lt;/td&gt;
      &lt;td&gt;2017-06-28 20:52:00&lt;/td&gt;
      &lt;td&gt;이유미녹취록에맛장구치고놀아난언론은~~~~????&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;軍, 송영무 인사청문회서 공개된 ‘군사기밀 유출’ 조사 착수&lt;/td&gt;
      &lt;td&gt;2017-06-29 12:24:00&lt;/td&gt;
      &lt;td&gt;자유당놈들답다도둑놈들&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文대통령, 내일 트럼프와 만난다…취임 후 첫 韓美정상회담&lt;/td&gt;
      &lt;td&gt;2017-06-29 13:54:00&lt;/td&gt;
      &lt;td&gt;국익과국가안보가최우선입니다부디좋은결과있으시길~&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[단독] ‘제보 조작’ 수사망 좁혀오자 安 독대한 이준서…왜?&lt;/td&gt;
      &lt;td&gt;2017-06-29 20:19:00&lt;/td&gt;
      &lt;td&gt;철수야~깜빵갈시간이다가오네~~~~&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;트럼프, 文대통령 부부에 백악관 사적공간 ‘트리티 룸’ 깜짝공개(종합)&lt;/td&gt;
      &lt;td&gt;2017-06-30 13:23:00&lt;/td&gt;
      &lt;td&gt;문재인대통령님~멋저부러요~♡♡♡&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;한반도 이슈서 ‘주도권’ 확보 성과…한미FTA 재협상 ‘숙제’(종합)&lt;/td&gt;
      &lt;td&gt;2017-07-01 10:05:00&lt;/td&gt;
      &lt;td&gt;국익과국가안보가최우선입니다부디좋은결과있으시길~~~~~~~~♡♡&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;남북 “4월말 정상회담 판문점서 개최”…특사단 발표(종합)&lt;/td&gt;
      &lt;td&gt;2018-03-06 20:24:00&lt;/td&gt;
      &lt;td&gt;이게 실화나ㅡ ㅡㅡㅡㅡ&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;문 대통령 “서울·평양·판문점 중 北이 판문점 정상회담 선택”&lt;/td&gt;
      &lt;td&gt;2018-03-07 15:28:00&lt;/td&gt;
      &lt;td&gt;문통 지지 합니다&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;문 대통령 “국외 대북 비밀접촉 없어…저쪽에 놀아나는 것 아냐”&lt;/td&gt;
      &lt;td&gt;2018-03-07 16:53:00&lt;/td&gt;
      &lt;td&gt;문 대통 령님 지지 합니다&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文대통령 “북핵목표는 비핵화…제재완화, 지금은 불가능”(종합)&lt;/td&gt;
      &lt;td&gt;2018-03-07 17:16:00&lt;/td&gt;
      &lt;td&gt;문대통령님 지지합니다&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표&lt;/td&gt;
      &lt;td&gt;2018-03-09 09:11:00&lt;/td&gt;
      &lt;td&gt;이게 실화냐ㅡㅡㅡㅡ&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-conclusions&quot;&gt;2. Conclusions&lt;/h2&gt;

&lt;p&gt;어뷰저를 &lt;strong&gt;타인의 생각에 영향을 미치고 비정상적인 행태를 보이는 사용자&lt;/strong&gt;로 정의하였고, 이 기준에 따라 어뷰저로 의심되는 사용자를 찾아내고자 하였다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;순공감 기준, 댓글이 10위권 내에 들었던 횟수가 많았던 작성자 중에서&lt;/li&gt;
  &lt;li&gt;작성한 댓글 수 대비 top 10 댓글 수의 비율이 일반적이지 않은 작성자&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;개인적으로, &lt;strong&gt;1.&lt;/strong&gt;과 &lt;strong&gt;2.&lt;/strong&gt;의 기준에 드는 사용자는 &lt;code class=&quot;highlighter-rouge&quot;&gt;user 1&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;user 3&lt;/code&gt; 라는 생각이다. 작성한 전체 댓글 수는 다른 사용자들에 비해 적은 편이었지만 top 댓글에 들었던 비율은 가장 높았고, 그 수치가 일반적이지는 않았다.&lt;/p&gt;

&lt;p&gt;어뷰저를 찾고자 시작한 분석이었지만 데이터를 살펴보면서 네이버 뉴스 댓글이 가지는 단일하고 공개된 ranking system이 얼마나 위험한지를 오히려 인식하게 되었다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;분석한 기간에서 중복 제거한 기사의 수는 총 100,780개 였고, 만약 top 10 댓글의 작성자가 모두 다른 사용자였다면 1,007,800명이 각자의 의견을 개시했을 것이다. 하지만 실제 그 기간에 집계된 unique한 작성자는 총 308,731 명에 불과했다. 게다가 중복 댓글까지 포함하면, 그 다양성은 조금 더 떨어진다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 같은 면에서 &lt;strong&gt;네이버 뉴스 댓글은 다양성을 충분히 수용하고 있지 못하다는 생각이 들었다.&lt;/strong&gt; “플랫폼이기 때문에 그럴 수 있지 않을까?” 싶지만 페이스북이나 유튜브, 레딧같은 다른 플랫폼에서의 댓글을 보면 무작정 호감순으로 정렬하지는 않는다. 이 플랫폼들의 기준이 문제가 없다는 것은 아니다. 하지만 네이버 뉴스 보다 다양한 기준으로 댓글을 정렬시키고 있으며 (최신순, 오래된 순, 공감을 많이 받은 순, relevance 순, 호감 + vote의 크기 등) 이를 통해 다양한 의견이 쉽게 노출될 수 있는 환경을 조성하였다는 점에서는 좀 더 높은 점수를 주고 싶다.&lt;/p&gt;

&lt;p&gt;그래서 세 번째 글은, 간단하지만 다른 정렬 기준을 적용했을 때 발견되는 새로운 댓글에 대해서 다뤄 볼 예정이다. 프로젝트 초창기에, 같이 작업을 진행했던 재명님이 돌려본 결과가 있는데 이 것도 조금 다듬어서 올리면 재밌을 것 같다! 3탄은… 휴가(@Norway) 다녀오고 나서 작업해볼까 싶다. To be continued…&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-a-gaussian-mixture-model-fitting&quot;&gt;Appendix A: Gaussian Mixture Model Fitting&lt;/h2&gt;

&lt;p&gt;GMM의 &lt;code class=&quot;highlighter-rouge&quot;&gt;n_components&lt;/code&gt; 최적 개수를 구하기 위해 &lt;code class=&quot;highlighter-rouge&quot;&gt;silhouette score&lt;/code&gt;를 계산하였다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/silhouette_score.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;score&lt;/code&gt;가 가장 높은 &lt;code class=&quot;highlighter-rouge&quot;&gt;n_components=2&lt;/code&gt; 이므로 2개의 gaussian을 가정하여 fitting 해보면 아래 그림과 같다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/gmm.png&quot; width=&quot;80%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;cluster 1: blue / cluster 2: orange&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;userId&lt;/th&gt;
      &lt;th&gt;cluster 1 prob&lt;/th&gt;
      &lt;th&gt;cluster 2 prob&lt;/th&gt;
      &lt;th&gt;ratio (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;user 1&lt;/td&gt;
      &lt;td&gt;2.27E-16&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;95.60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 3&lt;/td&gt;
      &lt;td&gt;2.56E-15&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;93.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 2&lt;/td&gt;
      &lt;td&gt;9.31E-14&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;89.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 39&lt;/td&gt;
      &lt;td&gt;4.96E-11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;82.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 6&lt;/td&gt;
      &lt;td&gt;8.11E-01&lt;/td&gt;
      &lt;td&gt;0.188898&lt;/td&gt;
      &lt;td&gt;41.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;user 9&lt;/td&gt;
      &lt;td&gt;1.00E+00&lt;/td&gt;
      &lt;td&gt;0.000219&lt;/td&gt;
      &lt;td&gt;15.05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;https://ko.wikipedia.org/wiki/2016년_대한민국 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://ko.wikipedia.org/wiki/2017년_대한민국 &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;https://ko.wikipedia.org/wiki/2018년_대한민국 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 03 Aug 2019 17:59:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/08/03/Naver-News-Comments-Analysis-(2)</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/08/03/Naver-News-Comments-Analysis-(2)</guid>
        
        
        <category>데이터분석</category>
        
        <category>data-analysis</category>
        
        <category>사회</category>
        
        <category>society</category>
        
        <category>뉴스댓글</category>
        
        <category>news-comments</category>
        
      </item>
    
      <item>
        <title>Naver News Comment Analysis (1)</title>
        <description>&lt;p&gt;올초(3월)부터 &lt;a href=&quot;https://www.facebook.com/zaemyung?__tn__=%2CdK-R-R&amp;amp;eid=ARD489RY8VCrqBTRnzKR0LmKEIU1b6PmeDG85URewUV0YZzKsyl6Wl-DKN8TSMKUTI_-6XSwjZQ19-v7&amp;amp;fref=mentions&quot;&gt;재명님&lt;/a&gt;과 &lt;strong&gt;네이버 뉴스 댓글&lt;/strong&gt; 데이터로 사이드 프로젝트를 시작했다. 직접 크롤링하신 데이터였는데, 그 양이 방대해서 “이 정도 데이터가 있으면, 뭔갈 해볼 수 있겠지!” 라는 가벼운 마음으로 사이드 프로젝트 제안을 덥석 받아물었다. 그리고 여느 사이드 프로젝트가 그렇듯 그 과정은 결코 생각만큼 가볍지는 않았더랬다…&lt;/p&gt;

&lt;p&gt;마침 작년 사내 Hackday에서 Abuser Detection 분석으로 좋은 성과를 얻었던터라 어뷰저 분석을 해보고 싶었고, 그 결과로 나름 재밌는 것들이 발견되었다.
하지만 좋은 발표 자리(이를테면 파이콘이라든지,,,)에 등록할 시기를 놓쳐서 논문을 arXiv에 올려두듯이 블로그에 댓글 분석한 내용을 공유하고자 한다.&lt;/p&gt;

&lt;p&gt;내용은 크게 &lt;strong&gt;뉴스 댓글 수집&lt;/strong&gt;과 &lt;strong&gt;뉴스 댓글 분석&lt;/strong&gt; 파트로 나뉘며, 전자는 재명님이 후자는 내가 주로 담당해서 정리하였다. 
이번 글은 &lt;strong&gt;뉴스 댓글 분석&lt;/strong&gt; 1편이다. 궁금한 점이나 추가 분석 요청은 댓글로 남겨주면 좋을 것 같다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;#1-data&quot;&gt;1. Data&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#2-basic-statistics&quot;&gt;2. Basic Statistics&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#3-news-sentiment-analysis&quot;&gt;3. News Sentiment Analysis&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#4-conclusions&quot;&gt;4. Conclusions&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;1-data&quot;&gt;1. Data&lt;/h2&gt;

&lt;h4 id=&quot;수집-기간&quot;&gt;수집 기간&lt;/h4&gt;
&lt;p&gt;2006.04.26 ~ 2018.05.25 (수집 시점: 2018.10)&lt;/p&gt;

&lt;h4 id=&quot;수집-내용&quot;&gt;수집 내용&lt;/h4&gt;
&lt;p&gt;네이버 뉴스의 6개 분야별(정치, 경제, 사회, 생활/문화, 세계, IT/과학) &lt;strong&gt;가장 많이 본 뉴스&lt;/strong&gt; 30건&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/naver_news_ranking.png&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;네이버 뉴스 &amp;gt; 랭킹뉴스 화면 예시&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;같은 기사이지만 2-3일 동안 랭킹뉴스에 오를 수 있으므로 중복 기사를 제거해주었다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;중복 제거 전 기사 #: 751,751 (약 75만)&lt;/li&gt;
  &lt;li&gt;중복 제거 후 기사 #: 643,226 (약 64만)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;분석에-사용한-필드&quot;&gt;분석에 사용한 필드&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;기사: 기사 id, 기사 제목, 기사 입력 시각, 기사 내용, 언론사, 기사 감정&lt;/li&gt;
  &lt;li&gt;댓글: 댓글 작성 기사id, 작성자 hashed id,  댓글 작성 시각, 댓글 내용, 공감수, 비공감수&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-basic-statistics&quot;&gt;2. Basic Statistics&lt;/h2&gt;

&lt;p&gt;중복 제거된 기사에 대해, 기사 작성 시점을 기준으로 한 달 단위로 기사에 달린 코멘트를 집계해서 그래프를 그리면 다음과 같다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_cmnt_all_year.png&quot; width=&quot;90%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
네이버 뉴스 개편 history &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 와 엮어서 이 그래프를 해석하면 재밌어진다.&lt;/p&gt;

&lt;h4 id=&quot;2009년&quot;&gt;2009년&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;2009년 개편 때는 메인 페이지 뉴스 박스 편집권을 신문사에 넘겼고, 기사를 클릭하면 바로 신문사 링크로 연결되게 바뀌었다. 이로 인해 네이버 뉴스의 트래픽이 감소하게 되었고 예전과 비교해서 리플 개수나 조회수가 &lt;strong&gt;상당히 줄어들었다&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2010년&quot;&gt;2010년&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;2010년대 초반에 뉴스 스탠드가 도입되면서 메인화면 뉴스 편집권을 포기하게 된다. 기사를 클릭하면 기본적으로 네이버 페이지가 아닌 언론사 사이트로 연결된다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;모바일로 댓글을 달 수 없었다&lt;/strong&gt;. 또한 댓글 형태가 댓글 제목을 클릭해야만 내용을 볼 수 있는 형태라서 결과적으로는 당시 뉴스 댓글 란은 지금보다 훨씬 폐쇄적인 모양새였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2012년&quot;&gt;2012년&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;검색과 지식인의 인기를 바탕으로 &lt;strong&gt;네이버가 2012년 1일 방문자 1800만 명을 기록할 정도로 성장&lt;/strong&gt;하는 동안, 네티즌의 뉴스 읽기 방식도 달라졌다. 종이신문을 읽거나 신문방송의 홈페이지를 찾아가는 대신, 네이버나 다음 등 포털의 뉴스캐스트를 통해 여러 언론사 기사를 한꺼번에 읽는 사람들이 크게 늘어난 것이다. 이 때문에 뉴스 편집 기능을 수행하는 포털을 언론사로 봐야 할 것이냐 아니냐 하는 논쟁이 언론관련 심의기구 등에서 벌어지고 있기도 하다. &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;2012년 중반부터 &lt;strong&gt;모바일로도 댓글을 달 수 있&lt;/strong&gt;게 되었다.&lt;/li&gt;
  &lt;li&gt;네이버 아이디로 로그인하지 않아도 트위터나 페이스북 등의 &lt;strong&gt;SNS 계정으로 댓글을 달 수 있&lt;/strong&gt;게 되었다. 이 때문에 네이버 영화 평점 조작처럼 추천수 조작하기도 쉬워졌다. 네이버, 미투데이, 트위터, 페이스북, 다음으로 한 번씩만 로그인해도 공감 및 비공감 5개를 줄 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2016년&quot;&gt;2016년&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;10월, JTBC에서 최순실의 태블릿 pc를 발견하였고 최순실 게이트 사건의 포문이 열리기 시작했다.&lt;/strong&gt; (&lt;del&gt;&lt;em&gt;트래픽 측면에서 네이버 뉴스는 최순실에게 감사하는 마음이 없지 않아 있을 것이다…&lt;/em&gt;&lt;/del&gt;)&lt;/li&gt;
  &lt;li&gt;그리고 동시에, &lt;strong&gt;드루킹의 댓글 조작 사건&lt;/strong&gt;도 시작 &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 되었다.
    &lt;ul&gt;
      &lt;li&gt;2018년 5월 말, 특검법이 통과된 이후에 댓글이 줄었다는 기사 &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; 가 보도되었다. 2018년 6월 이후의 댓글이 있었다면 그간 댓글부대의 위력이 어느 정도였는지 가늠해볼 수 있었을 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결론적으로 2016년 후반부 이후 폭발적인 댓글 수의 증가는 정치 및 사회 영역의 엄청난 트래픽 덕분이었을 것이다. 가설 검증 차원에서 섹션 별로 나누어 같은 방식으로 댓글을 집계해 보았다.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_cmnt_sections_year.png&quot; width=&quot;90%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;섹션 별 기사 댓글 (누적 그래프)&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_cmnt_politics_year.png&quot; width=&quot;90%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;타 섹션과 비교한 정치 기사 댓글 그래프&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_cmnt_society_year.png&quot; width=&quot;90%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;타 섹션과 비교한 사회 기사 댓글 그래프&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-news-sentiment-analysis&quot;&gt;3. News Sentiment Analysis&lt;/h2&gt;

&lt;p&gt;네이버 뉴스는 기사에 &lt;strong&gt;“좋아요”&lt;/strong&gt; 를 시작으로 &lt;strong&gt;“훈훈해요”&lt;/strong&gt;, &lt;strong&gt;“슬퍼요”&lt;/strong&gt;, &lt;strong&gt;“화나요”&lt;/strong&gt;, &lt;strong&gt;“후속기사 원해요”&lt;/strong&gt; 의 label을 달 수 있게 만들었다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“좋아요”:  2014년 초 시작&lt;/li&gt;
  &lt;li&gt;“훈훈해요”, “슬퍼요”, “화나요”, “후속기사 원해요”:  2017년 초 시작&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_only_like.png&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;2016.10.20 기사. &quot;좋아요&quot;만 굉장히 많다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_all.png&quot; width=&quot;75%&quot; /&gt;
&lt;figcaption class=&quot;caption&quot;&gt;2017.10.20 기사. 다섯 가지 감정 모두 표를 받(긴)했다.&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;“좋아요” 만 있을 때와 다섯 가지의 감정이 있을 때의 추이가 또 재밌다.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_all_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“좋아요” 외의 다른 감정이 허가된 순간 이후로 “화나요” 가 급격히 증가한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;정치&quot;&gt;정치&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_politics_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;참고: 사드배치 (2017.03), 문재인 당선 (2017.05) &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, 평창 동계 올림픽 (2018.02), 이명박 수감 (2018.03) &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;경제&quot;&gt;경제&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_economy_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;사회&quot;&gt;사회&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_society_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;문화&quot;&gt;문화&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_life_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2018년 2월에는 무슨 일이.. (추운 날씨, 성추행 등의 사건 때문으로 추측됨)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;it&quot;&gt;IT&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_it_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;세계&quot;&gt;세계&lt;/h4&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img class=&quot;image&quot; src=&quot;/assets/images/news_emoji_world_year.png&quot; width=&quot;80%&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-conclusions&quot;&gt;4. Conclusions&lt;/h2&gt;

&lt;p&gt;여기까지는 기초적인 데이터 탐색 작업이었다. 간단히 시간 순으로 댓글 수를 집계하기만 해도 재미있는 분석 결과를 얻을 수 있었다. (가령, 박근혜-최순실 게이트가 얼마나 큰 이슈였는지, 뉴스는 대부분 우리를 열받게 하는 내용이라든지 등)&lt;/p&gt;

&lt;p&gt;이 다음 분석은, &lt;em&gt;의심&lt;/em&gt;하기만 했던 댓글 어뷰저 집단이 실제로 존재하는지에 대해 다룰 예정이다. 마침 댓글 수집 기간과 드루킹의 댓글 조작 기간이 맞물려 있어서 분석해 볼 수 있는 데이터가 손에 쥐어졌다. 최대한 선입견없이 담백한 분석을 해보려고 노력했다. 정말인지 아닌지 다음 글에서 확인해보자.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://namu.wiki/w/네이버 뉴스&quot;&gt;https://namu.wiki/w/네이버 뉴스&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.wikitree.co.kr/main/news_view.php?id=71675&quot;&gt;https://www.wikitree.co.kr/main/news_view.php?id=71675&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.mk.co.kr/news/society/view/2018/05/294952/&quot;&gt;https://www.mk.co.kr/news/society/view/2018/05/294952/&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001&quot;&gt;https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/2017년_대한민국&quot;&gt;https://ko.wikipedia.org/wiki/2017년_대한민국&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/2018년_대한민국&quot;&gt;https://ko.wikipedia.org/wiki/2018년_대한민국&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 25 Jul 2019 22:23:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/07/25/Naver-News-Comments-Analysis-(1)</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/07/25/Naver-News-Comments-Analysis-(1)</guid>
        
        
        <category>데이터분석</category>
        
        <category>data-analysis</category>
        
        <category>사회</category>
        
        <category>society</category>
        
        <category>뉴스댓글</category>
        
        <category>news-comments</category>
        
      </item>
    
      <item>
        <title>나, 다시 쓰는 자기소개서</title>
        <description>&lt;p&gt;매달 여는 연울림 모임이지만 특히나 지난 달에 했던 가치 워크샵은 내게 많은 고민을 남겨준 시간이었다. 
200여 개의 가치들 중, 내가 중요하게 여기는 가치를 선택하는 과정 속에서 종착점이 생각보다 명확하다는 인상을 받았는데, 그 지점을 향해 나는 제대로 가고 있는 것인지에 대한 의문이 생겼기 때문이었다.&lt;/p&gt;

&lt;p&gt;내가 골랐던 가치는 &lt;strong&gt;공헌&lt;/strong&gt;, &lt;strong&gt;능력&lt;/strong&gt;, &lt;strong&gt;리더십&lt;/strong&gt;, &lt;strong&gt;이타주의&lt;/strong&gt;, &lt;strong&gt;지혜&lt;/strong&gt;였고, &lt;strong&gt;지혜로우면서도 능력있는 사회적 문제를 해결하는 기업가&lt;/strong&gt;로 요약되었다. 
반면, 내가 지금 발 딛고 있는 현재는 데이터 분석가이자 모임 기획자로 간략하게 소개해 볼 수 있었다. 
이 자리에서 어떻게 사회적 기업가로 나아갈 수 있을지, 왜 현재의 그 일을 하고 있고, 내가 풀고 싶은 사회적 문제와 어떻게 연관되는지 등을 다시 한 번 현재의 나에게 물어야 할 때가 왔다는 생각이 들었다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;그래서 간략하게 나를 소개해보는 문장을 적어보았다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;저는 파파고라는 인공지능 번역 서비스를 함께 만들어나가고 있는 머신러닝 엔지니어입니다. &lt;br /&gt;
대학에 입학했을 때의 전공은 화학생물공학으로 현재 하고 있는 일과 큰 관련이 없는 전공이었습니다. &lt;br /&gt;
사람들은 저마다의 가치와 추구하는 바가 있고, 이를 최대한으로 발현할 수 있는 사회를 만들고 싶습니다. &lt;br /&gt;
‘날리다’라는, 더 많은 사람들이 자기이해를 할 수 있는 모임을 기획하는 단체에서 ‘연울림’이라는 모임을 기획하고 운영하고 있습니다. &lt;br /&gt;
제 삶의 마지막 단계에서는 소모적인 경쟁이 없고, 다양한 장점을 인정하는 시스템을 만들고 싶다는 꿈이 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;한 문장, 한 문장이 서로 크게 연관이 되어 보이지는 않는 피상적인 형태의 자기소개서가 되었다. 
하지만 아직 발견되지 못했을 뿐, 제각각의 문장을 관통하는 무언가가 있을 것이라는 생각이 들었다. 
그 답은 스스로에게 던져진 질문에 답하는 과정에서 찾을 수 있겠지. 
‘나’라는 사람을 설명해주는 &lt;a href=&quot;https://inmoonlight.github.io/%EC%9E%90%EA%B8%B0%EC%9D%B4%ED%95%B4/%EB%82%A0%EB%A6%AC%EB%8B%A4/2019/01/20/%EB%8B%B5%EC%9D%80-%EC%96%B8%EC%A0%9C%EB%82%98-%EB%82%98%EC%97%90%EA%B2%8C-%EC%9E%88%EB%8B%A4.html&quot;&gt;답은 언제나 나에게 있&lt;/a&gt;으니까.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;언제나 그렇듯, 좋은 질문은 좋은 답을 이끄는 방향키다. 
당신은 어떤 사람인가?&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Jul 2019 20:26:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/07/20/Rewriting-self-introduction</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/07/20/Rewriting-self-introduction</guid>
        
        
        <category>연울림</category>
        
      </item>
    
      <item>
        <title>유쾌한 슬럼프</title>
        <description>&lt;p&gt;어렸을 때의 나는 무언가 새롭게 ‘시작’하는 것을 좋아했다. 무언가를 빨리 배우는 편이었고, 새롭게 어떤 환경이나 개념에 적응하는데에 드는 시간이 적게 들었기 때문에 ‘시작’을 즐길 수 있었기 때문이었던 것 같다. ‘시작’이 주는 그 몰입감과 성취감은 권태로움에서 나를 꺼내주는 좋은 처방전이었다.&lt;/p&gt;

&lt;p&gt;하지만 동시에, 내가 ‘시작’했던 많은 자잘한 일들은 소위말해 ‘꿀만 빨고’ 그만 둘 수 없었다. 처음이 주는 신선함에 어느 정도 익숙해지고나면 이 분야의 ‘탁월함’이 보이기 시작하고, 나는 한참 밑에 자리하고 있다는 사실을 인지하게 된다. 그리고, 그때쯤 항상 그만두고 싶어졌다.&lt;/p&gt;

&lt;p&gt;이 시기를 견디지 못하는 이유는, 아마 기준이 낮았기 때문이었을 것이다. 내가 성취하고자 하는 것은 굉장히 낮은 수준의 것이었고 그 수준을 빠르게 달성하고 나면 더 이상의 열정이 생기지 않았다. 오히려 ‘왜 해야하지?’ 라는 생각이 나를 합리화시키면서 금방 포기하게 만들었다.
그래서 참 많은, 잡다한 무언가를 하지만 어느 것 하나 제대로 마치지 못하는 나를 발견하곤, ‘잘’하는 것에 좀 더 집중해보기로 했다. 그 이후로 시작에 대한 두려움이 커졌다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;시간과 실력의 관계를 그래프 형태로 나타내면, ‘계단’형이라고 할 수 있겠다. 기나긴 슬럼프 끝에 오는 어느 순간의 깨달음으로, 실력이 갑자기 반등한다. 내게 두려운 기간은 바로 이 ‘슬럼프’의 시기다. 이 또한 지나가리라는 것을 알면서도, 그 순간들은 언제나 고통스럽다. 마치 근력운동을 할 때 정말 힘들지만, 이 순간을 견뎌내야 근육이 붙는다는 사실을 인지하는 느낌이다.&lt;/p&gt;

&lt;p&gt;그 때에 나는 고민한다. &lt;strong&gt;새로운 시작을 할 때인가 아니면, 버텨내야할 때인가를.&lt;/strong&gt; 잘하고 싶기에 버텨내야 한다는 답을 내리지만, 이게 무슨 의미가 있지? 라고 물었을 땐, 쉽게 답하기가 어려울 때가 많다. 왜 버텨내야 하는지, 스스로를 격려하지 않으면 자꾸만 과거의 습관이 튀어나와 새로운 시작을 시도하려고 한다.&lt;/p&gt;

&lt;p&gt;안타까운 건, 이젠 새로운 시작마저도 그 과정에서 다시금 슬럼프를 마주한다는 사실을 인지하고 있기 때문에 잘 하려고 들지 않는다는 것이다. 그래서, 차악으로, 내가 겪고 있는 슬럼프의 시기를 버티게 되는 요즘이다.&lt;/p&gt;

&lt;div class=&quot;breaker&quot;&gt;&lt;/div&gt;

&lt;p&gt;당신의 슬럼프는 안녕하신가요? 유쾌한 슬럼프란, 존재할 수 없는 개념일까요?&lt;/p&gt;
</description>
        <pubDate>Wed, 15 May 2019 22:01:00 +0900</pubDate>
        <link>https://inmoonlight.github.io/2019/05/15/Joyful-slump</link>
        <guid isPermaLink="true">https://inmoonlight.github.io/2019/05/15/Joyful-slump</guid>
        
        
        <category>연울림</category>
        
      </item>
    
  </channel>
</rss>
