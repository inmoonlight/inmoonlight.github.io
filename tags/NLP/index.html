<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: NLP - Space Moon</title><meta description="개인적이지만 사소하지 않은 이야기를 담고 싶습니다"><meta property="og:type" content="blog"><meta property="og:title" content="Space Moon"><meta property="og:url" content="https://inmoonlight.github.io/"><meta property="og:site_name" content="Space Moon"><meta property="og:description" content="개인적이지만 사소하지 않은 이야기를 담고 싶습니다"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://inmoonlight.github.io/img/og_image.png"><meta property="article:author" content="Jihyung Moon"><meta property="article:tag" content="machine-learning"><meta property="article:tag" content=" natural-language-processing"><meta property="article:tag" content=" review"><meta property="article:tag" content=" study"><meta property="article:tag" content=" 머신러닝"><meta property="article:tag" content=" 자연어처리"><meta property="article:tag" content=" 리뷰"><meta property="article:tag" content=" 에세이"><meta property="article:tag" content=" essay"><meta property="article:tag" content=" 논문"><meta property="article:tag" content=" paper"><meta property="article:tag" content=" 책"><meta property="article:tag" content=" book"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://inmoonlight.github.io"},"headline":"Space Moon","image":["https://inmoonlight.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Jihyung Moon"},"description":"개인적이지만 사소하지 않은 이야기를 담고 싶습니다"}</script><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131297969-1" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131297969-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="alternate" href="/feed.xml" title="Space Moon" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/inmoonlight"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-12-13T09:33:00.000Z" title="2020-12-13T09:33:00.000Z">2020-12-13</time><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">22 minutes read (About 3226 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/13/With-little-power-comes-great-responsibiltiy/">With Little Power Comes Great Responsibility</a></h1><div class="content"><p>요즘 등장하는 NLP model 페이퍼들은 주로 GLUE 벤치마크에 성능을 report 하면서 아주 미세한 성능 개선을 근거로 "우리 방법론은 효과적이었다!"를 주장하고 있다. 과연 이 결과가 실제로 그 모델이 더 나은 모델임을 주장할 수 있을만큼 근거가 탄탄할까?</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>이번에 소개하는 논문에서는 NLP research에서 모델의 성능 개선을 주장하는 실험 결과에 대해 그 결과가 "정말 유의미한 모델의 성능 개선을 보장할 수 있는가?"에 대해 분석한다. 더불어, 분석 결과를 통해 발견된 문제점을 개선할 수 있는 간단한 overview 까지 제안하고 있다.</p></div><a class="article-more button is-small size-small" href="/2020/12/13/With-little-power-comes-great-responsibiltiy/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-11-30T06:00:00.000Z" title="2020-11-30T06:00:00.000Z">2020-11-30</time><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">12 minutes read (About 1851 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/30/Don-t-stop-pretraining/">Don&#039;t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></h1><div class="content"><p>다양한 출처의 데이터로 학습한 pretrained model이 NLP task에서 좋은 성능을 보여주고 있다. 하지만 아직 주어진 labeled data 의 크기나 target domain의 코퍼스와 사전학습 코퍼스의 유사도가 특정 task의 결과에 얼마나 영향을 미치는지에 대해 알려진 바가 없다. 또한 RoBERTa와 같은 LM이 정말 다양한 task에 generalize될만큼의 다양한 source로 학습되었는지도 확실하지 않다. 이 논문에서는 pretrained model을 풀고자 하는 특정 task의 domain에 tailor시켜서 추가로 학습시키면 더 좋은 성능을 보일 수 있을까? 라는 질문에 대한 답을 하고 있다.</p></div><a class="article-more button is-small size-small" href="/2020/11/30/Don-t-stop-pretraining/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-11-14T02:00:00.000Z" title="2020-11-14T02:00:00.000Z">2020-11-14</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">28 minutes read (About 4200 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/14/Social-bias-in-NLP-models/">Social Bias in NLP Models</a></h1><div class="content"><p>한 스타트업에서 개발한 인공지능 채용솔루션(a.k.a. AI 면접관)을 벌써 여러 기업에서 사용하고 있다는 뉴스기사를 접하게 되었다. 해당 기업은 "성별이나 학력 등에 따른 차별 방지와 정확한 역량 추정"을 위해 5만 2천여명의 데이터를 확보하여 학습했다고 말한다. 5만 2천여명의 데이터와 차별 방지가 어떤 관련이 있는지는 모르겠지만, <em>많은</em> 양의 데이터를 사용한다는 걸 내세우고 싶었다면 대량의 데이터가 편향성을 줄이는 것과는 무관하다고 말하고 싶다. 5만 2천개보다 더 많은 데이터로 학습한 Language Model 도 편향성 문제가 있으며 이 이슈는 아직도 연구자들에 의해 활발히 연구되고 있다.</p></div><a class="article-more button is-small size-small" href="/2020/11/14/Social-bias-in-NLP-models/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-08-29T08:00:00.000Z" title="2020-08-29T08:00:00.000Z">2020-08-29</time><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">16 minutes read (About 2349 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/29/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (a.k.a. T5)</a></h1><div class="content"><p>최근 NLP task에서 좋은 성능을 보이는 모델은 대량의 monolingual corpus를 통해 unsupervised pre-training을 한 LM을 task에 맞게 supervised fine-tuning을 하는 transfer learning에 기반하고 있다. 같은 transfer learning framework 안에서도 다양한 모델이 존재한다. 우리가 아는 모델만 하더라도 BERT, GPT, ELMO 등이 있고, GLUE benchmark에 대해서 테스트한 점수가 있다.</p>
<p>하지만 과연 점수가 더 높다고 더 좋은 모델이라고 할 수 있을까? 우리가 모델이라고 부르는 것 안에는 학습 방식 외에도 학습에 사용한 데이터셋, optimizer, 모델의 크기 등 많은 내용이 함축되어 있다. 그래서 각 모델의 아이디어 중 과연 <strong>"어떤 특징이 좋은 모델 성능을 내는데에 도움이 되었을까?"</strong>라고 묻는다면 쉽게 대답하기 어렵다.</p>
<p>이 논문에서 소개하는 Text-to-Text Transfer Transformer (T5) 는 그 답을 찾기 위해 고안한 framework이다.</p></div><a class="article-more button is-small size-small" href="/2020/08/29/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-06-20T08:00:00.000Z" title="2020-06-20T08:00:00.000Z">2020-06-20</time><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">19 minutes read (About 2911 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/20/How-multilingual-is-multilingual-BERT/">How multilingual is Multilingual BERT?</a></h1><div class="content"><p>"How multilingual is Multilingual BERT?"<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/P19-1493.pdf&gt;
">[1]</span></a></sup> 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해 분석한 논문을 쓴 것이 아닐까?</p></div><a class="article-more button is-small size-small" href="/2020/06/20/How-multilingual-is-multilingual-BERT/#more">Read More</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/"><img class="thumbnail" src="/assets/images/hate-speech-background.jpg" alt="한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-05-28T12:00:00.000Z" title="2020-05-28T12:00:00.000Z">2020-05-28</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">18 minutes read (About 2688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/">한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기</a></h1><div class="content"><p>약 4-5개월동안 사이드로 진행했던 혐오 댓글 프로젝트<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://github.com/kocohub/korean-hate-speech&gt;
">[1]</span></a></sup>가 성공적으로 마무리되었다. 같은 문제의식을 가진 사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이 무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는 활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고 원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의 상호보완적인 역량 덕분이 아니었을까 싶다.</p>
<p>사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한 이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작 작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던 부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는 사람들에게도 좋은 팁이 되지 않을까?</p></div><a class="article-more button is-small size-small" href="/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/#more">Read More</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/27/Attention-in-NLP/"><img class="thumbnail" src="/assets/images/attention_camera.jpg" alt="Attention in NLP"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-01-27T12:00:00.000Z" title="2020-01-27T12:00:00.000Z">2020-01-27</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">7 minutes read (About 1070 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/27/Attention-in-NLP/">Attention in NLP</a></h1><div class="content"><blockquote><p>You can't cram the meaning of a whole %&amp;!# sentence into a single &amp;!#* vector!</p>
<footer><strong>Raymond Mooney</strong></footer></blockquote>
<p>Attention은 single vector에 한 문장의 의미를 완벽하게 담을 수 없기 때문에 필요한 순간에, 필요한 정보를 사용하기 위한 방법이다. 기본적으로 <strong>query</strong> vector와 <strong>key</strong> vector의 조합으로 attention weight가 계산된다. 여기서 "조합"의 방법에는 크게 두가지가 있다. 하나는 Additive Attention으로 query vector와 key vector에 feed-forward network를 적용한 것이고, 다른 하나는 Dot-Product Attention으로 문자그대로 query vector와 key vector의 dot-product를 이용한 것이다. 이번 글에서는 각 Attention 방법들과 이들의 장단점을 소개하려고 한다.</p></div><a class="article-more button is-small size-small" href="/2020/01/27/Attention-in-NLP/#more">Read More</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/26/Positional-Encoding/"><img class="thumbnail" src="/assets/images/positional_encoding.png" alt="Positional Encoding in NLP"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-01-26T08:00:00.000Z" title="2020-01-26T08:00:00.000Z">2020-01-26</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">12 minutes read (About 1860 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/26/Positional-Encoding/">Positional Encoding in NLP</a></h1><div class="content"><p>Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다. 보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다. 이번 글에서는 Convolutional Neural Network (CNN), End-to-End Memory Network (MemN2N), Transformer에서 sentence embedding을 위해 사용된 positional encoding에 대해 소개하려고 한다.</p></div><a class="article-more button is-small size-small" href="/2020/01/26/Positional-Encoding/#more">Read More</a></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2019/12/22/GLUE-benchmark/"><img class="thumbnail" src="/assets/images/glue.png" alt="General Language Understanding Evaluation (GLUE) benchmark"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2019-12-22T13:22:00.000Z" title="2019-12-22T13:22:00.000Z">2019-12-22</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">21 minutes read (About 3099 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/12/22/GLUE-benchmark/">General Language Understanding Evaluation (GLUE) benchmark</a></h1><div class="content"><p>General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, <a href="https://openreview.net/pdf?id=rJ4km2R5t7" rel="external nofollow noopener noreferrer" target="_blank">전자</a>는 설명이 상세하고 <a href="https://www.aclweb.org/anthology/W18-5446.pdf" rel="external nofollow noopener noreferrer" target="_blank">후자</a>는 요약되어 있다. 이 글은 가장 최근(2019.2.22)에 업데이트된 <a href="https://arxiv.org/pdf/1804.07461.pdf" rel="external nofollow noopener noreferrer" target="_blank">arXiv에 있는 논문</a>을 기반으로 작성되었다.</p></div><a class="article-more button is-small size-small" href="/2019/12/22/GLUE-benchmark/#more">Read More</a></article></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/sota.png" alt="Jihyung Moon"></figure><p class="title is-size-4 is-block line-height-inherit">Jihyung Moon</p><p class="is-size-6 is-block">개인적이지만 사소하지 않은 이야기</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">23</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="/" target="_self" rel="noopener">Home</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Github" href="https://github.com/inmoonlight"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Facebook" href="https://facebook.com/jihyung.moon.9"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Twitter" href="https://twitter.com/jihyung_moon"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="LinkedIn" href="https://www.linkedin.com/in/mjihyung"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/feed.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Book/"><span class="level-start"><span class="level-item">Book</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Essay/"><span class="level-start"><span class="level-item">Essay</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/ML/Data-Analysis/"><span class="level-start"><span class="level-item">Data Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/Quantum-Computing/"><span class="level-start"><span class="level-item">Quantum Computing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Ops/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-3-tablet is-3-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time datetime="2021-07-10T16:03:00.000Z">2021-07-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/11/Git-merge-strategy/">Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Ops/">Ops</a> / <a class="link-muted" href="/categories/Ops/Git/">Git</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-03-02T19:22:00.000Z">2021-03-03</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/03/PyTorch-view-transpose-reshape/">PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-21T14:22:00.000Z">2021-02-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/21/PyTorch-IterableDataset/">PyTorch의 IterableDataset을 사용해서 데이터 불러오기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-04T05:22:00.000Z">2021-02-04</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/04/Pandas-Dataframe-iterations/">Pandas Dataframe의 다양한 iteration 방법 비교</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a></p></div></article><article class="media"><a class="media-left" href="/2021/01/10/Adieu-2020-and-happy-new-year/"><p class="image is-64x64"><img class="thumbnail" src="/assets/images/2020-2021.jpg" alt="2020년을 정리하고, 2021년을 맞이하는 글"></p></a><div class="media-content size-small"><p><time datetime="2021-01-10T12:13:00.000Z">2021-01-10</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/10/Adieu-2020-and-happy-new-year/">2020년을 정리하고, 2021년을 맞이하는 글</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Essay/">Essay</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google/"><span class="tag">Google</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LM/"><span class="tag">LM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag is-grey-lightest">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bias/"><span class="tag">bias</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/book/"><span class="tag">book</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-analysis/"><span class="tag">data analysis</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dataset/"><span class="tag">dataset</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/essay/"><span class="tag">essay</span><span class="tag is-grey-lightest">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/evaluation/"><span class="tag">evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hate-speech/"><span class="tag">hate speech</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mentoring/"><span class="tag">mentoring</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/news-comments/"><span class="tag">news comments</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/norway/"><span class="tag">norway</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pandas/"><span class="tag">pandas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/paper/"><span class="tag">paper</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantum/"><span class="tag">quantum</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantum-computing/"><span class="tag">quantum computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/social-good/"><span class="tag">social good</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/travel/"><span class="tag">travel</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a><p class="size-small"><span>&copy; 2022 Jihyung Moon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://inmoonlight.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>