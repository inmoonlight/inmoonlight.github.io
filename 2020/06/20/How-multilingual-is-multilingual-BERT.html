<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
<link rel="icon" href="/assets/images/logo.png">
    
<title>How multilingual is Multilingual BERT? | Space Moon</title>
    
 
    
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+KR:300">

<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
    
<link href="/assets/css/screen.css" rel="stylesheet">
    
<link href="/assets/css/main.css" rel="stylesheet">
    
</head>
    

    

<body class="layout-post">

    
<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">
    
    <div class="container pr-0">    
    
    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="Space Moon">
    </a>
    <!-- End Logo -->
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>
    
    
    <div class="collapse navbar-collapse" id="navbarMediumish">
       
        <!-- Begin Menu -->
        
            <ul class="navbar-nav ml-auto">
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>
                
                
                <li class="nav-item">
                
                <a class="nav-link" href="/about">About</a>
                </li>
                
                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/inmoonlight/"><i class="fab fa-github"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.linkedin.com/in/mjihyung/"><i class="fab fa-linkedin"></i></a>
                </li>
                
                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="https://twitter.com/inmoonlight_kr"><i class="fab fa-twitter"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="https://www.instagram.com/jihyung.moon/"><i class="fab fa-instagram"></i></a>
                </li>

                <li class="nav-item">
                <a target="_blank"  class="nav-link" href="/feed.xml"><i class="fa fa-rss"></i></a>
                </li>
                
            </ul>		
  
        <!-- End Menu -->

    </div>
        
    </div>
</nav>
<!-- End Navigation
================================================== -->
    
<div class="site-content">   
<div class="container">
    
<!-- Site Title
================================================== -->
<!-- <div class="mainheading">
    <h1 class="sitetitle">Space Moon</h1>
    <p class="lead">
         개인적이지만 사소하지 않은 이야기를 담고 싶습니다
    </p>
</div> -->

    
    
<!-- Content
================================================== --> 
<div class="main-content">
    <!-- Begin Article
================================================== -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>How multilingual is Multilingual BERT?</title>
  <meta name="description" content="“How multilingual is Multilingual BERT?” 1 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 ..." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="" />
  <meta property="og:title" content="How multilingual is Multilingual BERT?"/>
  
  <meta property="og:description" content="“How multilingual is Multilingual BERT?” 1 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 ..." />
  
  <meta property="og:image" content="https://inmoonlight.github.io/assets/images/mbert.png" />
  <meta property="og:url" content="https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2020-06-20T17:00:00+09:00">

  <link rel="canonical" href="https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT"/>
  <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
</head>

  <body itemscope itemtype="http://schema.org/Article">
	<!-- header start -->


<!-- header end -->

	<main class="content" role="main">
	  <article class="post">
			<script type="text/javascript" async
				src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
			</script>
		
		<div class="article-image">
		  <div class="post-image-image" style="background-image: url(/assets/images/mbert.png)">
			Article Image
		  </div>
		  <div class="post-image-image2" style="background-image: url(/assets/images/mbert.png)">
			Article Image
		  </div>
		  <div class="post-meta">
			<h1 class="post-title">How multilingual is Multilingual BERT?</h1>
			<div class="cf post-meta-text">
        <time datetime="20-06-2020">Jun 20, 2020</time>
        | <span class="reading-time" title="Estimated read time">
    

    
        25 mins
    
</span> read
			  <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
			</div>
			<!-- <div style="text-align:center">
			  <a href="#topofpage" class="topofpage"><i class="fa fa-angle-down"></i></a>
			</div> -->
		  </div>
		</div>
		
		<section class="post-content-mediator">
		  <p>“How multilingual is Multilingual BERT?” <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해 분석한 논문을 쓴 것이 아닐까?</p>

<div class="breaker"></div>

<h3 id="table-of-contents">Table of Contents</h3>

<p><a href="#abstract">Abstract</a><br />
<a href="#motivation">Motivation</a><br />
<a href="#main-idea">Main Idea</a><br />
<a href="#main-findings">Main Findings</a><br />
<a href="#detailed-experiments-and-results">Detailed Experiments and Results</a><br />
    <a href="#0-preliminaries">0. Preliminaries</a><br />
        <a href="#01-ner-task">0.1. NER task</a><br />
        <a href="#02-pos-task">0.2. POS task</a><br />
        <a href="#03-code-switching-cs-and-transliterate-tlit-task">0.3. Code-switching (CS) and Transliterate (Tlit) task</a><br />
    <a href="#1-m-bert-의-cross-lingual-transferability-는-vocab-overlap-때문일까--no">1. <code class="highlighter-rouge">M-BERT</code> 의 cross-lingual transferability 는 vocab overlap 때문일까? ➡️   NO</a><br />
    <a href="#2-m-bert의-cross-lingual-transferability-는-언어의-typological-특징-때문일까--yes">2. <code class="highlighter-rouge">M-BERT</code>의 cross-lingual transferability 는 언어의 typological 특징 때문일까? ➡️   YES</a><br />
    <a href="#3-m-bert의-cross-lingual-transferability-는-cs-혹은-tlit-까지-적용될-수-있을까--cs-yes--tlit-notable-of-contents">3. <code class="highlighter-rouge">M-BERT</code>의 cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? ➡️  CS (YES) / Tlit (NO)</a><br />
    <a href="#4-m-bert의-feature-space">4. <code class="highlighter-rouge">M-BERT</code>의 feature space</a><br />
<a href="#my-thoughts-on-the-results">My Thoughts on the Results</a><br />
    <a href="#1-vocab-overlap-실험에서-en-bert와의-비교는-정당한가">1. vocab overlap 실험에서 <code class="highlighter-rouge">EN-BERT</code>와의 비교는 정당한가?</a><br />
    <a href="#2-sov-order-가-중요한-점이었을까">2. SOV order 가 중요한 점이었을까?</a><br />
    <a href="#3-feature-space">3. Feature space</a><br />
    <a href="#4-논문의-분석-내용">4. 논문의 분석 내용</a><br />
<a href="#references">References</a></p>

<div class="breaker"></div>

<h2 id="abstract"><a href="#table-of-contents">Abstract</a></h2>
<p>In this paper, we show that Multilingual BERT (<code class="highlighter-rouge">M-BERT</code>), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is <strong>surprisingly good at zero-shot cross-lingual model transfer</strong>, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that <strong>transfer is possible even to languages in different scripts</strong>, that <strong>transfer works best between typologically similar languages</strong>, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that <code class="highlighter-rouge">M-BERT</code> does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.</p>

<div class="breaker"></div>

<h2 id="motivation"><a href="#table-of-contents">Motivation</a></h2>
<p>Pretrained LM 이 다양한 NLP downstream task 에서 좋은 성능을 보여주었다. Pretrained LM의 probing 연구들은 모델이 학습한 representation 이 syntactic and named entity 에서 특히 유용한 정보를 가지고 있다는 사실을 보여주었지만, 이 연구들은 영어에 대해서만 집중적으로 진행되어 왔다. (2019 년 6월 기준) (참고로, BERT 는 2018년 11월에 첫 release)</p>

<div class="breaker"></div>

<h2 id="main-idea"><a href="#table-of-contents">Main Idea</a></h2>
<p>영어에 대해서 Pretrained LM이 가지고 있는 정보, 그 중에서도 syntactic and named entity information 이 언어에 상관없이 잘 generalize 되는지 분석</p>
<ul>
  <li>Main task: NER, POS</li>
  <li>Main method: Zero-shot cross-lingual transfer (Multilingual BERT 모델을 한 언어에 대해 finetuning 시키고, 다른 언어에 대해 같은 task의 성능을 평가)</li>
</ul>

<div class="breaker"></div>

<h2 id="main-findings"><a href="#table-of-contents">Main Findings</a></h2>
<ul>
  <li>아래 내용으로 학습한 Multilingual BERT (<code class="highlighter-rouge">M-BERT</code>) 는 NER과 POS task 에 대해 cross-lingual transfer ability 가 좋다.
    <ul>
      <li>language identifier 없이</li>
      <li>위키피디아의 문서로 학습 (140 개 언어)</li>
      <li>w/ shared word piece vocab</li>
    </ul>
  </li>
  <li>모든 언어쌍에 대해 zero-shot transfer 가 잘 된 것은 아니었는데 그렇다면 왜 이런 차이가 발생할까?
    <ul>
      <li>finetuning 언어와 evaluation 언어의 vocab overlap 때문은 아님</li>
      <li>오히려 언어의 typological 특징 때문
        <ul>
          <li>typological 특징도 여러 종류가 있는데 (여기서는 subject/object/verb order, adjective/noun order에 대해서만 결과를 보여줌), 그 중 <strong>SVO order</strong> 에 가장 큰 영향을 받음</li>
        </ul>
      </li>
      <li>transfer 하기 위한 언어에 대해 학습한 적이 있을 때 transfer 가능</li>
      <li><code class="highlighter-rouge">M-BERT</code> 의 중간 layer (8/12) 에서  cross-lingual information 이 높음</li>
    </ul>
  </li>
</ul>

<div class="breaker"></div>

<h2 id="detailed-experiments-and-results"><a href="#table-of-contents">Detailed Experiments and Results</a></h2>

<p><strong>Main Question</strong>: 무엇이 <code class="highlighter-rouge">M-BERT</code>의 zero-shot cross-lingual transferability를 만들어내는가?</p>

<h3 id="0-preliminaries"><a href="#table-of-contents">0. Preliminaries</a></h3>
<h4 id="01-ner-task"><a href="#table-of-contents">0.1. NER task</a></h4>
<ul>
  <li>dataset: <a href="https://www.clips.uantwerpen.be/conll2002/ner/">CoNLL-2002</a>, <a href="https://www.clips.uantwerpen.be/conll2003/ner/">2003 dataset</a>, Google in-house dataset
    <ul>
      <li>There are four types of phrases: person names (<code class="highlighter-rouge">PER</code>), organizations (<code class="highlighter-rouge">ORG</code>), locations (<code class="highlighter-rouge">LOC</code>) and miscellaneous names (<code class="highlighter-rouge">MISC</code>)</li>
    </ul>
  </li>
  <li>lang: <code class="highlighter-rouge">nl</code>, <code class="highlighter-rouge">es</code> (2002) / <code class="highlighter-rouge">en</code>, <code class="highlighter-rouge">de</code> (2003) 총 4개 + in-house dataset with 16 languages (Arabic, Bengali, Czech, German, English, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, and Chinese)</li>
  <li>example (<code class="highlighter-rouge">en</code>)
    <ul>
      <li>NER tagged plain text: <code class="highlighter-rouge">[PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .</code></li>
      <li>NER data:
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Wolff B-PER
     , O
currently O
     a O
journalist O
    in O
Argentina B-LOC
     , O
played O
  with O
   Del B-PER
Bosque I-PER
    in O
   the O
 final O
 years O
    of O
   the O
seventies O
    in O
  Real B-ORG
Madrid I-ORG
     . O
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h4 id="02-pos-task"><a href="#table-of-contents">0.2. POS task</a></h4>
<ul>
  <li>dataset: Universal Dependencies (UD) data (Universal dependencies v1: A multilingual treebank collection, Nivre, 2019) for 41 languages
    <ul>
      <li>Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew, Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese</li>
    </ul>
  </li>
  <li>evaluation set: Multilingual Parsing from Raw Text to Universal Dependencies, Zemman et al. 2017 (CoNLL 2017 shared Task)
    <ul>
      <li><a href="https://raw.githubusercontent.com/UniversalDependencies/UD_Korean-PUD">example (<code class="highlighter-rouge">ko</code>)</a>:
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># sent_id = n01007012
# text = 이 부분에서 게임과 우리 일상 생활 사이의 유사점을 찾을 수 있습니다.
# text_en = There are parallels to draw here between games and our everyday lives.
# translit = .i .bu.bun.e.seo .ge.im.gwa .u.ri .il.sang .saeng.hwal .sa.i.yi .yu.sa.jeom.eul .chaj.eul .su .iss.seub.ni.da.
1	이	_	DET	DT	_	2	det	_	Translit=.i|LTranslit=_
2	부분에서	부분	NOUN	NN+CM	Case=Advb|Polite=Form	9	advmod	_	MSeg=부분-에서|Translit=.bu.bun.e.seo|LTranslit=.bu.bun
3	게임과	게임	NOUN	NN+CP	Polite=Form	7	compound	_	MSeg=게임-과|Translit=.ge.im.gwa|LTranslit=.ge.im
4	우리	_	PRON	PRP	Person=1	6	compound	_	Translit=.u.ri|LTranslit=_
5	일상	_	NOUN	NN	_	6	compound	_	Translit=.il.sang|LTranslit=_
6	생활	_	NOUN	NN	_	3	conj	_	Translit=.saeng.hwal|LTranslit=_
7	사이의	사이	NOUN	NN+CM	Case=Gen|Polite=Form	8	nmod:poss	_	MSeg=사이-의|Translit=.sa.i.yi|LTranslit=.sa.i
8	유사점을	유사점	NOUN	NN+CM	Case=Acc|Polite=Form	9	obj	_	MSeg=유사점-을|Translit=.yu.sa.jeom.eul|LTranslit=.yu.sa.jeom
9	찾을	_	VERB	VV	Form=Adn	10	acl:relcl	_	Translit=.chaj.eul|LTranslit=_
10	수	_	NOUN	NNB	_	11	nsubj	_	Translit=.su|LTranslit=_
11	있습니다	_	ADJ	JJ	Mood=Ind|VerbForm=Fin	0	root	_	SpaceAfter=No|Translit=.iss.seub.ni.da|LTranslit=_
12	.	.	PUNCT	.	_	11	punct	_	Translit=.|LTranslit=_
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h4 id="03-code-switching-cs-and-transliterate-tlit-task"><a href="#table-of-contents">0.3. Code-switching (CS) and Transliterate (Tlit) task</a></h4>
<ul>
  <li>Code-switching: 여러 언어가 한 문장에 등장하는 경우
    <ul>
      <li>ex 1)  <code class="highlighter-rouge">I thought मौसम different होगा बस fog है</code></li>
      <li>ex 2) <code class="highlighter-rouge">Hate speech를 investigate하기 위한 social media platform으로는 대표적으로 Twitter가 많이 utilize되며, 종종 facebook도 consider된다.</code></li>
    </ul>
  </li>
  <li>Tlit: 음차 표기
    <ul>
      <li>ex 1) <code class="highlighter-rouge">I thought mosam different hoga bas fog hy</code></li>
      <li>ex 2)</li>
    </ul>
  </li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85153201-de219900-b290-11ea-9ab3-ff2ccebc45d8.png" width="80%" />
</div>
<p><br /></p>

<div class="breaker"></div>

<h3 id="1-m-bert-의-cross-lingual-transferability-는-vocab-overlap-때문일까--no"><a href="#table-of-contents">1. <code class="highlighter-rouge">M-BERT</code> 의 cross-lingual transferability 는 vocab overlap 때문일까? → NO</a></h3>
<ul>
  <li>vocab overlap: fine-tuning dataset (train) 의 word piece 와 evaluation dataset (test) 의 word piece 간의 overlap
<script type="math/tex">overlap = | E_{train} ∩ E_{eval} | / | E_{train} ∪ E_{eval} |</script></li>
  <li>검증 방식:
    <ul>
      <li>NER task 중 16개의 언어에 대한 in-house 데이터셋으로 가능한 언어쌍 (16 * 15 = 240 개) 에 대해 overlap을 구하고, trasfer score (F1) 를 report</li>
      <li><em><strong>(결과) <code class="highlighter-rouge">M-BERT</code>는 vocab overlap 과 무관하게 generally 성능이 좋다. vocab overlap 이 0인 언어쌍에 대해서도 최소 40%의 F1 score 를 보인다. 반면 EN-BERT 는 vocab overlap 에 굉장히 많이 영향을 받는다.</strong></em></li>
    </ul>
  </li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png" width="85%" />
</div>
<figcaption class="caption">credit) http://www.dhgarrette.com/papers/pires_multilingual_bert_acl2019_slides.pdf</figcaption>
<p><br /></p>

<div class="breaker"></div>

<h3 id="2-m-bert의-cross-lingual-transferability-는-언어의-typological-특징-때문일까--yes"><a href="#table-of-contents">2. <code class="highlighter-rouge">M-BERT</code>의 cross-lingual transferability 는 언어의 typological 특징 때문일까? → YES</a></h3>
<ul>
  <li>근거: POS accuracy of <code class="highlighter-rouge">ur</code> → <code class="highlighter-rouge">hi</code> (91%) while <code class="highlighter-rouge">en</code>→ <code class="highlighter-rouge">ja</code> (49.4%) (둘 다 다른 script 를 사용하는 언어쌍, a.k.a vocab overlap ~= 0)</li>
  <li>typological features <sup id="fnref:4"><a href="#fn:4" class="footnote">2</a></sup></li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85156077-8553ff80-b294-11ea-8a8d-5478762d6b43.png" width="85%" />
</div>
<p><br /></p>

<ul>
  <li><em><strong>(결과) 공통된 typological feature 의 개수가 많을수록 transferabiltiy 향상</strong></em></li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85156248-c3e9ba00-b294-11ea-8ca1-b3e74ea88860.png" width="60%" />
</div>
<p><br /></p>

<ul>
  <li><em><strong>(결과) 여러 typological features 중에서 SOV order 와 AN order의 영향을 비교했을 때 전자가 더 영향이 큼</strong></em></li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85156350-ea0f5a00-b294-11ea-862f-64ab5e3166f3.png" width="60%" />
</div>
<p><br /></p>

<div class="breaker"></div>

<h3 id="3-m-bert의-cross-lingual-transferability-는-cs-혹은-tlit-까지-적용될-수-있을까--cs-yes--tlit-no"><a href="#table-of-contents">3. <code class="highlighter-rouge">M-BERT</code>의 cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? → CS (YES) / Tlit (NO)</a></h3>
<ul>
  <li>CS 실험 목적: Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would beneﬁt to an even larger degree from a shared multilingual representation.</li>
  <li>Tlit 실험 목적: Generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that <em><code class="highlighter-rouge">M-BERT</code> was not pre-trained on text that looks like the target</em>.</li>
  <li><strong><em>(결과) <code class="highlighter-rouge">M-BERT</code>는 CS text 에 좋은 성능을 보임 (90.56% ⇒ 86.59%). 하지만 Tlit 은 이러한 종류의 데이터에 학습되지 않고서는 trasferability를 기대하기 어려움 (85.64% ⇒ 50.41%)</em></strong></li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85159242-34dea100-b298-11ea-9922-3b1acc66ef7b.png" width="60%" />
<figcaption class="caption">credit) https://www.youtube.com/watch?v=ZGZy_GrFkAY</figcaption>
</div>
<p><br /></p>

<div class="breaker"></div>

<h3 id="4-m-bert의-feature-space"><a href="#table-of-contents">4. <code class="highlighter-rouge">M-BERT</code>의 feature space</a></h3>
<ul>
  <li>WMT16 병렬 코퍼스를 사용해서 언어쌍 간의 NN accuracy 측정</li>
  <li><strong><em>(결과) 중간 layer에서 linguistic information을 공유하고 이는 언어에 관계없이 비슷하게 나타남</em></strong></li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85160445-7ae83480-b299-11ea-8ff2-3209922e78e7.png" width="60%" />
</div>
<p><br /></p>

<div class="breaker"></div>

<h2 id="my-thoughts-on-the-results"><a href="#table-of-contents">My Thoughts on the Results</a></h2>
<h3 id="1-vocab-overlap-실험에서-en-bert와의-비교는-정당한가"><a href="#table-of-contents">1. vocab overlap 실험에서 <code class="highlighter-rouge">EN-BERT</code>와의 비교는 정당한가?</a></h3>
<ul>
  <li>제 추측으로는, vocab overlap 실험에서 <code class="highlighter-rouge">M-BERT</code> 만을 보았을 때, 이 경향성이 overlap에 영향을 받는 것인지 아닌지 판단하기 어려웠기 때문에 상대 비교를 할만한 결과가 필요해서 EN-BERT의 실험 결과를 넣은 것 같습니다.
    <ul>
      <li>아래 이미지에서 corr 을 보면 어느 정도 유의미해 보이는 양의 상관관계가 나올 것 같고, 그렇다면 영향을 받는다고 해석해야할 수도 있지만,</li>
      <li>vocab overlap이 0임에도 40%의 성능을 보이므로 영향을 받는다고 하기도 애매한 상황이 아니었을까요</li>
    </ul>
  </li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85162060-ac61ff80-b29b-11ea-90a2-7e497a5196a9.png" width="70%" />
</div>
<p><br /></p>

<ul>
  <li>그렇지만 EN-BERT와의 비교가 정당하다고 생각하긴 어려운 것 같습니다.
    <ul>
      <li>NER 예측 task 는 sent -&gt; model (either <code class="highlighter-rouge">M-BERT</code> or EN-BERT) -&gt; last activation -&gt; add.layer -&gt; NER prediciton 로 진행되는데, sent 가 영어가 아닌 경우 tok 단계에서부터 <code class="highlighter-rouge">unk</code>으로 인식될 가능성이 높기 때문에 transfer는 고사하고 fine-tuning도 어려울 수 있습니다. 이 때문에 논문에서 EN-BERT와 XLM을 비교하는데요, Indo-european 인 (<code class="highlighter-rouge">de</code>, <code class="highlighter-rouge">nl</code>, <code class="highlighter-rouge">es</code>) 에 대해서만 나와있습니다. (Table 3)</li>
      <li>영어와 alphabet 이 비슷하면, unk 이 나오지 않을 가능성이 높다고 생각해서 위의 결과가 EN-BERT로 얻어진 것에 대해 크게 거부감이 들지 않았고, 오히려 CJK 에 대해서 보였어야 하는 것 아닌가 하는 의심이 들었습니다.</li>
      <li>그래서 🤗  로 간단하게 tokenize 결과를 비교해보았습니다.</li>
      <li>sent 가 <code class="highlighter-rouge">es</code> 인 경우
        <ul>
          <li>sent: <code class="highlighter-rouge">Por su parte , el Abogado General de Victoria , Rob Hulls , indicó que no hay nadie que controle que las informaciones contenidas en CrimeNet son veraces .</code></li>
          <li><code class="highlighter-rouge">M-BERT</code> tok: <code class="highlighter-rouge">Por su parte , el Ab ##oga ##do General de Victoria , Rob Hull ##s , ind ##ic ##ó que no hay nadie que controle que las informa ##ciones conte ##nida ##s en Crime ##Net son vera ##ces .</code></li>
          <li>EN-BERT tok: <code class="highlighter-rouge">Po ##r su part ##e , el A ##bo ##gado General de Victoria , Rob Hull ##s , in ##dic ##ó que no hay na ##die que control ##e que las inform ##ac ##ione ##s con ##ten ##idas en Crime ##Net son ve ##race ##s .</code></li>
        </ul>
      </li>
      <li>sent 가 <code class="highlighter-rouge">ko</code> 인 경우
        <ul>
          <li>sent: <code class="highlighter-rouge">언어(言語)에 대한 정의는 여러가지 시도가 있었다.</code></li>
          <li><code class="highlighter-rouge">M-BERT</code> tok: <code class="highlighter-rouge">언 ##어 ( 言 語 ) 에 대한 정 ##의 ##는 여러 ##가지 시 ##도가 있었다 .</code></li>
          <li>EN-BERT tok: <code class="highlighter-rouge">[UNK] ( [UNK] [UNK] ) [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] .</code></li>
        </ul>
      </li>
      <li><strong><em>위의 결과로 미루어보아, vocab overlap 이 낮으면서 성능도 낮았던 점들의 언어쌍에는 EN-BERT에서 unk이 많이 나왔던 언어가 포함되어 있지 않을까하는 생각이 들었고, 비교가 공정하지 않다는 생각이 들었습니다.</em></strong></li>
    </ul>
  </li>
  <li>또한 tlit. 을 <code class="highlighter-rouge">M-BERT</code> 가 못하는 이유로, pre-train step에서 tlit. corpus 가 없었기 때문이라고 언급하였는데 EN-BERT 또한 같은 이유로 성능이 낮을 수 밖에 없었을 것이라고 생각합니다.</li>
</ul>

<div class="breaker"></div>

<h3 id="2-sov-order-가-중요한-점이었을까"><a href="#table-of-contents">2. SOV order 가 중요한 점이었을까?</a></h3>
<ul>
  <li>논문에서는 아래 표에서 SVO -&gt; SVO (81.55) &gt; SVO -&gt; SOV (66.52) 라는 점 때문에 SOV order 가 가장 중요하다고 주장합니다.
    <ul>
      <li>하지만 반대로 SOV -&gt; SOV (64.22) &gt; SOV -&gt; SVO (63.98) 의 차이가 적게 나는 점은 설명할 수 없습니다.</li>
    </ul>
  </li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85165966-93f4e380-b2a1-11ea-9290-8451958499e3.png" width="60%" />
</div>
<p><br /></p>

<ul>
  <li>제 추측으로는, 오히려 각 그룹을 구성하는 언어와 그 언어들이 Wikipedia 에서 차지하는 비율, 즉 <code class="highlighter-rouge">M-BERT</code> 학습에 영향을 많이 끼친 언어가 중요한 역할을 했을 수도 있을 것 같습니다.
    <ul>
      <li>SVO languages: Bulgarian, Catalan, Czech, Danish, English, Spanish, Estonian, Finnish, French, Galician, Hebrew, Croatian, Indonesian, Italian, Latvian, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, and Chinese.</li>
      <li>SOV Languages: Basque, Farsi, Hindi, Japanese, Korean, Marathi, Tamil, Telugu, Turkish, and Urdu.
        <ul>
          <li>Urdu -&gt; Hindi 의 성능이 91% 였던 점을 고려하면 이 중 어떤 언어쌍에서 굉장히 낮은 성능을 보였을 것으로 예상됩니다. (평균이 64.22 이어야하므로)</li>
          <li>그룹의 평균치를 보지 않았다면 다른 해석이 가능했을 수도..?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SVO order 가 비슷하면 -&gt; transfer 가 잘된다! 라는 주장을 하고 싶었다면 비교하려는 대상 언어쌍들간에 SVO order 빼고는 조건을 동일하게 만족시켰어야 하지 않을까하는 아쉬움이 남습니다.</li>
</ul>

<div class="breaker"></div>

<h3 id="3-feature-space"><a href="#table-of-contents">3. Feature space</a></h3>
<ul>
  <li>MLM은 보통 중간 layer 에서 semantic 한 성질이 가장 두드러지게 나타나는 것 같습니다.</li>
</ul>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85170841-ed144580-b2a8-11ea-9db9-099c019568f3.png" width="60%" />
<figcaption class="caption">credit) The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</figcaption>
</div>
<p><br /></p>
<div style="text-align:center">
<img class="image" src="https://user-images.githubusercontent.com/24843996/85170937-1208b880-b2a9-11ea-81e8-c1102286646a.png" width="80%" />
<figcaption class="caption">credit) BERTScore: Evaluating Text Generation with BERT</figcaption>
</div>
<p><br /></p>
<div class="breaker"></div>

<h3 id="4-논문의-분석-내용"><a href="#table-of-contents">4. 논문의 분석 내용</a></h3>
<ul>
  <li>
    <p>논문에서 분석한 결론이 지나친 일반화가 아닐까하는 생각도 듭니다.</p>
  </li>
  <li>일단 <code class="highlighter-rouge">M-BERT</code>가 zero-shot transferability 가 높은 이유는 어쩌면 같은 내용의 Wikipedia 문서로 학습했기 때문일 수 있습니다.
    <ul>
      <li>만약에 한국어는 동일 내용에 대한 번역된 문서가 없는 (e.g., 네이버 블로그) 로 학습하고, 영어도 영어 나름대로의 번역문이 없는 문서로 학습된 BERT 였더라도 같은 결과가 도출되었을지는 모르겠습니다.</li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">M-BERT</code> 의 학습 데이터의 특징에서 기인한 특징이 얼마나 되는지도 궁금합니다. 오히려 여기에서 영향을 많이 받았을 수도 있을 것 같아요.</li>
</ul>

<div class="breaker"></div>

<h2 id="references"><a href="#table-of-contents">References</a></h2>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://www.aclweb.org/anthology/P19-1493.pdf">How multilingual is Multilingual BERT?</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://www.aclweb.org/anthology/P12-1066.pdf">Selective Sharing for Multilingual Dependency Parsing</a> <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

		</section>
		<footer class="post-footer">
			<!-- <div class="col-md-8 p-5 align-self-center text-center"> -->
			<ul class="tag-box inline">
					  
					
						 
							<a href="/tag/machine-learning">Machine-learning </a>
						 
							<a href="/tag/nlp">Nlp </a>
						 
							<a href="/tag/multilingual">Multilingual </a>
						 
							<a href="/tag/BERT">Bert </a>
						 
							<a href="/tag/머신러닝">머신러닝 </a>
						 
							<a href="/tag/자연어처리">자연어처리 </a>
						
					
					
			</ul>
		  <section class="share">
				
			  	
						<a class="icon-twitter" href="http://twitter.com/share?text=How+multilingual+is+Multilingual+BERT%3F&amp;url=https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT"
				  		onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
						<i class="fa fa-twitter"></i><span class="hidden">twitter</span>
						</a>
			 		
				
			  	
						<a class="icon-facebook" href="https://www.facebook.com/sharer.php?t=How+multilingual+is+Multilingual+BERT%3F&amp;u=https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT"
				  		onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
						<i class="fa fa-facebook"></i><span class="hidden">facebook</span>
						</a>
			 		
				
		  </section>
		</footer>
	  </article>
	</main>
	<script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
   })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-131297969-1', 'auto');
ga('send', 'pageview');

</script>



  </body>
</html>
<!-- End Article
================================================== -->

  

<!-- Begin Comments
================================================== -->

	<div class="container">
		<div id="comments" class="row justify-content-center mb-5">
			<div class="col-md-10">              
				<section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'inmoonlight'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
                
			</div>
		</div>
	</div>

<!--End Comments
================================================== -->
</div>

<!-- Bottom Alert Bar
================================================== -->
<!-- <div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt=""> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>     -->
    
</div><!-- /.container>
    
<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
          
             
              <a href="/tag/멘토링">멘토링 (2)</a>
             
              <a href="/tag/mentoring">Mentoring (2)</a>
             
              <a href="/tag/essay">Essay (3)</a>
             
              <a href="/tag/에세이">에세이 (3)</a>
             
              <a href="/tag/machine-learning">Machine-learning (8)</a>
             
              <a href="/tag/book">Book (4)</a>
             
              <a href="/tag/summary">Summary (3)</a>
             
              <a href="/tag/머신러닝">머신러닝 (8)</a>
             
              <a href="/tag/책">책 (4)</a>
             
              <a href="/tag/요약">요약 (3)</a>
             
              <a href="/tag/연울림">연울림 (4)</a>
             
              <a href="/tag/데이터분석">데이터분석 (3)</a>
             
              <a href="/tag/data-analysis">Data-analysis (3)</a>
             
              <a href="/tag/사회">사회 (3)</a>
             
              <a href="/tag/society">Society (3)</a>
             
              <a href="/tag/뉴스댓글">뉴스댓글 (3)</a>
             
              <a href="/tag/news-comments">News-comments (3)</a>
             
              <a href="/tag/노르웨이">노르웨이 (1)</a>
             
              <a href="/tag/여행">여행 (1)</a>
             
              <a href="/tag/quantum-computer">Quantum-computer (1)</a>
             
              <a href="/tag/basic">Basic (1)</a>
             
              <a href="/tag/양자컴퓨터">양자컴퓨터 (1)</a>
             
              <a href="/tag/기초">기초 (1)</a>
             
              <a href="/tag/nlp">Nlp (5)</a>
             
              <a href="/tag/dataset">Dataset (1)</a>
             
              <a href="/tag/자연어처리">자연어처리 (5)</a>
             
              <a href="/tag/데이터셋">데이터셋 (1)</a>
             
              <a href="/tag/review">Review (1)</a>
             
              <a href="/tag/후기">후기 (1)</a>
             
              <a href="/tag/technique">Technique (1)</a>
             
              <a href="/tag/attention">Attention (1)</a>
             
              <a href="/tag/korean">Korean (1)</a>
             
              <a href="/tag/hate-speech">Hate-speech (1)</a>
             
              <a href="/tag/한국어">한국어 (1)</a>
             
              <a href="/tag/혐오발언">혐오발언 (1)</a>
             
              <a href="/tag/multilingual">Multilingual (1)</a>
             
              <a href="/tag/BERT">Bert (1)</a>
            
          
        

		</div>
	</div>
</div>
 


<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                 Copyright © 2020 Space Moon 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net">Mediumish Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

   
</div> <!-- /.site-content>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
    
<script src="/assets/js/jquery.min.js"></script>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
    
<script src="/assets/js/ie10-viewport-bug-workaround.js"></script>
    
<script src="/assets/js/mediumish.js"></script>
    
<script id="dsq-count-scr" src="//inmoonlight.disqus.com/count.js"></script>
</body>
</html>
