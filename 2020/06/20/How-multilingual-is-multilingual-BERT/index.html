<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>How multilingual is Multilingual BERT? - Space Moon</title><meta description="&amp;quot;How multilingual is Multilingual BERT?&amp;quot;[1] 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해"><meta property="og:type" content="blog"><meta property="og:title" content="How multilingual is Multilingual BERT?"><meta property="og:url" content="https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/"><meta property="og:site_name" content="Space Moon"><meta property="og:description" content="&amp;quot;How multilingual is Multilingual BERT?&amp;quot;[1] 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85156077-8553ff80-b294-11ea-8a8d-5478762d6b43.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85156248-c3e9ba00-b294-11ea-8ca1-b3e74ea88860.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85156350-ea0f5a00-b294-11ea-862f-64ab5e3166f3.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85159242-34dea100-b298-11ea-9922-3b1acc66ef7b.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85160445-7ae83480-b299-11ea-8ff2-3209922e78e7.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85162060-ac61ff80-b29b-11ea-90a2-7e497a5196a9.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85165966-93f4e380-b2a1-11ea-9290-8451958499e3.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85170841-ed144580-b2a8-11ea-9db9-099c019568f3.png?style=centerme"><meta property="og:image" content="https://user-images.githubusercontent.com/24843996/85170937-1208b880-b2a9-11ea-81e8-c1102286646a.png"><meta property="article:published_time" content="2020-06-20T08:00:00.000Z"><meta property="article:modified_time" content="2023-11-02T02:50:49.145Z"><meta property="article:author" content="Jihyung Moon"><meta property="article:tag" content="ML"><meta property="article:tag" content="NLP"><meta property="article:tag" content="paper"><meta property="article:tag" content="BERT"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/"},"headline":"Space Moon","image":["https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png","https://user-images.githubusercontent.com/24843996/85170937-1208b880-b2a9-11ea-81e8-c1102286646a.png"],"datePublished":"2020-06-20T08:00:00.000Z","dateModified":"2023-11-02T02:50:49.145Z","author":{"@type":"Person","name":"Jihyung Moon"},"description":"&quot;How multilingual is Multilingual BERT?&quot;[1] 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해"}</script><link rel="canonical" href="https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131297969-1" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131297969-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="alternate" href="/feed.xml" title="Space Moon" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/inmoonlight"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-06-20T08:00:00.000Z" title="2020-06-20T08:00:00.000Z">2020-06-20</time><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a></span><span class="level-item">19 minutes read (About 2911 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">How multilingual is Multilingual BERT?</h1><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>"How multilingual is Multilingual BERT?"<sup id="fnref:1"><a href="#fn:1" rel="footnote"><!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/P19-1493.pdf&gt;
">[1]</span></a></sup> 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해 분석한 논문을 쓴 것이 아닐까? <a id="more"></a></p>
<h2 id="abstract">Abstract</h2>
<p>In this paper, we show that Multilingual BERT (<code>M-BERT</code>), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is <strong>surprisingly good at zero-shot cross-lingual model transfer</strong>, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that <strong>transfer is possible even to languages in different scripts</strong>, that <strong>transfer works best between typologically similar languages</strong>, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that <code>M-BERT</code> does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.</p>
<h2 id="motivation">Motivation</h2>
<p>Pretrained LM 이 다양한 NLP downstream task 에서 좋은 성능을 보여주었다. Pretrained LM의 probing 연구들은 모델이 학습한 representation 이 syntactic and named entity 에서 특히 유용한 정보를 가지고 있다는 사실을 보여주었지만, 이 연구들은 영어에 대해서만 집중적으로 진행되어 왔다. (2019 년 6월 기준) (참고로, BERT 는 2018년 11월에 첫 release)</p>
<h2 id="main-idea">Main Idea</h2>
<p>영어에 대해서 Pretrained LM이 가지고 있는 정보, 그 중에서도 syntactic and named entity information 이 언어에 상관없이 잘 generalize 되는지 분석 - Main task: NER, POS - Main method: Zero-shot cross-lingual transfer (Multilingual BERT 모델을 한 언어에 대해 finetuning 시키고, 다른 언어에 대해 같은 task의 성능을 평가)</p>
<h2 id="main-findings">Main Findings</h2>
<ul>
<li>아래 내용으로 학습한 Multilingual BERT (<code>M-BERT</code>) 는 NER과 POS task 에 대해 cross-lingual transfer ability 가 좋다.
<ul>
<li>language identifier 없이</li>
<li>위키피디아의 문서로 학습 (140 개 언어)</li>
<li>w/ shared word piece vocab</li>
</ul></li>
<li>모든 언어쌍에 대해 zero-shot transfer 가 잘 된 것은 아니었는데 그렇다면 왜 이런 차이가 발생할까?
<ul>
<li>finetuning 언어와 evaluation 언어의 vocab overlap 때문은 아님</li>
<li>오히려 언어의 typological 특징 때문
<ul>
<li>typological 특징도 여러 종류가 있는데 (여기서는 subject/object/verb order, adjective/noun order에 대해서만 결과를 보여줌), 그 중 <strong>SVO order</strong> 에 가장 큰 영향을 받음</li>
</ul></li>
<li>transfer 하기 위한 언어에 대해 학습한 적이 있을 때 transfer 가능</li>
<li><code>M-BERT</code> 의 중간 layer (8/12) 에서 cross-lingual information 이 높음</li>
</ul></li>
</ul>
<h2 id="detailed-experiments-and-results">Detailed Experiments and Results</h2>
<p>Main Question: 무엇이 <code>M-BERT</code>의 zero-shot cross-lingual transferability를 만들어내는가?</p>
<h3 id="preliminaries">Preliminaries</h3>
<h4 id="ner-task">NER task</h4>
<ul>
<li>dataset: <a href="https://www.clips.uantwerpen.be/conll2002/ner/" rel="external nofollow noopener noreferrer" target="_blank">CoNLL-2002</a>, <a href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="external nofollow noopener noreferrer" target="_blank">2003 dataset</a>, Google in-house dataset
<ul>
<li>There are four types of phrases: person names (<code>PER</code>), organizations (<code>ORG</code>), locations (<code>LOC</code>) and miscellaneous names (<code>MISC</code>)</li>
</ul></li>
<li>lang: <code>nl</code>, <code>es</code> (2002) / <code>en</code>, <code>de</code> (2003) 총 4개 + in-house dataset with 16 languages (Arabic, Bengali, Czech, German, English, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, and Chinese)</li>
<li>example (<code>en</code>)
<ul>
<li>NER tagged plain text: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .</span><br></pre></td></tr></table></figure></li>
<li>NER data: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     Wolff B-PER</span><br><span class="line">         , O</span><br><span class="line"> currently O</span><br><span class="line">         a O</span><br><span class="line">journalist O</span><br><span class="line">        in O</span><br><span class="line"> Argentina B-LOC</span><br><span class="line">         , O</span><br><span class="line">    played O</span><br><span class="line">      with O</span><br><span class="line">       Del B-PER</span><br><span class="line">    Bosque I-PER</span><br><span class="line">        in O</span><br><span class="line">       the O</span><br><span class="line">     final O</span><br><span class="line">     years O</span><br><span class="line">        of O</span><br><span class="line">       the O</span><br><span class="line"> seventies O</span><br><span class="line">        in O</span><br><span class="line">      Real B-ORG</span><br><span class="line">    Madrid I-ORG</span><br><span class="line">         . O</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h4 id="pos-task">POS task</h4>
<ul>
<li>dataset: Universal Dependencies (UD) data (Universal dependencies v1: A multilingual treebank collection, Nivre, 2019) for 41 languages
<ul>
<li>Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew, Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese</li>
</ul></li>
<li>evaluation set: Multilingual Parsing from Raw Text to Universal Dependencies, Zemman et al. 2017 (CoNLL 2017 shared Task)
<ul>
<li><a href="https://raw.githubusercontent.com/UniversalDependencies/UD_Korean-PUD" rel="external nofollow noopener noreferrer" target="_blank">example (<code>ko</code>)</a>: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sent_id &#x3D; n01007012</span><br><span class="line"># text &#x3D; 이 부분에서 게임과 우리 일상 생활 사이의 유사점을 찾을 수 있습니다.</span><br><span class="line"># text_en &#x3D; There are parallels to draw here between games and our everyday lives.</span><br><span class="line"># translit &#x3D; .i .bu.bun.e.seo .ge.im.gwa .u.ri .il.sang .saeng.hwal .sa.i.yi .yu.sa.jeom.eul .chaj.eul .su .iss.seub.ni.da.</span><br><span class="line">1	이	_	DET	DT	_	2	det	_	Translit&#x3D;.i|LTranslit&#x3D;_</span><br><span class="line">2	부분에서	부분	NOUN	NN+CM	Case&#x3D;Advb|Polite&#x3D;Form	9	advmod	_	MSeg&#x3D;부분-에서|Translit&#x3D;.bu.bun.e.seo|LTranslit&#x3D;.bu.bun</span><br><span class="line">3	게임과	게임	NOUN	NN+CP	Polite&#x3D;Form	7	compound	_	MSeg&#x3D;게임-과|Translit&#x3D;.ge.im.gwa|LTranslit&#x3D;.ge.im</span><br><span class="line">4	우리	_	PRON	PRP	Person&#x3D;1	6	compound	_	Translit&#x3D;.u.ri|LTranslit&#x3D;_</span><br><span class="line">5	일상	_	NOUN	NN	_	6	compound	_	Translit&#x3D;.il.sang|LTranslit&#x3D;_</span><br><span class="line">6	생활	_	NOUN	NN	_	3	conj	_	Translit&#x3D;.saeng.hwal|LTranslit&#x3D;_</span><br><span class="line">7	사이의	사이	NOUN	NN+CM	Case&#x3D;Gen|Polite&#x3D;Form	8	nmod:poss	_	MSeg&#x3D;사이-의|Translit&#x3D;.sa.i.yi|LTranslit&#x3D;.sa.i</span><br><span class="line">8	유사점을	유사점	NOUN	NN+CM	Case&#x3D;Acc|Polite&#x3D;Form	9	obj	_	MSeg&#x3D;유사점-을|Translit&#x3D;.yu.sa.jeom.eul|LTranslit&#x3D;.yu.sa.jeom</span><br><span class="line">9	찾을	_	VERB	VV	Form&#x3D;Adn	10	acl:relcl	_	Translit&#x3D;.chaj.eul|LTranslit&#x3D;_</span><br><span class="line">10	수	_	NOUN	NNB	_	11	nsubj	_	Translit&#x3D;.su|LTranslit&#x3D;_</span><br><span class="line">11	있습니다	_	ADJ	JJ	Mood&#x3D;Ind|VerbForm&#x3D;Fin	0	root	_	SpaceAfter&#x3D;No|Translit&#x3D;.iss.seub.ni.da|LTranslit&#x3D;_</span><br><span class="line">12	.	.	PUNCT	.	_	11	punct	_	Translit&#x3D;.|LTranslit&#x3D;_</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h4 id="code-switching-cs-and-transliterate-tlit-task">Code-switching (CS) and Transliterate (Tlit) task</h4>
<ul>
<li>Code-switching: 여러 언어가 한 문장에 등장하는 경우
<ul>
<li>ex) <code>I thought मौसम different होगा बस fog है</code></li>
</ul></li>
<li>Tlit: 음차 표기
<ul>
<li>ex) <code>I thought mosam different hoga bas fog hy</code></li>
</ul></li>
</ul>
<hr>
<h3 id="m-bert-의-cross-lingual-transferability-는-vocab-overlap-때문일까-no"><code>M-BERT</code> 의 cross-lingual transferability 는 vocab overlap 때문일까? → NO</h3>
<ul>
<li>vocab overlap: fine-tuning dataset (train) 의 word piece 와 evaluation dataset (test) 의 word piece 간의 overlap \[overlap = {| E_{train} ∩ E_{eval} |}{| E_{train} ∪ E_{eval}|}\]</li>
<li>검증 방식:
<ul>
<li>NER task 중 16개의 언어에 대한 in-house 데이터셋으로 가능한 언어쌍 (16 * 15 = 240 개) 에 대해 overlap을 구하고, trasfer score (F1) 를 report</li>
<li><em><strong>(결과) <code>M-BERT</code>는 vocab overlap 과 무관하게 generally 성능이 좋다. vocab overlap 이 0인 언어쌍에 대해서도 최소 40%의 F1 score 를 보인다. 반면 EN-BERT 는 vocab overlap 에 굉장히 많이 영향을 받는다.</strong></em> <img src="https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png" width="90%" alt="credit) http://www.dhgarrette.com/papers/pires_multilingual_bert_acl2019_slides.pdf"></li>
</ul></li>
</ul>
<hr>
<h3 id="m-bert의-cross-lingual-transferability-는-언어의-typological-특징-때문일까-yes"><code>M-BERT</code>의 cross-lingual transferability 는 언어의 typological 특징 때문일까? → YES</h3>
<ul>
<li><p>근거: POS accuracy of <code>ur</code> → <code>hi</code> (91%) while <code>en</code>→ <code>ja</code> (49.4%) (둘 다 다른 script 를 사용하는 언어쌍, a.k.a vocab overlap ~= 0)</p></li>
<li><p>typological features<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/P12-1066.pdf&gt;
">[2]</span></a></sup> <img src="https://user-images.githubusercontent.com/24843996/85156077-8553ff80-b294-11ea-8a8d-5478762d6b43.png?style=centerme" width="85%" alt="typological features"> <br></p></li>
<li><p><em><strong>(결과) 공통된 typological feature 의 개수가 많을수록 transferabiltiy 향상</strong></em> <img src="https://user-images.githubusercontent.com/24843996/85156248-c3e9ba00-b294-11ea-8ca1-b3e74ea88860.png?style=centerme" width="45%"></p></li>
<li><p><em><strong>(결과) 여러 typological features 중에서 SOV order 와 AN order의 영향을 비교했을 때 전자가 더 영향이 큼</strong></em> <img src="https://user-images.githubusercontent.com/24843996/85156350-ea0f5a00-b294-11ea-862f-64ab5e3166f3.png?style=centerme" width="50%"></p></li>
</ul>
<h3 id="m-bert의-cross-lingual-transferability-는-cs-혹은-tlit-까지-적용될-수-있을까-cs-yes-tlit-no"><code>M-BERT</code>의 cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? → CS (YES) / Tlit (NO)</h3>
<ul>
<li>CS 실험 목적: Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would beneﬁt to an even larger degree from a shared multilingual representation.</li>
<li>Tlit 실험 목적: Generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that <em><code>M-BERT</code> was not pre-trained on text that looks like the target</em>.</li>
<li><strong><em>(결과) <code>M-BERT</code>는 CS text 에 좋은 성능을 보임 (90.56% ⇒ 86.59%). 하지만 Tlit 은 이러한 종류의 데이터에 학습되지 않고서는 trasferability를 기대하기 어려움 (85.64% ⇒ 50.41%)</em></strong> <img src="https://user-images.githubusercontent.com/24843996/85159242-34dea100-b298-11ea-9922-3b1acc66ef7b.png?style=centerme" width="60%" alt="credit) https://www.youtube.com/watch?v=ZGZy_GrFkAY"></li>
</ul>
<h3 id="m-bert의-feature-space">4. <code>M-BERT</code>의 feature space</h3>
<ul>
<li>WMT16 병렬 코퍼스를 사용해서 언어쌍 간의 NN accuracy 측정</li>
<li><strong><em>(결과) 중간 layer에서 linguistic information을 공유하고 이는 언어에 관계없이 비슷하게 나타남</em></strong> <img src="https://user-images.githubusercontent.com/24843996/85160445-7ae83480-b299-11ea-8ff2-3209922e78e7.png?style=centerme" width="45%"></li>
</ul>
<h2 id="my-thoughts-on-the-results">My Thoughts on the Results</h2>
<h3 id="vocab-overlap-실험에서-en-bert와의-비교는-정당한가">vocab overlap 실험에서 <code>EN-BERT</code>와의 비교는 정당한가?</h3>
<ul>
<li>제 추측으로는, vocab overlap 실험에서 <code>M-BERT</code> 만을 보았을 때, 이 경향성이 overlap에 영향을 받는 것인지 아닌지 판단하기 어려웠기 때문에 상대 비교를 할만한 결과가 필요해서 EN-BERT의 실험 결과를 넣은 것 같다.
<ul>
<li>아래 이미지에서 corr 을 보면 어느 정도 유의미해 보이는 양의 상관관계가 나올 것 같고, 그렇다면 영향을 받는다고 해석해야할 수도 있지만,</li>
<li>vocab overlap이 0임에도 40%의 성능을 보이므로 영향을 받는다고 하기도 애매한 상황이 아니었을까? <img src="https://user-images.githubusercontent.com/24843996/85162060-ac61ff80-b29b-11ea-90a2-7e497a5196a9.png?style=centerme" width="70%"></li>
</ul></li>
<li>그렇지만 EN-BERT와의 비교가 정당하다고 생각하긴 어려운 것 같다.
<ul>
<li>NER 예측 task 는 sent -&gt; model (either <code>M-BERT</code> or EN-BERT) -&gt; last activation -&gt; add.layer -&gt; NER prediciton 로 진행되는데, sent 가 영어가 아닌 경우 tok 단계에서부터 <code>unk</code>으로 인식될 가능성이 높기 때문에 transfer는 고사하고 fine-tuning도 어려울 수 있다. 이 때문에 논문에서 EN-BERT와 XLM을 비교하는데, Indo-european 인 (<code>de</code>, <code>nl</code>, <code>es</code>) 에 대해서만 나와있다. (Table 3)</li>
<li>영어와 alphabet 이 비슷하면, unk 이 나오지 않을 가능성이 높다고 생각해서 위의 결과가 EN-BERT로 얻어진 것에 대해 크게 거부감이 들지 않았고, 오히려 CJK 에 대해서 보였어야 하는 것 아닌가 하는 의심이 들었다.</li>
<li>그래서 🤗 로 간단하게 tokenize 결과를 비교해보았다.</li>
<li>sent 가 <code>es</code> 인 경우
<ul>
<li>sent: Por su parte , el Abogado General de Victoria , Rob Hulls , indicó que no hay nadie que controle que las informaciones contenidas en CrimeNet son veraces .</li>
<li><code>M-BERT</code> tok: Por su parte , el Ab ##oga ##do General de Victoria , Rob Hull ##s , ind ##ic ##ó que no hay nadie que controle que las informa ##ciones conte ##nida ##s en Crime ##Net son vera ##ces .</li>
<li>EN-BERT tok: Po ##r su part ##e , el A ##bo ##gado General de Victoria , Rob Hull ##s , in ##dic ##ó que no hay na ##die que control ##e que las inform ##ac ##ione ##s con ##ten ##idas en Crime ##Net son ve ##race ##s .</li>
</ul></li>
<li>sent 가 <code>ko</code> 인 경우
<ul>
<li>sent: 언어(言語)에 대한 정의는 여러가지 시도가 있었다.</li>
<li><code>M-BERT</code> tok: 언 ##어 ( 言 語 ) 에 대한 정 ##의 ##는 여러 ##가지 시 ##도가 있었다 .</li>
<li>EN-BERT tok: [UNK] ( [UNK] [UNK] ) [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] .</li>
</ul></li>
<li><strong><em>위의 결과로 미루어보아, vocab overlap 이 낮으면서 성능도 낮았던 점들의 언어쌍에는 EN-BERT에서 unk이 많이 나왔던 언어가 포함되어 있지 않을까하는 생각이 들었고, 비교가 공정하지 않다는 생각이 들었다.</em></strong></li>
</ul></li>
<li>또한 tlit. 을 <code>M-BERT</code> 가 못하는 이유로, pre-train step에서 tlit. corpus 가 없었기 때문이라고 언급하였는데 EN-BERT 또한 같은 이유로 성능이 낮을 수 밖에 없었을 것이라고 생각.</li>
</ul>
<h3 id="sov-order-가-중요한-점이었을까">SOV order 가 중요한 점이었을까?</h3>
<ul>
<li>논문에서는 아래 표에서 SVO -&gt; SVO (81.55) &gt; SVO -&gt; SOV (66.52) 라는 점 때문에 SOV order 가 가장 중요하다고 주장한다.
<ul>
<li>하지만 반대로 SOV -&gt; SOV (64.22) &gt; SOV -&gt; SVO (63.98) 의 차이가 적게 나는 점은 설명할 수 없다. <img src="https://user-images.githubusercontent.com/24843996/85165966-93f4e380-b2a1-11ea-9290-8451958499e3.png?style=centerme" width="50%"></li>
</ul></li>
<li>제 추측으로는, 오히려 각 그룹을 구성하는 언어와 그 언어들이 Wikipedia 에서 차지하는 비율, 즉 <code>M-BERT</code> 학습에 영향을 많이 끼친 언어가 중요한 역할을 했을 수도 있을 것 같다.
<ul>
<li>SVO languages: Bulgarian, Catalan, Czech, Danish, English, Spanish, Estonian, Finnish, French, Galician, Hebrew, Croatian, Indonesian, Italian, Latvian, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, and Chinese.</li>
<li>SOV Languages: Basque, Farsi, Hindi, Japanese, Korean, Marathi, Tamil, Telugu, Turkish, and Urdu.
<ul>
<li>Urdu -&gt; Hindi 의 성능이 91% 였던 점을 고려하면 이 중 어떤 언어쌍에서 굉장히 낮은 성능을 보였을 것으로 예상 (평균이 64.22 이어야하므로)</li>
<li>그룹의 평균치를 보지 않았다면 다른 해석이 가능했을 수도..?</li>
</ul></li>
</ul></li>
<li>SVO order 가 비슷하면 -&gt; transfer 가 잘된다! 라는 주장을 하고 싶었다면 비교하려는 대상 언어쌍들간에 SVO order 빼고는 조건을 동일하게 만족시켰어야 하지 않을까하는 아쉬움이 남는다.</li>
</ul>
<h3 id="feature-space">Feature space</h3>
<ul>
<li><p>MLM은 보통 중간 layer 에서 semantic 한 성질이 가장 두드러지게 나타나는 것 같다. <img src="https://user-images.githubusercontent.com/24843996/85170841-ed144580-b2a8-11ea-9db9-099c019568f3.png?style=centerme" alt="credit) The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives" width="60%"></p>
<figure>
<img src="https://user-images.githubusercontent.com/24843996/85170937-1208b880-b2a9-11ea-81e8-c1102286646a.png" alt="credit) BERTScore: Evaluating Text Generation with BERT"><figcaption aria-hidden="true">credit) BERTScore: Evaluating Text Generation with BERT</figcaption>
</figure></li>
</ul>
<h3 id="논문의-분석-내용">논문의 분석 내용</h3>
<ul>
<li>논문에서 분석한 결론이 지나친 일반화가 아닐까하는 생각도 든다.</li>
<li>일단 <code>M-BERT</code>가 zero-shot transferability 가 높은 이유는 어쩌면 같은 내용의 Wikipedia 문서로 학습했기 때문일 수 있다.
<ul>
<li>만약에 한국어는 동일 내용에 대한 번역된 문서가 없는 (e.g., 네이버 블로그) 로 학습하고, 영어도 영어 나름대로의 번역문이 없는 문서로 학습된 BERT 였더라도 같은 결과가 도출되었을지는 모르겠다.</li>
</ul></li>
<li><code>M-BERT</code> 의 학습 데이터의 특징에서 기인한 특징이 얼마나 되는지도 궁금하다. 오히려 여기에서 영향을 많이 받았을 수도 있을 것 같다.</li>
</ul>
<h2 id="references">References</h2>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P19-1493.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/P19-1493.pdf</a><a href="#fnref:1" rev="footnote"> ↩︎</a></span>
</li>
<li id="fn:2">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P12-1066.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/P12-1066.pdf</a><a href="#fnref:2" rev="footnote"> ↩︎</a></span>
</li>
</ol>
</div>
</div>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a><a class="link-muted mr-2" rel="tag" href="/tags/paper/">paper</a><a class="link-muted mr-2" rel="tag" href="/tags/BERT/">BERT</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/07/20/Zero-to-one-by-peter/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Zero to One (제로투원)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/"><span class="level-item">한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/';
            this.page.identifier = '2020/06/20/How-multilingual-is-multilingual-BERT/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'inmoonlight' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#abstract"><span class="mr-2">1</span><span>Abstract</span></a></li><li><a class="is-flex" href="#motivation"><span class="mr-2">2</span><span>Motivation</span></a></li><li><a class="is-flex" href="#main-idea"><span class="mr-2">3</span><span>Main Idea</span></a></li><li><a class="is-flex" href="#main-findings"><span class="mr-2">4</span><span>Main Findings</span></a></li><li><a class="is-flex" href="#detailed-experiments-and-results"><span class="mr-2">5</span><span>Detailed Experiments and Results</span></a><ul class="menu-list"><li><a class="is-flex" href="#preliminaries"><span class="mr-2">5.1</span><span>Preliminaries</span></a><ul class="menu-list"><li><a class="is-flex" href="#ner-task"><span class="mr-2">5.1.1</span><span>NER task</span></a></li><li><a class="is-flex" href="#pos-task"><span class="mr-2">5.1.2</span><span>POS task</span></a></li><li><a class="is-flex" href="#code-switching-cs-and-transliterate-tlit-task"><span class="mr-2">5.1.3</span><span>Code-switching (CS) and Transliterate (Tlit) task</span></a></li></ul></li><li><a class="is-flex" href="#m-bert-의-cross-lingual-transferability-는-vocab-overlap-때문일까-no"><span class="mr-2">5.2</span><span>M-BERT 의 cross-lingual transferability 는 vocab overlap 때문일까? → NO</span></a></li><li><a class="is-flex" href="#m-bert의-cross-lingual-transferability-는-언어의-typological-특징-때문일까-yes"><span class="mr-2">5.3</span><span>M-BERT의 cross-lingual transferability 는 언어의 typological 특징 때문일까? → YES</span></a></li><li><a class="is-flex" href="#m-bert의-cross-lingual-transferability-는-cs-혹은-tlit-까지-적용될-수-있을까-cs-yes-tlit-no"><span class="mr-2">5.4</span><span>M-BERT의 cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? → CS (YES) / Tlit (NO)</span></a></li><li><a class="is-flex" href="#m-bert의-feature-space"><span class="mr-2">5.5</span><span>4. M-BERT의 feature space</span></a></li></ul></li><li><a class="is-flex" href="#my-thoughts-on-the-results"><span class="mr-2">6</span><span>My Thoughts on the Results</span></a><ul class="menu-list"><li><a class="is-flex" href="#vocab-overlap-실험에서-en-bert와의-비교는-정당한가"><span class="mr-2">6.1</span><span>vocab overlap 실험에서 EN-BERT와의 비교는 정당한가?</span></a></li><li><a class="is-flex" href="#sov-order-가-중요한-점이었을까"><span class="mr-2">6.2</span><span>SOV order 가 중요한 점이었을까?</span></a></li><li><a class="is-flex" href="#feature-space"><span class="mr-2">6.3</span><span>Feature space</span></a></li><li><a class="is-flex" href="#논문의-분석-내용"><span class="mr-2">6.4</span><span>논문의 분석 내용</span></a></li></ul></li><li><a class="is-flex" href="#references"><span class="mr-2">7</span><span>References</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Book/"><span class="level-start"><span class="level-item">Book</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Essay/"><span class="level-start"><span class="level-item">Essay</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/ML/Data-Analysis/"><span class="level-start"><span class="level-item">Data Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/Quantum-Computing/"><span class="level-start"><span class="level-item">Quantum Computing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Ops/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time datetime="2021-07-10T16:03:00.000Z">2021-07-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/11/Git-merge-strategy/">Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Ops/">Ops</a> / <a class="link-muted" href="/categories/Ops/Git/">Git</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-03-02T19:22:00.000Z">2021-03-03</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/03/PyTorch-view-transpose-reshape/">PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-21T14:22:00.000Z">2021-02-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/21/PyTorch-IterableDataset/">PyTorch의 IterableDataset을 사용해서 데이터 불러오기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-04T05:22:00.000Z">2021-02-04</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/04/Pandas-Dataframe-iterations/">Pandas Dataframe의 다양한 iteration 방법 비교</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a></p></div></article><article class="media"><a class="media-left" href="/2021/01/10/Adieu-2020-and-happy-new-year/"><p class="image is-64x64"><img class="thumbnail" src="/assets/images/2020-2021.jpg" alt="2020년을 정리하고, 2021년을 맞이하는 글"></p></a><div class="media-content size-small"><p><time datetime="2021-01-10T12:13:00.000Z">2021-01-10</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/10/Adieu-2020-and-happy-new-year/">2020년을 정리하고, 2021년을 맞이하는 글</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Essay/">Essay</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a><p class="size-small"><span>&copy; 2024 Jihyung Moon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://inmoonlight.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>