<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>PyTorch의 IterableDataset을 사용해서 데이터 불러오기 - Space Moon</title><meta description="PyTorch 1.2 이상부터 torch.utils.data 에서는 크게 map-style dataset (torch.utils.data.Dataset) 과 iterable dataset (torch.utils.data.IterableDataset) 의 두 종류의 데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는 IterableDataset 을 사용"><meta property="og:type" content="blog"><meta property="og:title" content="PyTorch의 IterableDataset을 사용해서 데이터 불러오기"><meta property="og:url" content="https://inmoonlight.github.io/2021/02/21/pytorch-IterableDataset/"><meta property="og:site_name" content="Space Moon"><meta property="og:description" content="PyTorch 1.2 이상부터 torch.utils.data 에서는 크게 map-style dataset (torch.utils.data.Dataset) 과 iterable dataset (torch.utils.data.IterableDataset) 의 두 종류의 데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는 IterableDataset 을 사용"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://inmoonlight.github.io/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme"><meta property="article:published_time" content="2021-02-21T14:22:00.000Z"><meta property="article:modified_time" content="2024-01-21T16:59:56.997Z"><meta property="article:author" content="Jihyung Moon"><meta property="article:tag" content="pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://inmoonlight.github.io/2021/02/21/pytorch-IterableDataset/"},"headline":"Space Moon","image":[],"datePublished":"2021-02-21T14:22:00.000Z","dateModified":"2024-01-21T16:59:56.997Z","author":{"@type":"Person","name":"Jihyung Moon"},"description":"PyTorch 1.2 이상부터 torch.utils.data 에서는 크게 map-style dataset (torch.utils.data.Dataset) 과 iterable dataset (torch.utils.data.IterableDataset) 의 두 종류의 데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는 IterableDataset 을 사용"}</script><link rel="canonical" href="https://inmoonlight.github.io/2021/02/21/pytorch-IterableDataset/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131297969-1" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131297969-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="alternate" href="/feed.xml" title="Space Moon" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/inmoonlight"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2021-02-21T14:22:00.000Z" title="2021-02-21T14:22:00.000Z">2021-02-21</time><span class="level-item"><a class="link-muted" href="/categories/Tech/">Tech</a><span> / </span><a class="link-muted" href="/categories/Tech/Engineering/">Engineering</a></span><span class="level-item">10 minutes read (About 1474 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">PyTorch의 IterableDataset을 사용해서 데이터 불러오기</h1><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>PyTorch 1.2 이상부터 <code>torch.utils.data</code> <!-- hexo-inject:begin --><!-- hexo-inject:end -->에서는 크게
map-style dataset (<code>torch.utils.data.Dataset</code>) 과 iterable
dataset (<code>torch.utils.data.IterableDataset</code>) 의 두 종류의
데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는
<code>IterableDataset</code> 을 사용하는 것이 좋은데,
<code>Dataset</code> 과는 딜리 아직 개발되어야 할 기능이 더 필요한
클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다.</p>
<a id="more"></a>
<h2 id="map-style-dataset">Map-style Dataset</h2>
<p>1.2 이하 버전에서 사용되던 map-style dataset은 memory에 모든 데이터를
업로드할 수 있을 때 사용하는 가장 일반적인 dataset type 이다. custom
dataset class를 생성하고자 할 때 <code>torch.utils.data.Dataset</code>
을 상속받아 <code>__len__</code> , <code>__getitem__</code> 을 구현하면
된다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[<span class="string">'text'</span>][index]</span><br></pre></td></tr></table></figure>
<h2 id="iterable-dataset">Iterable Dataset</h2>
<p>하지만 학습 데이터가 메모리에 다 올라가지 않는 경우가 발생할 수 있다.
이 문제를 해결할 수 있는 다양한 방법 중에 하나로,
<code>torch.utils.data.IterableDataset</code> 을 사용하는 방법이 있다.
Map-style Dataset과 비슷하게
<code>torch.utils.data.IterableDataset</code> 을 상속받아서 custom
dataset class를 생성하고, <code>__iter__</code> 를 선언하면 된다.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> IterableDataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span><span class="params">(IterableDataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_path)</span>:</span></span><br><span class="line">        self.data_path = data_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        iter_csv = pd.read_csv(self.data_path, sep=<span class="string">'\t'</span>, iterator=<span class="literal">True</span>, chunksize=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> iter_csv:</span><br><span class="line">            line = line[<span class="string">'text'</span>].item()</span><br><span class="line">            <span class="keyword">yield</span> line</span><br></pre></td></tr></table></figure>
<p><code>Dataset</code>이 batch data를 생성할 때
<code>map_dataset[index]</code>를 사용한다면,
<code>IterableDataset</code>은 <code>next(iterable_dataset)</code> 을
사용한다. 이 때문에 <code>DataLoader</code>를 통해
<code>IterableDataset</code>을 불러와서 사용하게 되면
<code>sampler</code> 옵션의 사용이 어렵다. 그래서 random suffling 을
하고 싶다면 미리 데이터셋을 shuffling 한 이후에 불러오는 것이 좋다.</p>
<h2 id="going-parallel">Going Parallel</h2>
<p><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset" rel="external nofollow noopener noreferrer" target="_blank">PyTorch
공식문서</a>에 따르면 <code>IterableDataset</code>을
<code>num_workers &gt; 0</code>의 조건에서 사용할 때 특별히 다음을
유념할 것을 제안하고 있다.</p>
<blockquote>
<p>When <code>num_workers &gt; 0</code>, each worker process will have a
different copy of the dataset object, so it is often desired to
configure each copy independently to avoid having duplicate data
returned from the workers. <code>get_worker_info()</code>, when called
in a worker process, returns information about the worker. It can be
used in either the dataset’s <code>__iter__()</code> method or the
<code>DataLoader</code> ‘s <code>worker_init_fn</code> option to modify
each copy’s behavior.</p>
</blockquote>
<p>위의 문장을 이해하려면 <code>num_workers</code> 에 대한 이해와,
<code>num_workers &gt; 0</code> 일 때 <code>IterDataset</code> 에서 어떤
현상이 일어나는지 알아야한다.</p>
<p><img src="/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme" alt="num_workers == 2 인 경우 발생하는 모습이다. 위의 두 라인은 subprocess이며, 맨 아래 라인은 main process이다. 파란색 박스는 single datapoint를 로딩하는 것을 의미하며 붉은색 박스는 로딩된 데이터를 모델에 전달하는 프로세스를 의미한다. (image credit: https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd)" width="90%"></p>
<p><code>num_workers</code>는 데이터셋을 불러올 때 사용할 subprocess의
개수이다. <code>num_workers == 0</code> 은 main process에서 데이터를
불러오고 모델에 pass하는 작업을 모두 수행한다는 뜻이며,
<code>num_workers == 2</code>는 subprocess 2개에서 데이터를 불러오고
main process에서는 subprocess에서 불러온 데이터를 model에 pass하는
역할만 담당한다. 따라서 <code>num_workers &gt; 0</code> 일 때 data
loading에서의 병목이 줄어들며 gpu utilization 을 100% 가까이 끌어올릴 수
있다.</p>
<p>그럼, <code>num_workers &gt; 0</code> 일 때 어떤 현상이 발생하는지
살펴보자.</p>
<h3 id="map-style-dataset-1">Map-Style Dataset</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        worker = torch.utils.data.get_worker_info()</span><br><span class="line">        worker_id = worker.id <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        </span><br><span class="line">        start = time.time()</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        end = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.data[index], worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = range(<span class="number">16</span>)</span><br><span class="line">map_dataset = MyMapDataset(data)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_workers == 0</code> 인 경우</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_workers == 2</code> 인 경우</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>의도한대로, subprocess 별로 서로 다른 데이터를 불러오는 것을 알 수
있다.</p>
<h3 id="iterable-dataset-1">Iterable Dataset</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span><span class="params">(IterableDataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> self.data:</span><br><span class="line">            worker = torch.utils.data.get_worker_info()</span><br><span class="line">            worker_id = worker.id <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        </span><br><span class="line">            start = time.time()</span><br><span class="line">            time.sleep(<span class="number">0.1</span>)</span><br><span class="line">            end = time.time()</span><br><span class="line">        </span><br><span class="line">            <span class="keyword">yield</span> x, worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = range(<span class="number">16</span>)</span><br><span class="line">iterable_dataset = MyIterableDataset(data)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_workers == 0</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_workers == 2</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>⚠️ worker 0과 worker 1에서 같은 데이터를 로딩하고 있다. 이 점이
공식문서에서 지적하고 있는 내용이다. 각 워커별로 같은
<code>__iter__()</code>를 사용하기 때문에 같은 데이터를 로딩하게 된다.
<strong>이를 방지하기 위해서는 <code>worker_init_fn</code> 에서 직접
워커 별 데이터를 재분배 시켜줘야 한다.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker_init_fn</span><span class="params">(_)</span>:</span></span><br><span class="line">    worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">    </span><br><span class="line">    dataset = worker_info.dataset</span><br><span class="line">    worker_id = worker_info.id</span><br><span class="line">    split_size = len(dataset.data) // worker_info.num_workers</span><br><span class="line">    </span><br><span class="line">    dataset.data = dataset.data[worker_id * split_size: (worker_id + <span class="number">1</span>) * split_size]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>, worker_init_fn=worker_init_fn)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><code>worker_init_fn</code> 을 통해 분배시켜준 결과 worker 0과 worker
1 에서 다른 데이터를 순차적으로 불러오는 것을 알 수 있다 🙂</p>
<h2 id="conclusions">Conclusions</h2>
<ul>
<li><code>IterableDataset</code> 은 데이터가 메모리에 올라가지 않을만큼
클 때 사용하면 좋다.</li>
<li><code>Dataset</code>과 다르게 <code>__iter__()</code>를 선언해서
데이터를 부른다.
<ul>
<li>하지만 이 특징 때문에 <code>Sampler</code> 와 함께 사용할 수
없다.</li>
<li>또한 <code>num_workers &gt; 0</code> 인 세팅에서는 각 워커에서 다른
데이터를 불러올 수 있도록 <code>worker_init_fn</code>을 선언해야
한다.</li>
</ul></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd" rel="external nofollow noopener noreferrer" target="_blank">How
to Build a Streaming DataLoader with PyTorch | by David MacLeod |
Speechmatics | Medium</a></li>
<li><a href="https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html">https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html</a></li>
<li><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset" rel="external nofollow noopener noreferrer" target="_blank">torch.utils.data
— PyTorch 1.7.1 documentation</a></li>
</ul>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/pytorch/">pytorch</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/03/03/pytorch-view-transpose-reshape/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/02/04/pandas-dataframe-iterations/"><span class="level-item">Pandas Dataframe의 다양한 iteration 방법 비교</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://inmoonlight.github.io/2021/02/21/pytorch-IterableDataset/';
            this.page.identifier = '2021/02/21/pytorch-IterableDataset/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'inmoonlight' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#map-style-dataset"><span class="mr-2">1</span><span>Map-style Dataset</span></a></li><li><a class="is-flex" href="#iterable-dataset"><span class="mr-2">2</span><span>Iterable Dataset</span></a></li><li><a class="is-flex" href="#going-parallel"><span class="mr-2">3</span><span>Going Parallel</span></a><ul class="menu-list"><li><a class="is-flex" href="#map-style-dataset-1"><span class="mr-2">3.1</span><span>Map-Style Dataset</span></a></li><li><a class="is-flex" href="#iterable-dataset-1"><span class="mr-2">3.2</span><span>Iterable Dataset</span></a></li></ul></li><li><a class="is-flex" href="#conclusions"><span class="mr-2">4</span><span>Conclusions</span></a></li><li><a class="is-flex" href="#references"><span class="mr-2">5</span><span>References</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Startup/"><span class="level-start"><span class="level-item">Startup</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Tech/"><span class="level-start"><span class="level-item">Tech</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Tech/Engineering/"><span class="level-start"><span class="level-item">Engineering</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Tech/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Tech/Quantum-Computing/"><span class="level-start"><span class="level-item">Quantum Computing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time datetime="2024-01-13T12:14:00.000Z">2024-01-13</time></p><p class="title is-6"><a class="link-muted" href="/2024/01/13/lessons-learned-in-my-first-2-years-as-a-startup-founder/">Lessons learned in my first 2 years as a startup founder</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Startup/">Startup</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-07-10T16:03:00.000Z">2021-07-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/11/git-merge-strategy/">Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Tech/">Tech</a> / <a class="link-muted" href="/categories/Tech/Engineering/">Engineering</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-03-02T19:22:00.000Z">2021-03-03</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/03/pytorch-view-transpose-reshape/">PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Tech/">Tech</a> / <a class="link-muted" href="/categories/Tech/Engineering/">Engineering</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-21T14:22:00.000Z">2021-02-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/21/pytorch-IterableDataset/">PyTorch의 IterableDataset을 사용해서 데이터 불러오기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Tech/">Tech</a> / <a class="link-muted" href="/categories/Tech/Engineering/">Engineering</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-04T05:22:00.000Z">2021-02-04</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/04/pandas-dataframe-iterations/">Pandas Dataframe의 다양한 iteration 방법 비교</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Tech/">Tech</a> / <a class="link-muted" href="/categories/Tech/Engineering/">Engineering</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a><p class="size-small"><span>&copy; 2024 Jihyung Moon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://inmoonlight.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>