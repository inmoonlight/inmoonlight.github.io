{"pages":[{"title":"Hello,  안녕하세요  👋","text":"I'm Jihyung Moon, Co-founder &amp; CTO at SoftlyAI Our team build AI junior employees so that people can focus on bigger problems. 💹 For investors: AILookUp | 🧑‍💻 For SaaS makers: AI Receptionist API My current interests are Building a great AI product Effective and efficient AI product development Scale Work and Education [JAN 2022 ~ ] CTO / Co-founder, SoftlyAI [OCT 2020 ~ DEC 2021] NLP Research Engineer, Upstage [DEC 2018 ~ OCT 2020] NLP Research Engineer, Papago, NAVER Corporation [FEB 2018 ~ DEC 2018] ML Research Engineer, Search Modeling, Search &amp; Clova, NAVER Corporation [MAR 2016 ~ FEB 2018] M.S., Datamining Lab, Seoul National University [MAR 2011 ~ FEB 2016] B.S., Major in Chemical and Biological Engineering and minor in Industrial Engineering, Seoul National University Publications Analyzing Norm Violations in Real-Time Live-Streaming Chat. EMNLP 2023 [paper] Jihyung Moon*, Dong-Ho Lee*, Hyundong J. Cho, Woojeong Jin, Chan Young Park, Minwoo Kim, Jay Pujara and Sungjoon Park KOLD: Korean Offensive Language Dataset. EMNLP 2022 [paper] Younghoon Jeong, Juhyun Oh, Jaimeen Ahn, Jongwon Lee, Jihyung Moon, Sungjoon Park, and Alice Oh KLUE: Korean Language Understanding Evaluation. NeurIPS 2021 Datasets and Benchmarks Track [paper] [github] Sungjoon Park*, Jihyung Moon *, Sungdong Kim*, Won Ik Cho*, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho PATQUEST: Papago Translation Quality Estimation. Proceedings of the Fifth Conference on Machine Translation [paper] Yujin Baek*, Zae Myung Kim*, Jihyung Moon, Hyunjoong Kim, and Eunjeong L. Park BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection. SocialNLP@ACL 2020 [paper] [github] [slide] Jihyung Moon *, Won Ik Cho*, and Junbum Lee Revisiting Round-Trip Translation for Quality Estimation. EAMT 2020 [paper] Jihyung Moon, Hyunchang Cho, and Eunjeong L. Park Talks AI and Marginarlized Language. July 2023. ICML Panel Discussion. KLUE and XTREME. Sep 2021. XTREME Talks (Google Internal Seminar Series). KLUE: Korean Language Understanding Evaluation. Sep 2021. BigScience Episode #2. [youtube] [AI와 저작권법] 내가 만든 AI 모델은 합법일까, 불법일까?. Feb 2021. Boostcamp AI Tech. 서비스 관점에서의 AI 모델 개발. Nov 2020. 멋쟁이 사자처럼. 이 선 넘으면 침범이야, BEEP!. Sep 2020. PyCon. [slide] [youtube] BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection. July 2020. 카카오브레인. [slide] 파파고가 언어를 배우는 방법. June 2020. 인문학도를 위한 언어과학 콜로키움, 서울대학교 동아문화연구소. [slide] 온라인 뉴스 댓글 생태계를 흐리는 어뷰저 분석기. Oct 2019. 데이터야놀자. [slide] Academic Services Reviewer for EMNLP 2021, EMNLP 2022, ACL 2023, and more Patents 활동 데이터 분석을 통해 비정상 사용자 그룹을 탐지하는 방법 및 시스템 (METHOD AND SYSTEM FOR DETECTING ABUSER USING LOG DATA ANALYSIS), KR 1022091000000, filed 5 Sep 2018, issued 22 Jan 2021. [kipris] 김태욱, 문지형, 최인식, 박효균 Projects [NOV 2018] PyTorchTutorial for Beginners [github] [JUL 2017] Tensorflow Implementation of Relation Network (RN) [github] [JAN 2017 ~ FEB 2017] 구글 여성 소프트웨어 캠프 (Develop with Google) 1기 Things you don't need to know Emoticon creator, debuted with \"Daily life of SOTA, an ML Research Engineer\"","link":"/about/index.html"}],"posts":[{"title":"Attention in NLP","text":"You can't cram the meaning of a whole %&amp;!# sentence into a single &amp;!#* vector! Raymond Mooney Attention은 single vector에 한 문장의 의미를 완벽하게 담을 수 없기 때문에 필요한 순간에, 필요한 정보를 사용하기 위한 방법이다. 기본적으로 query vector와 key vector의 조합으로 attention weight가 계산된다. 여기서 \"조합\"의 방법에는 크게 두가지가 있다. 하나는 Additive Attention으로 query vector와 key vector에 feed-forward network를 적용한 것이고, 다른 하나는 Dot-Product Attention으로 문자그대로 query vector와 key vector의 dot-product를 이용한 것이다. 이번 글에서는 각 Attention 방법들과 이들의 장단점을 소개하려고 한다. Additive Attention Additive Attention은 query vector와 key vector의 조합으로 attention 값을 얻을 때 단일 hidden layer를 가진 feed-forward network를 이용한다. query vector (q) 와 key vector (k) 가 같은 dimension을 가질 필요가 없으며, dimension의 크기에 상관없이 좋은 성능을 보인다는 장점이 있다. Bilinear[2] \\[a(q, k)= q^{T} Wk\\] \\(q\\) 와 \\(k\\) 에 \\(W\\) 를 곱하는 방법이다. \\(q\\) 를 linear transform 시킨 후, \\(k\\) 와 dot-product를 한 것과 같다. Multi-layer Perceptron[3] \\[a(\\boldsymbol{q}, \\boldsymbol{k})=\\boldsymbol{w}_{2}^{\\top} \\tanh \\left(W_{1}[\\boldsymbol{q} ; \\boldsymbol{k}]\\right)\\] \\(q\\) 와 \\(k\\) 를 concat시킨 후 single hidden layer와 activation 함수로 tanh를 사용한 feed-forward network를 사용하였다. 이 방법은 익숙할 것이다. 왜냐하면 Neural Machine Translation by Jointly Learning to Align and Translate에서 이 attention을 이용했기 때문이다. 아래의 오른쪽 수식에서 \\(a\\)는 alignment model로 위에서 언급한 feed-forward network와 같은 역할을 한다. Dot-Product Attention Dot-Product attention은 \\(^{} \\)을 기반으로 attention weight를 구하는 방법이다. Additive attention과는 달리 hidden layer를 곱하는 과정이 추가되지 않아서 연산 속도와 space 측면에서 효율적이다. 하지만 반드시 \\(q\\)와 \\(k\\)의 dimension이 같아야 한다는 제약조건이 있으며, dimension이 클 때 학습에 방해가 된다는 단점이 있다. Dot-Product \\[a(\\boldsymbol{q}, \\boldsymbol{k})=\\boldsymbol{q}^{\\top} \\boldsymbol{k}\\] \\(q\\)와 \\(k\\)의 elment-wise product의 합이다. 이 연산의 특성 상, 반드시 dimension이 같아야 한다. 만약 \\(q\\)와 \\(k\\)의 각 요소가 독립적이고 평균이 0, 분산이 1 인 분포의 random variable이라면, \\({q}^{\\top}{k}\\) 는 평균이 0이고 분산이 dimension의 크기인 분포를 따른다. 분산이 증가되면 dot-product에 softmax를 취했을 때 어떤 값은 굉장히 크지만 대부분의 값은 굉장히 작게 만든다. 작은 값들은 back-propagation 시 gradient도 작기 때문에 전체적으로 학습이 잘 되지 않게 만든다. 따라서 dimension이 큰 경우 성능이 좋지 않다. Scaled Dot-Product[4] \\[a(\\boldsymbol{q}, \\boldsymbol{k})=\\frac{\\boldsymbol{q}^{\\top} \\boldsymbol{k}}{\\sqrt{|\\boldsymbol{d}|}}\\] Scaled Dot-Product는 Attention is All You Need 에서 처음으로 소개된 방법이다. Dot-Product의 dimension이 클 때 학습이 잘 되지 않는 단점을 극복하게 위해 dot-product 결과를 \\(q\\)의 dimension \\(d\\) (\\(k\\) 의 dimension 이기도 하다) 의 root 값으로 나누어주었다.[5] Conclusions Additive attention은 dimension에 상관없이 좋은 결과를 내고, attention을 계산하는 재료인 query vector와 key vector의 dimension에 상관없이 사용할 수 있다. 하지만 최근의 NLP trend라고 할 수 있는, 대량의 데이터를 굉장히 큰 모델로 학습시키는 방법에는 안그래도 많은 연산량에 부담이 되는 방법이다. 그래서 계산의 부담이 적으면서 dimension이 큰 경우에 대해서도 좋은 성능을 보이는 scaled dot-product 기반의 attention이 잘 쓰이는 편이다. References 1.전반적인 내용은 Graham Neubig의 NN4NLP Attention 강의를 참고했습니다. ↩︎ 2.Effective Approaches to Attention-based Neural Machine Translation ↩︎ 3.Neural Machine Translation by Jointly Learning to Align and Translate ↩︎ 4.Attention is All You Need ↩︎ 5.root로 나누어 준 이유는 scaled dot-product의 결과를 평균이 0, 분산이 1 인 분포로 만들기 위해서다. ↩︎","link":"/2020/01/27/Attention-in-NLP/"},{"title":"2020년을 정리하고, 2021년을 맞이하는 글","text":"2020년의 연말은 그 어느 때보다도 연말같지 않았다. 일 년을 주기로 움직였던 학생 때나 회사의 근로자일 때와는 처한 상황이 다르기도 했고, 코로나로 인한 고요함도 한 몫했던 것 같다. 새롭게 회사를 만들고 멤버들과 align되어 달려나갈 준비를 마친 시점이 11월 정도였고, 한창 KLUE 프로젝트가 달리고 있던 시기가 12월이었으니 12월 말이 연말보다는 월말의 느낌이었다. 게다가 코로나로 사회적 거리두기 단계가 격상되면서 연말파티를 하며 사람들과 강제로라도 일년을 회고하고 마무리하는 것조차 어려웠다. 따로 시간을 내서 마무리하기에는 체력적으로도, 정신적으로도 휴식이 더 필요했기 때문에 \"나의 연말은 이 프로젝트가 마무리되는 시점으로 유예한다!\"고 자기 합리화를 하며 연말과 연초를 보냈다. 그런데 떡하니 회고의 다짐의 주제를 담을 법한 이 글은 대체 무엇이냐. 작년 상반기의 프로젝트가 완료되고 한 템포 쉬어가던 4월을 제외하고 하반기를 자의반 타의반으로 쉴틈없이 달려왔던지라 번아웃으로 인한 슬럼프가 오기 시작했다. 나의 번아웃 극복 노하우 중의 하나로 당장 해야할 것 같은 일을 머리에서 지우고, 그 일을 해야하는 이유를 정리하는 것이 있다. 왜 나는 이 시간에 이 일을 하고 있는지 스스로를 이해시키고 정말 해야하는 일과 하면 좋을 일을 구분하면 현재의 내가 짊어진 무게를 덜어낼 수 있고, 꼭 짊어져야 할 무게를 감당할 의지를 다잡을 수 있어서 자연스럽게 슬럼프가 극복된다. 지금 정리하는 글은, 그래서 새해부터 찾아온 번아웃 극복의 일환이다. 최근에 사람들을 만날 때마다 가장 자주 들었던 질문은, \"그래서 어쩌다가 업스테이지에 합류하게 된거야?\" 이다. 물어보는 사람에 따라 여러 버전으로 대답을 했던 것 같다. 사실이 아닌 답변은 없었지만, 진짜 나의 생각을 꾹꾹 담아 전달해본 적도 없던 것 같다. 나는 사람들의 삶에 비록 미약할지라도, 긍정적인 변화를 불어넣고 싶다. 그리고 그 변화는 내가 살아오면서 느꼈던 불만족스러움의 영역을 만족스러움의 영역으로 만드는 것을 의미한다. 나는 사람들 개개인의 독창성이 존중받는 사회를 원한다. 이 때문에 똑똑함을 판단하는 기준을 시험을 잘 보는 것 정도로 규정하는 우리나라의 교육제도와 개인에 대한 제대로 된 이해없이 일부의 특성만으로 개인을 규정하고 판단하는 사회적 편견과 혐오를 개선시키고 싶었다. 우리나라의 교육제도를 개선시키기 위해서는 무엇을 해야할까? 내가 교육에 있어 많은 사람들에게 전달하고 싶은 메시지는, \"똑똑하고 지혜로운 사람은 자리에 앉아서 책에 적힌 개념을 외우거나 이해해서 시험문제를 실수없이 잘 풀고 맞추는 사람\"이 아니라는 것이다. 그렇기 때문에 하루의 대부분을 학원이나 학교에서 보내고 정작 인생에서 중요한 \"나라는 사람을 다각도에서 이해하기 위해 세상을 경험하는 시간\"을 가지지 못하는 것이 안타까웠다. 왜 다들 학원과 학교에서 오랜 시간을 보낼까? 그 것은 입시에서 뒤쳐지면 망할 것이라는 어른들의 과도한 두려움과 양극화로 인한 불공정한 사회라는 사실 혹은 인식 때문이다. 개인의 노력으로는 더 이상 좋은 조건의 환경을 극복할 수 없는 사회라는 생각이 강해질수록, 공정해보이는 수능을 통한 학벌 사다리를 타기 위해 사교육시장은 더욱 치열하게 불타오를 것이다. 다행인지 불행인지, 나는 그렇게 좋은 수저를 물고 태어난 편은 아니다. 그렇기 때문에 이 사회가 얼만큼 불공정한지, 부모님들이 느끼는 두려움은 정당한 것인지 직접 경험하고 나의 인생이 저물어갈 때쯤에야 내가 한국 교육에서 문제라고 느끼는 가설에 대한 답을 찾을 수 있을 것이라고 생각했다. 답을 찾는다면, 주저없이 학교를 세워서 나의 경험 혹은 유사한 경험을 했던 분들의 이야기를 전달하고 싶다. 사회적 편견과 혐오는 왜 문제이고 어떻게 해결할 수 있을까? 편견과 혐오의 근거가 되는 속성이 \"나\"를 구성하는 특징이고 이는 나의 노력으로 어찌할 수 없는 영역이거나 존중받아야 마땅할 선택이기 때문이다. 나의 성별은 여자다. 이 속성은 나의 의지로 그렇게 된 것이 아니다 (놀랍게도 500여년 전에는 어찌할 수 있는 영역이라고 생각했었다). 성적지향성, 장애도 마찬가지라고 생각한다. 이민 혹은 정치적 성향은 나의 선택과 생각의 영역이지 남에게 비난받아야할 속성은 아니다. 어떻게 한 사람을 겉으로 드러날 뿐인 얕은 특성만으로 많은 부분을 규정짓고 호와 불호를 가볍게 판단해버릴 수 있는걸까. 교육과는 다르게 사회적 편견과 혐오는 내가 가진 능력으로 해결하기 위해 더 빠른 시점에 도전해볼 가치가 있는 문제라고 여겨졌다. 표현을 하는 행위 자체는 어찌할 수 없더라도 (일정부분 표현의 자유와도 맞물려 있는 영역이기도 하고) 적어도 그 피해가 피해자에게까지는 닿지 않았으면 하는 마음이 있었다. 그래서 조그맣게나마 내가 할 수 있는 일부터 시작했고, 그 일이 계기가 되어 다양한 사람들과 기회를 마주하게 되었다. 처음에는 아무 것도 없었기 때문에 데이터셋을 만들고 연구로서의 가치를 만드는 것으로 시작했지만 더 큰 변화를 만들기 위해서는 더 많은 사람들과 함께 해야한다고 생각했다. 사람을 모으는 것이 내가 아니더라도 상관없었고, 그저 나와 방향이 같은 사람들과 무에서 유를 창조하고 싶었다. 그 마음이 원익님, 준범님, 성준님을 거쳐 루시, 활석님, 그리고 성킴까지 만나게 해주었고 업스테이지의 합류를 결정하게 해주었다. 결과적으로 Making AI Beneficial 이라는 넓은 비전 아래에서 하고 싶은 방향, 해야한다고 생각하는 방향의 일을 하고 있다고 생각한다. 8월 어느 날, 업스테이지(그 때는 너울이었던...)의 합류를 결정짓고 내가 이 곳에서 배우고 싶고, 이 곳을 가야만 한다고 생각했던 이유를 적어둔 일기가 있었는데 대부분이 실현되고 있다. 2021년에는 이전과는 조금 다른 방향으로의 성장을 꿈꾸고 싶다. 우선 함께 일하고 싶은 사람이 되고 싶다. soft skill 측면에서 내가 생각하는 같이 일하고 싶은 사람이란 겸손하면서 도전하기를 멈추지 않고, 사람을 이해하는 사람이다. hard skill 측면에서는 자신만의 엣지를 가진 사람이다. 그 영역이 기술일 수도 있겠지만 매니징이나 멘토링도 포함된다고 생각한다. 필요하다고 생각하는 내용을, 설사 그 내용이 아플지라도 부드럽고 주저없이 전달하는 사람이 되고 싶다. 보다 스스로에게 떳떳한 사람이 되고도 싶다. 하지만 가장 되고 싶은 모습은, 많은 것들에 익숙해져서 여유가 있는 사람이다. 나의 인생에 slack을 두고 싶고, 그 시간에 나를 포함해서 나의 도움을 필요로 하는 사람을 살피고 돌보고 싶다. 그 밖에도 2021년에는 보다 규칙적인 생활을 하길 바라고 마음편히 운동하고 여행다니며 친구들과 수다떨 수 있는 날이 오길 소망한다.","link":"/2021/01/10/Adieu-2020-and-happy-new-year/"},{"title":"Don&#39;t Stop Pretraining: Adapt Language Models to Domains and Tasks","text":"다양한 출처의 데이터로 학습한 pretrained model이 NLP task에서 좋은 성능을 보여주고 있다. 하지만 아직 주어진 labeled data 의 크기나 target domain의 코퍼스와 사전학습 코퍼스의 유사도가 특정 task의 결과에 얼마나 영향을 미치는지에 대해 알려진 바가 없다. 또한 RoBERTa와 같은 LM이 정말 다양한 task에 generalize될만큼의 다양한 source로 학습되었는지도 확실하지 않다. 이 논문에서는 pretrained model을 풀고자 하는 특정 task의 domain에 tailor시켜서 추가로 학습시키면 더 좋은 성능을 보일 수 있을까? 라는 질문에 대한 답을 하고 있다. 결과는 \"그렇다\"이다. 보다 구체적으로는 다음의 3가지 findings가 있었다. 첫번째, pretrained model을 추가로 in-domain 데이터에 학습시키는 것이 성능향상에 도움을 준다 (Domain-Adaptive PreTraining; DAPT). 두번째, DAPT 이후로 풀고자 하는 task의 unlabeled data에 대해 추가로 학습하는 것(Task-Adaptive PreTraining; TAPT)도 성능향상에 도움을 준다. 세번째, DAPT를 할 corpus가 없을 때 간단한 data selection strategies으로 task corpus를 augment 한 후 TAPT를 수행해도 성능 향상을 보인다. Experimental settings Pretrained model: RoBERTa-base 160GB heterogeneous data로 학습 bookcorpus, stories, wikipedia, and realnews Domains: biomedical papers (BIOMED) cs papers (CS) newstext from realnews (NEWS) Amazon reviews (REVIEWS) Tasks: 2 text classfication tasks per each domain Domain 의 기준? 내가 아는 한에서는, domain 이라는 용어에 대한 확실한 정의는 없다. 어떤 논문은 domain에 대해 두루뭉술하게 언급하고 넘어가고, 어떤 논문은 뭐라도 정의하고 넘어가는 경우가 있는데 이번 논문에서는 domain에 대한 논의가 중요하다보니 선정한 4개의 도메인과 pretraining domain이 서로 상이함을 밝힐 필요가 있었다. 위의 이미지는 Vocab overlap 을 통한 domain의 similarity 를 구한 heatmap이다. 빈번한 10K 의 vocab 중 얼마나 겹치는지를 나타내고 있으며 결과는 직관과 일치하는 것으로 보인다. PT corpus는 news, reviews와 가장 유사하며 cs와 가장 거리가 멀다. 또 reviews와 cs의 거리가 가장 멀고, news와 reviews는 cs에 비해 상대적으로 더 유사하다. DAPT Results Domain 별 LM loss 변화 각 도메인에 RoBERTa를 12.5K steps 씩 학습시킨 후 LM loss의 전후를 비교하였다. domain similarity 가 가장 높았던 news를 제외한 나머지 도메인에서 marginal한 성능향상이 있었다. PLM vs. DAPT vs. ~DAPT 모델의 classification 결과 LM loss 의 변화가 시사했던 것처럼 BM과 CS 도메인에서의 효과가 가장 컸다. 하지만 이 변화가 단순히 더 많은 데이터에 노출되었기 때문인지 아닌지를 판단하기 위해 out-of-domain corpus로 DAPT를 시킨 후의 결과와 비교했다. news의 경우 CS로 DAPT한 LM을, reviews의 경우 BIOMED LM을, cs의 경우 news LM을, biomed의 경우 reviews LM을 사용했다. DAPT가 ~DAPT 모델보다 모든 경우에서 더 좋은 성능을 낸다. 심지어 RoBERTa와 비교해보면 ~DAPT 의 결과는 더 나빠지는 경향을 보인다. 이는 단순히 더 많은 데이터에 노출되는 것이 항상 모든 도메인의 결과에서 효과적이지 않다는 사실을 시사한다. Fuzzy 한 domain boundary Vocab overlap 결과에서도 알 수 있지만 domain 이라는 것이 무자르듯 나뉘는 것이 아니다. 지금까지의 실험은 news와 reviews 도메인을 구분했지만, reviews corpus가 news corpus에 아예 도움을 주지 않는다고 볼 수 없다. TAPT Domain 보다 더 협소한 범위의 Task에 대해서도 DAPT와 같은 효과가 있는지를 검증하였다. DAPT와 비교해서 더 적은 corpus로 학습한다는 단점이 있지만 더 task relevant한 corpus라는 장점이 있다. 만약 최종 성능이 비슷하다면 TAPT가 더 값싼 학습방식이라고 볼 수 있다. 여기서는 labeled training data를 사용해서 second phase PLM을 진행했다. PLM vs. DAPT vs. TAPT vs. DAPT+TAPT 모델의 classification 결과 TAPT의 경우, corpus 사이즈를 고려해서, second phase of pretraining은 100 epoch만 진행하였다. PLM vs TAPT 결과를 보면, TAPT를 진행한 LM으로 classficiation task를 수행한 경우가 RoBERTa-base 보다 항상 결과가 더 좋다. DAPT vs TAPT 하지만 DAPT와 비교해보면 언제나 더 좋은 결과를 보여주는 것은 아니었다. PLM vs DAPT + TAPT DAPT 이후 TAPT 를 적용하는 것이 언제나 최고의 성능을 보여준다. PLM에 많이 활용된 데이터가 AGNews와 IMDB와 비슷했다면 그 둘의 성능 폭이 작은 것이 이해가 되지만, 아니라면 HyperPartisan이랑 Helpfulness의 성능향상이 BIOMED와 비슷하다는 점에서 꼭 PLM의 학습 코퍼스의 도메인과 성능향상이 관련있다고 보긴 어렵다. 그 task 를 잘하기 위해서는 마지막에 weight 를 옮겨주는 것이 필요하지 않을까? Cross-Task Transfer 같은 도메인 내의 2 task 간 transfer 효과가 있는지에 대해서 살펴보았다. BIOMED 내의 RCT, ChemProt task를 예로 들면, Transfer-TAPT는 RCT의 unlabeled data로 pretraining한 이후, ChemProt의 결과를 본 것이다. 모든 경우 Transfer-TAPT의 결과가 TAPT보다 낮았다. Augmenting Training data for TAPT task: RCT, HyperPartisan, IMDB case 1) target task의 labeled data와 같은 distribution의 unlabeled target task corpus (by human) Curated TAPT의 경우 TAPT보다 더 많은 task corpus (unlabeled)로 PLM을 진행하였고, 좋은 성능을 보였다. DAPT와 함께 진행하게 되면 그 효과는 더욱 확실하다. case 2) Automated Data selection for TAPT task setup 당시에 large unlabeled corpus 조차 풀리지 않는 경우가 있다. 이 때, 자동으로 관련있는 데이터를 찾아서 이를 기반으로 학습하게 되면 어떨지를 실험하였다. 하지만 large in-domain corpus 여야 하며, 이 중에서 task와의 접점이 있는 task-relevant data를 찾는 것이다. 그리고 embedding space 내에서 접점을 찾는 것이므로 경량화모델이 필요하다. 여기서는 vampire model을 사용하였다. rand-TAPT vs kNN-TAPT kNN &gt; rand-TAPT TAPT vs automated data selection 방법이 무엇이든 추가 데이터를 활용하는 것이 나쁘지는 않음 아직 RoBERTa에게는 더 학습할 수 있는 여지가 남아있다고도 해석 가능 (데이터는 많을수록 좋다) DAPT vs 500NN-TAPT 약 500개의 데이터만 사용해도 DAPT 효과를 어느 정도 낼 수 있음 Conclusions Task 문제를 더 잘 풀기 위해서 관련된 distribution 의 데이터로 추가 학습을 하는 것이 효과적이다 Task-specific data 일 필요는 없다. Domain이 비슷하면 효과를 볼 수 있다. 다만 아쉬운 건, PLM 자체를 학습시킬 때 DAPT+TAPT에 사용한 데이터를 활용하면 점수가 어떻게 변하는지 알 수 없었다는 점이나, 이번 논문의 scope에 들어갈 필요는 없었다고도 생각한다. References https://www.aclweb.org/anthology/2020.acl-main.740.pdf","link":"/2020/11/30/Don-t-stop-pretraining/"},{"title":"수학으로 이해하는 양자컴퓨터의 기초","text":"최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은 전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이 때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을 넘나들며 관련 지식을 습득해보았다. 아마 나와 같이 관련 기사나 여러 블로그 글, 유튜브 등을 찾아본 사람들이라면 어렵지 않게 아래의 정보는 얻었을 것이다. n개의 qbit은 bit와 달리 \\(2^n\\)의 state를 표현할 수 있다. superposition이란 동시에 0과 1의 상태를 띠는 성질로, 병렬연산이 가능해져서 고전컴퓨터에 비해 계산 속도의 이점이 생긴다. 텍스트만 보면 \"아 그렇구나.\" 싶은 내용들이다. 이해가 된 것일까 싶었지만 스스로에게 세 질문을 던졌을 때 답하지 못하는 것을 보며 제대로 이해하지 못했음을 인지했다. Q1. n개의 bit로도 \\(2^n\\)을 표현할 수 있는거 아닌가? 3개의 bit가 000, 001, 010, 011, 100, 101, 110, 111 이렇게 8개의 상태를 표현할 수 있으니까. Q2. 양자 세계는 불확정성 원리에 지배받는다고 하는데, 대체 양자컴퓨터로 어떻게 연산하고 있는 것이며, 이 성질이 어떻게 계산 비용을 감소시킬 수 있는걸까? Q3. Entanglement는 qbit들이 어떻게 된 상태인거지? 이 질문들에 제대로 답하기 위해서는 수학이 필요하다는 생각이 들었다. 4차원 이상의 공간을 제대로 시각화하지 못하듯이 양자 세계를 자연어로 표현한다는 것 자체가 말이 되지 않는 것 같았기 때문이다. 그래서 수학으로 설명된 자료를 찾으려고 부던히 애를 썼고, 끝내 \"내 수준에 맞는\" (= 이 글을 읽을 모두가 다 이해할 수 있는) 수학으로 설명된 자료를 찾았다. [slide] 이 영상을 보는 것을 추천하지만 무려 한시간이 넘는지라 글로도 정리를 해보았다. 아래에 기술된 내용은 내 방식대로 위의 영상과 자료를 재구성한 것이다. 사실 이 자료를 다 보더라도 양자컴퓨터에 대해 많은 것을 알았다고 보긴 어렵다. python을 처음 접한 사람이 print(\"Hello World!\")를 성공했다고 해서 python을 잘 알았다고 하기 어려운 것처럼. 그리고 딥러닝에 관심있는 사람이 tutorial을 따라해보며 CNN을 돌려봤다고 해서 딥러닝을 잘 알았다고 하기 어려운 것처럼. 그렇지만 양자 세계에 한 번은 Hello World!를 날려봐야 하지 않을까? Introduction The Deutsch-Jozsa problem 이라는 아주 간단한 문제를 통해 양자컴퓨터가 고전컴퓨터에 비해 어떻게 연산 속도에서 이점을 보이는지 알아보려고 한다. 이 과정을 이해하기 위해 양자컴퓨터가 연산하는 방법에 대해 소개할 것이며 matrix 연산과 기초적인 논리회로에 대한 내용을 짚고 넘어갈 것이다. 추가로, entanglement에 대한 간단한 설명이 있다. Basics Qubit / Qbit Qubit 혹은 Qbit은 양자컴퓨터 계산의 기본적인 단위이다. 조금이라도 양자컴퓨터에 대해 알아본 사람들이라면 qbit은 지겹도록 보고 들었을 것이다. Qbit은 언제나 다음의 조건을 만족시킨다. A qbit, represented by \\(\\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}\\) where \\(\\alpha\\) and \\(\\beta\\) are complex numbers must be constrained by the equation \\(||\\alpha||^2 + ||\\beta||^2 = 1\\) 따라서 아래의 예시들은 qbit에 해당된다. \\(\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\) \\(\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) \\(\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\\) 그리고 이 모든 벡터들의 basis가 되는 \\(\\begin{pmatrix} 1 \\\\0 \\end{pmatrix}\\)과 \\(\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\)은 각각 \\(\\mid 0\\rangle\\)과 \\(\\mid 1\\rangle\\)이라는 특별한 기호로 정의한다. Superposition 양자컴퓨터의 qbit을 설명할 때 빠지지 않는 성질이다. \"동시에 0과 1을 가진다.\"는 문장으로 자주 설명되지만 이보다는 슈뢰딩거의 고양이 느낌이 물씬 나는 \"When we measure a qbit, it collapses to an actual value of 0 or 1.\" 이라는 문장이 더 좋은 설명인 것 같다. 위에서 qbit이라고 언급했던 벡터 하나를 예시로 들어보자. \\[\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\] 이 qbit은 \\(0\\) 혹은 \\(1\\)로 collapse될 확률이 \\(\\frac{1}{2}\\) ( \\(= || \\frac{1}{\\sqrt{2}} || ^2\\)) 이다. 감사하게도 IBM은 자사의 양자컴퓨터를 사용할 수 있는 API를 만들어 놓았다. 여기서 이 qbit을 만들고 1024번 관측해보면 0과 1이 50%씩 나오는 것을 확인할 수 있다. \\(\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}\\) 은 \\(0\\)으로 collapse 될 확률이 \\(\\frac{1}{4}\\), \\(1\\)로 collapse 될 확률이 \\(\\frac{3}{4}\\)인 qbit이다. \\(|0\\rangle\\)은 0으로만 collapse 한다. Tensor product 여러 개의 qbit을 나타내기 위해 Tensor product 개념이 필요하다. 수학적으로 엄밀한 표현은 아니지만, n개의 qbit 연산을 표현하기 위해서는 아래의 표기 방식을 따르는 것이 좋다. \\[ \\binom{x_0}{x_1} \\otimes \\binom{y_0}{y_1} = \\begin{pmatrix} x_0 \\binom{y_0}{y_1} \\\\ x_1 \\binom{y_0}{y_1} \\end{pmatrix} = \\begin{pmatrix} x_0 y_0 \\\\ x_0 y_1 \\\\ x_1 y_0 \\\\ x_1 y_1 \\end{pmatrix} \\] 이를 응용하면 2개, 3개의 qbit도 벡터처럼 표현할 수 있다. \\[ |01\\rangle = \\binom{1}{0} \\otimes \\binom{0}{1} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\hspace{10pt} |100\\rangle = \\binom{0}{1} \\otimes \\binom{1}{0} \\otimes \\binom{1}{0} = \\begin{pmatrix} 0\\\\ 0\\\\0\\\\0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] 이와 같이 tensor product의 결과로 표현된 벡터는 product state라고 한다. 여기서 우리는 \\(n\\)개 qbit의 product state 크기가 \\(2^n\\) 이라는 것을 알 수 있다. 만약 \\( \\binom{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}} \\otimes \\binom{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\) 의 multiple qbits 이 있다면 \\(\\mid 00\\rangle\\), \\(\\mid 01\\rangle\\), \\(\\mid 10\\rangle\\), \\(\\mid 11\\rangle\\)으로 collapse될 확률이 모두 \\(\\frac{1}{4}\\)이므로 동시에 4개의 state를 표현할 수 있게 된다. 즉, qbit이 bit와는 다르게 \\(2^n\\)개의 state를 표현할 수 있다고 한 것은 동시에 가질 수 있는 최대 state 관점에서 비교한 것이다. bit는 절대로 동시에 2개 이상의 state를 가질 수 없으므로 한 번에 계산할 수 있는 정보는 1개 뿐이다. 또한 product state는, 뒤의 entanglement와 구분되는 중요한 성질로, 독립적인 state들로 factorize가 가능하다는 점이 있다. Multiple qbits의 product state 또한 single qbit과 같은 성질을 만족시킨다. \\[ \\binom{a}{b} \\otimes \\binom{c}{d} = \\begin{pmatrix} ac \\\\ ad \\\\ bc \\\\ bd \\end{pmatrix} \\] \\[ \\text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1 \\] 1-bit operations 1-bit에서 가능한 연산은 Identity, Negation, Constant-0, Constant-1의 총 4가지가 있다. 각각의 연산은 matrix로 표현할 수 있다. \\[ \\text{Identity} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] \\[ \\text{Negation} = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] \\[ \\text{Contant-0} = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 0 \\end{pmatrix} \\] \\[ \\text{Contant-1} = \\begin{pmatrix} 0 &amp; 0 \\\\ 1 &amp; 1 \\end{pmatrix} \\] CNOT (one of the 2-bit operations) CNOT 연산은 control bit와 target bit로 구성된 2-bit가 있을 때 control bit가 0이면 target bit를 바꾸지 않고, control bit가 1일 때 target bit를 바꾸는 연산이다. 마찬가지로 이 연산도 matrix로 표현할 수 있다. \\[ C = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\] 2.4와 2.5에서 operation들을 matrix화 한 것에 주목하자. 확률이 지배하는 양자 세계에서 deterministic한 연산을 하기 위해서는 matrix를 관측하지 않은 qbit에 곱하는 것이 유일한 방법이기 때문이다. 아래의 예시에서 우리가 확신할 수 있는 정보는 qbit이 0과 1로 관측될 확률이 반대가 되었다는 것이다. \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\] 항상 0 혹은 1로 관측되는 \\(\\mid0\\rangle\\)이나 \\(\\mid1\\rangle\\)을 쓰면 matrix 연산을 고집하지 않아도 되지만 이런 qbit만 사용할거라면 고전컴퓨터를 쓰면 그만이다. 굳이 0K 가까이 되는 험악한 조건을 유지해가며 계산할 필요가 없다. 그래서 matrix 연산은 양자 컴퓨팅에서 굉장히 중요하다. 여기에는 한가지 추가조건이 있는데, 반드시 연산에 사용되는 matrix는 reversible해야한다는 것이다. 따라서 앞서 본 1-bit 연산 중 Constant-1과 Constant-0를 계산하기 위해서는 단순 matrix를 곱하는 것 외의 다른 방법이 필요하다. The Deutsch-Jozsa problem 이 문제[1]는 양자컴퓨터가 고전컴퓨터에 비해 계산적인 이점을 가지는 아주 간단한 (동시에 쓸데없는) 문제다. 1-bit를 입력받아서 1-bit를 내뱉는 어떤 함수가 있다고 하자. 만약 이 함수가 Constant(Contant-0, Constant-1)인지, 아니면 Variable(Identity, Negation)인지 알기 위해서는 최소 몇 번의 query를 날려야 할까? Classical computer 고전컴퓨터에서는 0과 1을 입력해야하므로 총 두 번의 연산이 필요하다. Quantum computer 예상했듯이 정답은 한 번이다. 왜인지 알기위해서는 추가적인 개념이 필요하다. Hadamard gate 앞서 언급된 적 있는 H gate이다. \\[ H = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] Hadamard gate는 0- 혹은 1-qbit을 받아서 0과 1을 같은 확률로 가지는 qbit으로 바꿔준다. \\[ H\\mid0\\rangle = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\] \\[ H\\mid1\\rangle = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] Hadamard gate는 또 다른 중요한 성질이 있다. 0과 1을 같은 확률로 가지는 qbit을 다시 0- 과 1-qbit으로 돌려보낸다는 것이다. \\[ \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] X gate X gate는 qbit의 위 아래를 바꿔준다. \\[ X = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\frac{-1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] H gate와 X gate 연산을 이해하기 쉽게 표현하면 아래의 그림이 된다. 붉은색이 X gate, 노란색이 H gate 연산의 방향이다. \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\)에 X - H - X - H - X gate를 씌운 결과는 그림으로 보면 더 쉽게 이해된다. \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\)에서 출발해 각 gate가 연산되는 방향으로 화살표를 움직이면 마지막에 도달하는 곳이 결과값이 된다. non-reversible matrix 앞서 양자컴퓨터는 non-reversible한 matrix 를 곱하는 연산은 불가능하다고 했다. 1-bit 연산 중에서 Constant-0과 Constant-1은 non-reversible하다. 그래서 양자 컴퓨팅에서는 2개의 qbit을 사용한다. 이 그림의 input과 output notation을 보면 \"응?\" 이라는 생각이 절로 들 것이다. 영상에서도 사람들이 대체 왜 \"Output\"이 Input 쪽에 가 있는지에 대해 끊임없이 묻는다. 아쉽게도 강연자는 속시원하게 답변을 해주지 않아서 그런가보다 하고 넘어갔는데 다시 보니 이해가 된 부분이 있어 글로 설명해보려고 한다. Input'과 Output'이 실제 1-bit 연산의 input과 output을 나타낸다. 그리고 Input과 Output은 Input'과 Output'이 BB 이후에 있기 때문에 BB 이전에 Input'과 Output'이 1-bit 연산의 input과 output이 되도록 넣어주는, 양자컴퓨터 연산 방식 때문에 필요한 input들이다. 이 약속에 따라서 양자컴퓨터가 1-bit 연산을 어떻게 수행하는지 아래의 예시를 통해 좀 더 이해해보자. Constant-0은 Input'이 \\(\\mid 0 \\rangle\\)일 때와 \\(\\mid 1 \\rangle\\)일 때 모두 Output'이 \\(\\mid 0 \\rangle\\)이어야 한다. 어떤 gate도 없는 왼쪽 위 그림의 회로에서 Input에 \\(\\mid 0 \\rangle\\) 혹은 \\(\\mid 1 \\rangle\\)을 대입해보면 Input'과 Output'이 Constant-0의 관계를 가지는 것을 확인할 수 있다. Indentity는 Input'이 \\(\\mid 0 \\rangle\\)일 때는 Output'이 \\(\\mid 0 \\rangle\\)이고 Input'이 \\(\\mid 1 \\rangle\\)일 때는 Output'이 \\(\\mid 1 \\rangle\\)인 함수다. 왼쪽 아래 그림의 회로는 CNOT gate를 표현하고 있다. 색이 채워진 원이 control bit 쪽을 나타내고 그렇지 않은 쪽 원은 target bit를 나타낸다. Input이 \\(\\mid 0 \\rangle\\) 이면 control bit가 0이므로 target bit도 그대로 유지한다. 그래서 Input'도 \\(\\mid 0 \\rangle\\), Output'도 \\(\\mid 0 \\rangle\\)이 된다. Input이 \\(\\mid 1 \\rangle\\) 이면 control bit가 1이므로 target bit가 바뀐다. 그래서 Input'도 \\(\\mid 1 \\rangle\\), Output'도 \\(\\mid 1 \\rangle\\)이 된다. 그럼 다시 The Deutsch-Jozsa problem로 돌아가서, 양자컴퓨터에서는 어떻게 한 번에 구할 수 있을까? 정답은 아래의 그림이 설명해준다. 이 연산대로라면 BB가 Constant(Contant-0, Constant-1)이었을 경우, 측정 결과가 \\(\\mid11\\rangle\\)이고, Variable(Identity, Negation)이었을 경우에는 \\(\\mid01\\rangle\\)이 된다. BB의 경우의 수를 따져가며 이해해보자. preprocessing (BB 입력 직전까지의 연산) BB에 들어가기 전 input (\\(\\mid 0 \\rangle\\)) 과 output qbit (\\(\\mid 0 \\rangle\\)) 모두 X와 H gate를 거쳐서 \\(\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix}\\) 가 된다. case 1) BB가 Constant-0 이었을 경우 Constant-0은 input과 output에 어떤 gate도 씌우지 않는다. 따라서 BB가 Constant-0이었을 때 Input과 Output은 H gate만 통과한 이후 관측된다. case 2) BB가 Contstant-1 이었을 경우 Constant-1은 output에만 X gate를 적용한다. 따라서 BB가 Constant-1이었을 때는 Output에 X gate가 추가되고, 이후 Input과 Output 모두에 H gate가 적용된다. case 3) BB가 Identity 이었을 경우 Identity는 CNOT gate를 통해 연산된다. 앞에서 CNOT 연산은 \\( \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\) 을 곱하는 것과 같다고 설명했다. Preprocessing을 거친 Input과 Output은 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 이므로 CNOT연산은 아래와 같이 표현할 수 있다. \\[ C \\begin{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\end{pmatrix} = C \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{-1}{2} \\\\ \\frac{-1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] 즉 Input은 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 에서 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\) 로 바뀌고 Output은 그대로 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 이 된다. 이 상태에서 H gate가 각각 적용되어 최종 결과는 \\(\\mid 01 \\rangle\\)이 된다. case 4) BB가 Negation 이었을 경우 Negation은 Indentity의 결과 중 Output에만 X gate가 추가되는 연산이다. 따라서 아래 그림처럼 연산이 이루어지고 Identity와 마찬가지로 최종 결과는 \\(\\mid 01 \\rangle\\)이 된다. 정리하면, 양자컴퓨터에서는 특정 설계 상황에서 고정된 BB input에 대한 BB output을 \"한 번\"만 관측하면 BB가 Constant인지 Variable인지 확인할 수 있다는 것이다! Entanglement Entanglement는 지금까지의 흐름에서는 동떨어진 이야기지만 양자컴퓨터에서 항상 소개되는 내용이기 때문에 추가하였다. 앞서 qbit과 product state의 성질을 수학적으로 나타낸 것처럼 entanglement도 수학적인 성질로 표현할 수 있다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\) 는 entangle된 qbit인데, 그 모양새가 product state와 닮아있다. 하지만 product state와는 중요한 성질에서 차이를 보인다. 위에서 설명했듯이 product state는 개별적인 qbit으로 factorize된다. 하지만 entanlged qbit은 개별적인 qbit으로 factorize 되지 않는다. (If the product state of two qbits cannot be factored, they are said to be entanlged.) 이 때문에 entangled qbit은 차원이 늘어난 하나의 qbit으로 볼 수 있으며 일부를 관측했을 때 나머지 일부의 상태가 유추된다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\) 이 entangle 되었음을 증명하는 것은 간단하다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\otimes \\begin{pmatrix} c \\\\ d \\end{pmatrix} \\) 를 만족하는 \\(a\\), \\(b\\), \\(c\\), \\(d\\)는 존재하지 않기 때문에 이는 entanlge되어 있는 qbit이다. Entanlged qbit은 CNOT과 H gate를 통해 쉽게 생성할 수 있다. \\[ CH_1 \\begin{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\end{pmatrix} = C \\begin{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\] 만약 이후에 이런 게이트의 조합을 본다면 곧바로 'entanlge 되었군!' 이라고 생각하면 된다 :) Conclusion 개인적으로 이 영상을 본 이후, 속이 뻥 뚫리는 기분이 들었다. 아직 matrix로 표현되는 qbit이 물리적으로 어떤 모습인지, gate들이 물리적으로 어떻게 qbit에 적용되는지는 모르지만 (이건 실제 양자컴퓨터를 눈으로 보면 이해가 되지 않을까) 이 정도라도 양자컴퓨터와 고전컴퓨터의 연산과정에서의 차이를 구체적으로 알 수 있었기 때문에 만족할 수 있었다. 양자컴퓨터의 연산 과정을 이해하고나니 양자 우월성은 그냥 달성되는 것은 아니었으며, 잘 설계된 gate가 뒷받침되었을 때 가능한 것임을 깨닫게 되기도 했다. 이 정도면 양자 세계에 Hello World!를 했다고 볼 수 있지 않을까? References 1.https://en.wikipedia.org/wiki/Deutsch–Jozsa_algorithm#Problem_statement ↩︎","link":"/2019/11/07/Basics-of-Quantum-Computings/"},{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (a.k.a. T5)","text":"최근 NLP task에서 좋은 성능을 보이는 모델은 대량의 monolingual corpus를 통해 unsupervised pre-training을 한 LM을 task에 맞게 supervised fine-tuning을 하는 transfer learning에 기반하고 있다. 같은 transfer learning framework 안에서도 다양한 모델이 존재한다. 우리가 아는 모델만 하더라도 BERT, GPT, ELMO 등이 있고, GLUE benchmark에 대해서 테스트한 점수가 있다. 하지만 과연 점수가 더 높다고 더 좋은 모델이라고 할 수 있을까? 우리가 모델이라고 부르는 것 안에는 학습 방식 외에도 학습에 사용한 데이터셋, optimizer, 모델의 크기 등 많은 내용이 함축되어 있다. 그래서 각 모델의 아이디어 중 과연 \"어떤 특징이 좋은 모델 성능을 내는데에 도움이 되었을까?\"라고 묻는다면 쉽게 대답하기 어렵다. 이 논문에서 소개하는 Text-to-Text Transfer Transformer (T5) 는 그 답을 찾기 위해 고안한 framework이다. Transfer learning framework: Text-to-Text Transfer Transformer (T5) T5는 모든 task를 transformer의 building block으로 하는 seq2seq framework 이다 (주의! encoder-decoder 와는 다르다. sequence X가 입력되면 sequence Y가 출력되는 것일 뿐). 다양한 downstream tasks (question answering, document summarization, semtiment classification, machine translation, etc) 를 하나의 모델 안에서 해결해야 서로 다른 transfer learning 방식의 효과를 공정하게 비교할 수 있기 때문에 이와 같은 unified framework 가 제안되었다. 분석하고 싶은 내용과 무관한 특징들 -- 사용한 데이터, tokenizer, vocab size, learning rate 등 -- 은 task에 상관없이 고정된다. Pre-training dataset: Colossal Clean Crawled Corpus (C4) Transfer learning framework에 사용되는 pre-trained model은 어떤 종류의 corpus를 사용했는지, 얼만큼의 corpus를 사용했는지에 따라서도 특징이 달라진다. 이에 대한 실험을 위해 논문에서는 common crawl로 수집한 아주 많은 양의 데이터를 소개한다. 양에 따른 모델 성능을 비교하기 위해 우선 기존의 Wikipedia corpus 보다 2배 이상 큰 사이즈의 데이터를 crawling 한다. 인터넷에서 crawling한 문서는 보통 매우 지저분하다. 이런 지저분한 데이터를 학습에 바로 사용하게 되면 side effect 가 있을 수 있기 때문에 중복 제거, 미완성 문장 제거, 공격적이거나 bias가 있는 문장 제거 등의 cleansing process를 거치고 깔끔한 데이터를 남긴다. 이 데이터셋의 이름이 Colossal Clean Crawled Corpus (C4) 이다. 여러 필터링을 거쳤음에도 Wikipedia corpus 크기의 2배 이상이라고 한다. TF dastasets에서 다운로드 가능하다. Main Contributions of the paper 앞서 소개했듯이, 이 논문에서 풀고 싶은 질문은 \"다양한 NLP transfer learning framework 중에서 어떤 feature가 좋은 성능을 내는데에 도움이 될까?\"이다. 따라서 조금씩 training schema를 달리해가며 여러 downstream task의 성능을 비교해야 한다. 이 논문에서는 크게 1) Pre-training architecture 2) Pre-training objective 3) Pre-training dataset 4) Pre-training datasize 5) Training strategy 6) Scaling strategy를 주목하고 있다. 그리고 중요한 Disclaimer로, 여기서 소개하는 architecture와 objective는 GPT, ELMO 등의 구현을 아주 정확하게 replicate 하고 있지 않다고 언급한다. 어느 정도 이 모델들의 구조에 motivate된 내용이 보이지만 정확하게 같지는 않다. Pre-training architecture Encoder-Decoder (Baseline) vs. Language Model vs. Prefix LM 크게 세가지 모델 구조를 실험하였다. Encoder-Decoder에서 Encoder는 fully-visible attention을 적용하였고 decoder만 causal attention을 적용하였다. LM은 전부 causal attention이며, 이는 uni-directional하게 정보를 습득하는 것을 나타낸다. PrefixLM의 경우 encoder-decoder 와 유사한 것으로 보인다. input인 x에 대해서는 bi-directional하게 정보를 습득하고 y만 causal attention이 적용된다. Q. 어떤 Pre-training model architecture 가 가장 효과적일까? Encoder-Decoder architecture가 가장 효과적이었다. LM의 성능이 가장 좋지 않았는데, 이를 통해 bi-directional context를 input으로 넣어주는 것이 효과적이라는 사실을 알 수 있다. Pre-training objective 아래 이미지에서 확인할 수 있듯, 크게 네단계로 나누어서 생각해볼 수 있다: 1) High-level approaches 2) Corruption strategies 3) Corruption rate 4) Corrupted span length High-level approaches 세가지 방식을 비교한다. Prefix language modeling 은 문장의 앞부분을 context로 제공하는 방식, BERT-style 은 Masked-LM (MLM) 방식, 그리고 Deshuffling 은 문장의 token을 뒤섞고 원 문장을 맞추는 방식이다. Objective Inputs Targets Prefix language modeling Thank you for inviting me to your party last week . BERT-style Thank you &lt;M&gt; &lt;M&gt; me to your party apple week. (original text) Deshuffling party me for your to . last fun you inviting week Thank (original text) Q. 어떤 High-level approach 가 가장 효과적일까? 위의 표의 결과에 따르면 BERT-style (MLM) objective 가 가장 좋은 성능을 보인다. Corruption strategies 이번에는 High-level approaches 중 가장 좋은 성능을 보인 BERT-style approach 에 대해서 적용해 볼 수 있는 다양한 corruption strategies 에 따라 실험한다. 총 세가지 전략이 있다. 모두 i.i.d. 로 masking을 하는 것은 동일하지만, token 단위로 masking 하는 방식이 있고 (i.i.d. noise, mask tokens) span 단위로 masking 해서 예측하는 문장은 입력에서 masking된 부분을 예측하고 아닌 부분을 masked token으로 예측하는 방식 (i.i.d. noise, replace spans (a.k.a. Denoising, Baseline)), 그리고 원문에서 token을 제외하고 제외된 부분을 예측하는 방식 (i.i.d. noise, drop tokens) 이 있다. Objective Inputs Targets i.i.d. noise, mask tokens Thank you &lt;M&gt; &lt;M&gt; me to your party &lt;M&gt; week. (original text) i.i.d. noise, replace spans (a.k.a, Denoising, Baseline) Thank you &lt;X&gt; me to your party &lt;Y&gt; week. &lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt; i.i.d. noise, drop tokens Thank you me to your party week. for inviting last Q. BERT-style approach의 다양한 corruption strategy 중 무엇이 가장 효과적일까? 위의 표에 따르면 Denoising 방식이 가장 좋은 성능을 낸다. Corruption rate 마찬가지로 위에서 가장 성능이 좋았던 Denoising 방식에서 corruption rate을 10%, 15%, 25%, 50%로 다르게하며 살펴본다. Q. 원 문장의 얼만큼을 corrupt 하는 것이 가장 좋을까? 표에 따르면 15%가 적당하다는 결론이 나온다. Random spans 15% 정도 denoising 방식으로 corrupt 할 때 평균적인 span의 길이를 다르게 조정해볼 수 있다. Baseline으로 사용한 i.i.d.와 2, 3, 5, 10 의 길이를 비교할 수 있다. Q. corrupt할 때 가장 적정한 평균 span 길이는 몇일까? 표에 따르면 i.i.d 에 따라 corrupt하는 것이 가장 효과적이다. Pre-training Dataset Pre-training 모델의 성능은 데이터셋에 따라 달라질 수 있다. 이를 실험하기 위해 논문에서 수집한 C4와 C4, unfiltered 그리고 다른 특징을 가진 데이터셋으로 pre-training 한 후 task 마다의 성능을 비교하였다. 결과는 C4를 쓰는 것이 가장 좋았다. Pre-training datasize GLUE benchmark의 상위권 모델은 보통 pre-training에 사용한 datasize가 크며 모델의 사이즈도 굉장히 크다. T5는 다행히도 모델의 capacity가 큰 편이라 데이터 사이즈를 조절해가며 실험을 할 수 있었고, C4 전체를 사용했을 때 가장 성능이 좋았다. 다시 말해, 아직 모델의 capacity 가 더 크다고 이해할 수 있다. Training strategy Transfer learning 은 Training strategy에 따라서도 성능이 달라질 수 있다. Training strategy는 Fine-tuning, Multi-task learning 방식으로 나뉘며 이를 어떻게 조합하는지 또한 전략에 포함되었다. Fine-tuning methods Fine-tuning 은 모든 파라미터를 사용했을 때 가장 성능이 좋다. Multi-task learning (pre-training) T5 의 task는 다양한데, 과연 각 task를 얼만큼 학습시키는 것이 좋을지에 대한 실험이다. Equal 의 경우 task dataset의 사이즈에 상관없이 같은 수의 문장을 학습시키는 방식이다. K threshold 를 사용한 경우, 기본적으로 task의 문장 사이즈만큼 샘플링하되, K 이상의 데이터는 K 만큼만 학습에 활용한다. 결과는 sampling 하지 않는 것이 가장 좋다. Combining fine-tuning and multi-task learning 위에서 소개한 다양한 방식을 조합하여 실험하였고, multi-task pre-training과 fine-tuning 조합이 가장 좋은 성능을 보였다. Scaling strategy scale-up 할 수 있는 hyperparameter의 값을 조절해가며 실험하였다. training steps, batch size, model size 를 조절한 결과, model size 를 키우고 training steps 을 늘린 경우에 가장 좋은 성능을 보였다. Conclusions 위의 여러 실험들의 결과를 종합하면, 1) Encoder-Decoder architecture 2) Span prediction objective 3) C4 dataset 4) Multi-task pre-training 5) Bigger models trained longer 의 구조로 학습할 때 가장 좋은 transfer learning 효과를 얻을 수 있다. References [1] https://youtu.be/eKqWC577WlI [2] https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html","link":"/2020/08/29/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/"},{"title":"General Language Understanding Evaluation (GLUE) benchmark","text":"General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, 전자는 설명이 상세하고 후자는 요약되어 있다. 이 글은 가장 최근(2019.2.22)에 업데이트된 arXiv에 있는 논문을 기반으로 작성되었다. GLUE overall GLUE는 총 9개의 task로 구성되었으며 각 task는 언어의 특정한 성질을 평가하기 위한 목적으로 만들어졌고, 최종 점수는 각 task 별 점수의 평균 값을 가져간다. task는 크게 3가지 - Single-Sentence Tasks (CoLA, SST-2), Similarity and Paraphrase Tasks (MRPC, QQP, STS-B), Inference Tasks (MNLI, RTE, QNLI, WNLI) - 로 구분할 수 있다. 세부 task에 대해 살펴보기 전에 전반적인 task의 특징을 아래의 표에 정리했다. 원 논문에 정리되어 있는 것을 바탕으로 재구성하였고 직접 다운로드 받은 데이터를 기준으로 측정했기 때문에 corpus의 size가 다를 수 있다. data train dev test domain input task metrics Corpus of Linguistic Acceptability (CoLA) 8.5k 1.0k 1.2k linguistics literature single-sentence - grammatical acceptability - binary classification (grammatical / ungrammatical) Matthews correlation Stanford Sentiment Treebank (SST-2) 67k 872 1.8k movie reviews single-sentence - sentiment - binary classification (positive / negative) acc. Microsoft Research Paraphrase Corpus (MRPC) 3.7k 408 1.7k news two sentences paraphrase acc./F1 Quora Question Pairs (QQP) 364k 40k 391k social QA questions two sentences paraphrase acc./F1 Semantic Textual Similarity Benchmark (STS-B) 5.8k 1.5k 1.4k news caption forum two sentences - sentence similarity - regression Pearson / Spearman corr. Multi-Genre NLI corpus (MNLI) 393k 20k 20k fiction face-to-face government letters 9/11 oxford university press (oup) slate telephone travel verbatim two sentences ternary classification (entailment / contradiction / neutral) matched acc. / mismatched acc. The Recognizing Textual Entailment (RTE) 2.5k 276 3.0k news wikipedia two sentences binary classification (entailment / not_entailment) acc. The Stanford Question Answering NLI (QNLI) 105k 5.5k 5.5k wikipedia two sentences (question, sentence) binary classification (entailment / not_entailment) acc. The Winograd Schema Challenge NLI (WNLI) 634 71 146 fiction books two sentences binary classification (entailment / not_entailment) acc. Corpus of Linguistic Acceptability (CoLA)[1] CoLA는 공개된 언어학 문헌(publised liguistics literature)에서 추출된 약 21k 문장들로 구성되어 있다. 이 문장들은 문법적으로 옳은지, 그른지가 표기되어 있다. 1 They drank the pub dry.0 * They drank the pub. 문법적으로 옳고 그름을 판단하는 기준은 다양하다. 아래의 표는 corpus를 제작하면서 기준에서 포함된 것들과 제외된 것들을 나타낸다. Included (a) Morphological Violation: \"should leave\" 가 올바른 표현이지만 \"should leaving\"으로 작성되었다. 동사의 형태(verbal inflection)가 맞지 않는 경우에 해당한다. (b) Syntactic Violation: \"What did Bill buy?\" 혹은 \"Bill bought potatoes and _\" 이 되어야 한다. 통사 구조가 틀린 경우에 해당한다. (c) Semantic Violation: 의미적으로 말이 되지 않는 문장에 해당한다. Excluded (d) Pragmatic Anomalies: grammar와 상관없는 외부 지식이 필요하므로 제외되었다. (e) Unavailable Meanings: 문장만보고는 판단이 애매하므로 제외되었다. (f) Prescriptive Rules: 사람도 누군가의 가르침없이는 터득하기 어려운 rule이기 때문에 제외되었다. (g) Nonce Words: \"arrivable\"과 같이 typical word-level NLP 모델의 vocab에는 등장하지 않는 단어가 포함된 경우이다. NLP 모델의 scope이 아니라고 판단되어 제외되었다. testset and metrics testset은 In-Domain과 Out-of-Domain으로 구성되어 있다. In-Domain은 training set이 추출된 source와 같은 source에서, Out-of-Domain은 training set이 추출되지 않은 source에서 구성되었다. GLUE는 원래 구분된 두 testset을 하나로 합쳐 단일 testset을 구축하였고 총 1,160 문장이다. 이 task의 평가는 unbalanced binary classification task에서 사용되는 Matthews correlation으로 한다. Stanford Sentiment Treebank (SST-2) rottentomatoes.com의 영화 리뷰 corpus로 구성되었으며 AMT(Amazon Mechanical Turk)를 통해 리뷰의 sentiment가 labeling 되었다. 1은 긍정, 0은 부정을 나타낸다. that loves its characters and communicates something rather beautiful about human nature 1on the worst revenge-of-the-nerds clichés the filmmakers could dredge up 0 testset and metrics 일반적인 binary classification 문제로 accuracy를 통해 평가한다. Microsoft Research Paraphrase Corpus (MRPC) MRPC는 온라인 뉴스에서 추출된 문장들로 구성되었으며 2개의 문장이 의미적으로 같은지 다른지를 평가하는 task이다. They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .1Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion . Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .0 testset and metrics testset이 label이 불균등(68% positive, 32% negative)하므로 accuracy와 F1 score를 metric으로 한다. Quora Question Pairs (QQP)[2] QQP는 https://www.quora.com/의 질문들로 구성되었으며, 두 개의 질문이 의미상 같은지 다른지가 표기되어있다. How do you start a bakery?How can one start bakery business?1What are natural numbers?What is a least natural number?0 testset and metrics MRPC와 마찬가지로 불균등(37% positive, 63% negative)하므로 accuracy와 F1 score가 metric으로 활용된다. Semantic Textual Similarity Benchmark (STS-B) 문장의 유사도는 번역, 요약, 문장 생성, QA, 대화 모델링 등등 다양한 NLP 분야에서 중요하게 다뤄진다. STS shared task는 모델이 문장들의 유사도를 얼마나 잘 파악하는지를 평가하기 위해 등장하였고, 2012년부터 2017년까지 매년 개최되었으며 그 때마다 다른 dataset이 사용되었다. 이 때문에 각 연도의 데이터셋을 적절히 조합한 common evaluation set으로 STS-B가 소개되었다. 이 전의 task와는 다르게 STS는 regression task이다. human annotator들은 두 문장의 의미적인 유사도를 1~5점으로 평가하였고 모델은 score를 예측해야한다. A plane is taking off. An air plane is taking off. 5.000Three men are playing chess. Two men are playing chess. 2.600A man is smoking. A man is skating. 0.500 testset and metrics Regression task이므로 human label과의 Pearson correlation으로 평가된다. Multi-Genre NLI corpus (MNLI) MNLI[3]는 SNLI(Stanford NLI) dataset의 단점을 개선시킨 데이터셋이다. SNLI는 image caption으로만 구성되었기 때문에 장면을 표현하는 짧고 간단한 문장이 많고 NLU(Natural Language Understanding) task와 무관한 단어들이 많이 등장한다. 그래서 NLU task의 benchmark로 사용되기는 어렵기 때문에 다양한 도메인(논문에서는 genre라고 표현)의 조합인 MNLI benchmark dataset이 등장하였다. 위의 표에서 나와있듯이 MNLI는 총 10개의 Genre로 구성되었다. Fiction을 제외한 9개의 domain은 Open American National Corpus에서 추출되었고 Fiction은 fiction literature에서 가져왔으며 mystery, humor, sci-fi 등 그 안에서도 다양한 장르로 구성되었다. OANC data constitutes the following nine genres: transcriptions from the Charlotte Narrative and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s–early 2000s (LETTERS); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE) written between 1996–2000; transcriptions from University of Pennsylvania’s Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM). For our tenth genre, FICTION, we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery (The Mysterious Affair at Styles, 3 Christie, 1921; The Secret Adversary, 4 Christie, 1922; Murder in the Gun Room, 5 Piper, 1953), humor (Password Incorrect, 6 Name, 2008), western (Rebel Spurs, 7 Norton, 1962), science fiction (Seven Swords, 8 Shea, 2008; Living History,9 Essex, 2016; The Sky Is Falling, 10 Del Rey, 1973; Youth, 11 Asimov, May 1952), and adventure (Captain Blood, 12 Sabatini, 1922). 선별된 문장을 premise로 두고 human annotator들이 premise와 같은 결론을 도출하는 문장(entailment), 반대되는 문장(contradiction), 두 경우 모두 아닌 문장(neutral)을 생성하고 label을 단다. How do you know? All this is their information again. This information belongs to them. entailmentVrenna and I both fought him and he nearly took us. Neither Vrenna nor myself have ever fought him. contradictionThere was nothing like that emotion now. There are few emotions that come close. neutral testset and metrics testset은 CoLA처럼 matched(in-domain)와 mismatched(cross-domain)로 구성되었다. mismatched에는 9/11, FACE-TO-FACE, LETTERS, OUP, VERBATIM처럼 training set에는 없는 domain이 포함되어 있다. (위의 표 참고) 각각의 경우를 나누어서 accuracy로 평가한다. The Recognizing Textual Entailment (RTE) RTE도 STS처럼 RTE1부터 RTE7까지의 데이터셋에서 만들어졌다. 구체적으로는 RTE1, RTE2, RTE3, RTE5로 구성되었고, 나머지 데이터셋 중 RTE4는 공개되지 않아서, RTE6와 7은 NLI task로는 부적합해서 제외했다고 한다. 취합하는 과정에서 일부는 세 개의 class, 일부는 두 개의 class로 labeling이 되어있어 이를 일괄적으로 두 개의 class(entailment, not_entailment)로 구분지었다. Swansea striker Lee Trundle has negotiated a lucrative image-rights deal with the League One club. Lee Trundle is in business with the League One club. entailmentNo Weapons of Mass Destruction Found in Iraq Yet. Weapons of Mass Destruction Found in Iraq. not_entailment testset and metrics 일반적인 binary classification 문제이므로 accuracy로 측정한다. The Stanford Question Answering NLI (QNLI) Stanford에서 구축한 Machine Comprehension 목적의 QA Dataset, a.k.a SQuAD,을 NLI task에 맞게 변형한 데이터셋이다. SQuAD는 wikipedia에서 paragraph를 가져와서 annotator들이 적절한 질문을 던지는데 이에 대한 답을 paragraph 내에 있는 문장, 구, 단어로 답할 수 있게 구성되었다. QNLI는 질문과 paragraph 내의 한 문장을 비교하여 이 둘이 entailment되었는지 아닌지를 판단하도록 바뀌었다. What two things does Popper argue Tarski's theory involves in an evaluation of truth? He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. entailmentWho was elected as the Watch Tower Society's president in January of 1917? His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner. not_entailment testset and metrics 일반적인 binary classification 문제이므로 accuracy로 측정한다. The Winograd Schema Challenge NLI (WNLI) 이 데이터셋도 entailment를 평가하는 목적으로 만들어졌다. original sentence와 이 문장에서 대명사를 일반명사로 치환한 문장 사이의 entailment가 있는지 없는지가 label로 달려있다. 아래 예시의 첫 번째 문장에서 \"it\" had a hole의 it이 \"The carrot\"으로 바뀐 문장이 두 번째 문장이고 이 두 문장의 관계가 entailment 되어 있으므로 label 1이 달린다. I stuck a pin through a carrot. When I pulled the pin out, it had a hole. The carrot had a hole. 1John was jogging through the park when he saw a man juggling watermelons. He was very impressive. John was very impressive. 0 testset and metrics GLUE FAQ의 12번 문항에는 WNLI에서 이상한 결과를 얻을 수 있는 이유가 적혀있다. 같은 문장이 포함된 다른 example 끼리는 반대의 label이 달려있는데 이 때문에 training set에 overfit된 모델은 dev set에서 성능이 매우 나쁠 수 있다는 것이다. 실제로 BERT는 이 이유로 WNLI의 성능은 report 하지 않았다. Download 링크에 있는 python script를 다운로드한 이후 실행시키면 된다. 지정한 dir에 전체 task를 받을 수도 있고 일부 task만 받을 수도 있다. python download_glue_data.py --data_dir data --tasks all Leaderboard 여태까지 제출한 모델의 성능은 GLUE leaderboard에 정리되어있다. Leaderboard에는 순기능과 역기능이 모두 공존하지만, 아직까지는 순기능이 더 많다고 생각한다. 상대적으로 공정하게 비교할 수 있는 데이터셋이고 덕분에 다양한 Language Model이 주목받을 수 있었기 때문이다. 너무 낡아버리기 전에 새로운 데이터셋이 나와야한다고도 생각했는데, Neurips 2019에 \"Stickier Benchmark\"라는 부제와 함께 SuperGLUE가 등장했다!! 이로 인해 열릴 새로운 LM들의 등장을 기대해본다 :) References 1.https://arxiv.org/pdf/1805.12471.pdf ↩︎ 2.https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs ↩︎ 3.https://www.aclweb.org/anthology/N18-1101.pdf ↩︎","link":"/2019/12/22/GLUE-benchmark/"},{"title":"Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?","text":"Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는 다양한 방법들을 제공한다. 이러한 방법들을 Merge 전략이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장 많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase and Merge에 대해 소개하려고 한다. 위의 그림과 같은 상태의 commit이 생성되었다고 가정하자. feat/multiply라는 브랜치가 있고, feat/sum이라는 브랜치가 있다. 각 commit 내의 숫자는 commit의 global 순서를 나타낸다. Merge Commit 브랜치의 commit log와 merge log가 동시에 기록된다. Commit log는 commit을 행한 순서대로 기록되고, merge log는 merge가 된 순서대로 기록된다. 동시에 기록되기 때문에 commit log가 verbose해지며 commit log의 순서가 merge 순서와 다르기 때문에 history 관리 및 이해가 어렵다. Squash and Merge Merge된 순서대로 master/main 브랜치에 기록된다. 그리고 작업 완료된 브랜치의 commit은 새로운 commit 으로 모두 squash되며, 새로운 commit의 제목은 PR 제목이 되고, 합쳐진 commit의 제목은 새로운 commit의 상세 내용이 된다. 이러한 특징 때문에 master/main 브랜치의 히스토리 관리가 쉬우나, atomic commit level로 rollback 하는 것은 불가능하다. Rebase and Merge Commit 순서가 아닌 merge 순서대로 기록된다. 그래서 하나의 PR에 담긴 commit message가 다른 PR의 commit message와 섞이지 않는다. 그리고 rebase 덕분에 merge된 이후의 로그를 보았을 때 하나의 브랜치에서 연속적으로 작업한 것과 같은 로그를 확인할 수 있다. 이 때문에 얼마든지 항상 원하는 수준으로 rollback 이 가능하다. 하지만 잘 적용하기 위해서는 commit을 생성할 때부터 올바른 commit 단위로 분리해야 하며, commit message 또한 설명력을 가지고 있어야 한다. 그리고 다른 PR이 먼저 merge되는 경우, rebase 작업이 필요할 수 있고 이 때 발생할 수 있는 conflict를 잘 해결할 수 있어야 한다. Summary Merge Strategy Pros Cons Merge Commit 아직 찾지 못함 불필요한 commit message가 생기고 merge 순서와 commit 순서가 별도로 기록되어 history 관리가 어려움 Squash and Merge Commit 단위 별로 꼼꼼하게 관리하지 않아도 PR title 만 제대로 관리하면 history가 깔끔하게 정리됨 Atomic level의 rollback이 어려움 Rebase and Merge Atomic level의 rollback이 용이하며 commit 단위의 history 기록이 됨 Commit을 잘 다루지 못하는 경우, rebase에 익숙하지 않은 경우 어려움이 발생 앞서 소개한 전략들의 장/단점을 정리하면 위와 같다. 개인적으로는 아직 Merge Commit의 장점을 발견하지 못했다. 결론적으로 팀원 전체가 git을 다루는데에 굉장히 익숙해서 commit 단위, commit message, rebase 등에 어려움이 없는 경우, 혹은 atomic level의 rollback이 필요한 개발 상황에서 Rebase and Merge가 선호된다. 하지만 atomic level 까지의 rollback은 필요하지 않고 팀이 이제 막 git으로 버전관리하는 법을 배우기 시작했다면 Squash and Merge가 좋을 것이다.","link":"/2021/07/11/Git-merge-strategy/"},{"title":"멘토에게 좋은 질문을 하는 방법","text":"잇다 멘토로 활동했던 3개월 남짓한 기간에 생각보다 다양한 질문을 받았다. 그 중에는 답변하고 싶게 만드는 질문이 있었고, 글이 무거운 나머지 내가 적게될 답변 하나하나가 조심스러워 결국 기간 내에 작성하지 못하게 된 질문도 있었고, 답변하기 싫은 질문도 (당연히) 있었다. 질문이 도착했음을 알리는 진동이 두려워진 적도 있었다. 좋은 질문이면 큰 어려움없이 글을 적을 수 있지만 그렇지 않으면 심적으로 부담이 많이 되기 때문이다. 그러다 문득, 멘티의 입장에서 좋은 질문을 적는 것이 생각보다 어려울 수 있겠다는 생각이 들었다. 당장 스스로가 혼란스러운데, 어디서부터 어떻게 그 상황을 전달해야할지 막연하지 않을까? 그래서 이 글을 쓰게 되었다. 좋은 질문을 고민하는 과정에서 멘티들이 스스로 답을 찾게되길 바라는 기대도 함께하면서 말이다. 최대한 스스로의 고민, 생각, 경험을 구체적으로 적자. 멘토는 질문을 받고 최대한 좋은 답변을 하기 위해 글만으로 드러나는 멘티를 있는 힘껏 이해하려고 노력한다. 문체를 보고, 스펙과 기타사항을 읽고, 최대한 질문한 사람의 입장을 헤아린다. 그래서 추상적이고 짧은 질문, 너무 간략한 스펙과 기타사항을 가진 멘티에게는 답변하기가 굉장히 어렵다. 맛집을 찾는 질문을 떠올려보자. A. 저는 맛있는 것을 먹고 싶어요. 추천해주세요. B. 저는 맵고 자극적인 음식을 싫어하고, 오늘은 밀가루 음식이 먹고 싶지 않아요. 조미료가 많이 첨가되지 않고 단백질 함량이 높은 음식을 먹고 싶은데, 제가 가진 돈이 별로 없어요. 비싸지 않으면서 괜찮은 식당을 알고 계신가요? 누가봐도 B의 질문이 더 매력적이고, 답변하기 쉽다. 때론, 멘토로서 잘 모르는 분야이더라도 어떻게든 주변에 물어물어 알려주고 싶을 정도다. 질문은 날카롭게 끝내자. 잇다 플랫폼은 질문과 답변이 채팅처럼 가능한 형식이 아니다. 그래서 좋은 답변을 단번에 이끌어내기 위해서는 질문이 조금 뽀죡해질 필요가 있다. 다시 맛집 예시를 들어보자. A. 저는 맵고 자극적인 음식을 싫어하고, 오늘은 밀가루 음식이 먹고 싶지 않아요. 조미료가 많이 첨가되지 않고 단백질 함량이 높은 음식을 먹고 싶은데, 제가 가진 돈이 별로 없어요. B. 저는 맵고 자극적인 음식을 싫어하고, 오늘은 밀가루 음식이 먹고 싶지 않아요. 조미료가 많이 첨가되지 않고 단백질 함량이 높은 음식을 먹고 싶은데, 제가 가진 돈이 별로 없어요. 비싸지 않으면서 괜찮은 식당을 알고 계신가요? A와 B의 차이는 고작 마지막 문장이다. 그러나 멘토의 입장에서 A와 B는 크게 다른 질문으로 다가온다. A는 스스로의 상황은 구체적이지만, 멘토에게 무엇을 기대하는지 알기 어렵다. 돈이 별로 없으니 돈을 많이 벌 수 있는 방법을 알려달라는 것인지, 비싸지 않으면서 괜찮은 식당을 알고 싶은 것인지, 저렴한 재료로 만들어 먹을 수 있는 음식의 레시피를 알고 싶은 것인지 질문의 의도를 알 수 없다. 반면 B는 멘토에게 질문한 목적이 뚜렷하다. 좋은 질문이 준비가 되었다면, 반드시 적절한 멘토에게 던지자. 나의 멘토링 분야 중에는 “데이터 사이언티스트로 일하는 것”이 있다. 그런데 때로 멘티들 중에 데이터 사이언티스트와 프로그래머를 혼동해서 프로그래머 관련 질문을 던지는 경우가 있다. 좋은 질문임에도 주인을 잘못 만난 기분이 들어 난처해진다. 아예 관련이 없어서 답변을 못하는 정도는 아니지만, 프로그래머보다는 답변의 질이 떨어질 수 밖에 없다. 그러니, 좋은 질문이 준비되었다면 꼭 좋은 대답을 기대할 수 있을 멘토에게 던지자. 꼭 잇다가 아니더라도 조언을 구하고 싶은 선배나 멘토가 있다면, 좋은 질문을 고민하는 과정에서 하나의 힌트가 되길 바란다. [잇다에서 보기]","link":"/2019/01/08/How-to-ask-good-question-to-my-mentor/"},{"title":"How multilingual is Multilingual BERT?","text":"\"How multilingual is Multilingual BERT?\"[1] 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가 Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual BERT에 대해 분석한 논문을 쓴 것이 아닐까? Abstract In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs. Motivation Pretrained LM 이 다양한 NLP downstream task 에서 좋은 성능을 보여주었다. Pretrained LM의 probing 연구들은 모델이 학습한 representation 이 syntactic and named entity 에서 특히 유용한 정보를 가지고 있다는 사실을 보여주었지만, 이 연구들은 영어에 대해서만 집중적으로 진행되어 왔다. (2019 년 6월 기준) (참고로, BERT 는 2018년 11월에 첫 release) Main Idea 영어에 대해서 Pretrained LM이 가지고 있는 정보, 그 중에서도 syntactic and named entity information 이 언어에 상관없이 잘 generalize 되는지 분석 - Main task: NER, POS - Main method: Zero-shot cross-lingual transfer (Multilingual BERT 모델을 한 언어에 대해 finetuning 시키고, 다른 언어에 대해 같은 task의 성능을 평가) Main Findings 아래 내용으로 학습한 Multilingual BERT (M-BERT) 는 NER과 POS task 에 대해 cross-lingual transfer ability 가 좋다. language identifier 없이 위키피디아의 문서로 학습 (140 개 언어) w/ shared word piece vocab 모든 언어쌍에 대해 zero-shot transfer 가 잘 된 것은 아니었는데 그렇다면 왜 이런 차이가 발생할까? finetuning 언어와 evaluation 언어의 vocab overlap 때문은 아님 오히려 언어의 typological 특징 때문 typological 특징도 여러 종류가 있는데 (여기서는 subject/object/verb order, adjective/noun order에 대해서만 결과를 보여줌), 그 중 SVO order 에 가장 큰 영향을 받음 transfer 하기 위한 언어에 대해 학습한 적이 있을 때 transfer 가능 M-BERT 의 중간 layer (8/12) 에서 cross-lingual information 이 높음 Detailed Experiments and Results Main Question: 무엇이 M-BERT의 zero-shot cross-lingual transferability를 만들어내는가? Preliminaries NER task dataset: CoNLL-2002, 2003 dataset, Google in-house dataset There are four types of phrases: person names (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC) lang: nl, es (2002) / en, de (2003) 총 4개 + in-house dataset with 16 languages (Arabic, Bengali, Czech, German, English, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, and Chinese) example (en) NER tagged plain text: [PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] . NER data: Wolff B-PER , O currently O a Ojournalist O in O Argentina B-LOC , O played O with O Del B-PER Bosque I-PER in O the O final O years O of O the O seventies O in O Real B-ORG Madrid I-ORG . O POS task dataset: Universal Dependencies (UD) data (Universal dependencies v1: A multilingual treebank collection, Nivre, 2019) for 41 languages Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew, Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese evaluation set: Multilingual Parsing from Raw Text to Universal Dependencies, Zemman et al. 2017 (CoNLL 2017 shared Task) example (ko): # sent_id = n01007012# text = 이 부분에서 게임과 우리 일상 생활 사이의 유사점을 찾을 수 있습니다.# text_en = There are parallels to draw here between games and our everyday lives.# translit = .i .bu.bun.e.seo .ge.im.gwa .u.ri .il.sang .saeng.hwal .sa.i.yi .yu.sa.jeom.eul .chaj.eul .su .iss.seub.ni.da.1 이 _ DET DT _ 2 det _ Translit=.i|LTranslit=_2 부분에서 부분 NOUN NN+CM Case=Advb|Polite=Form 9 advmod _ MSeg=부분-에서|Translit=.bu.bun.e.seo|LTranslit=.bu.bun3 게임과 게임 NOUN NN+CP Polite=Form 7 compound _ MSeg=게임-과|Translit=.ge.im.gwa|LTranslit=.ge.im4 우리 _ PRON PRP Person=1 6 compound _ Translit=.u.ri|LTranslit=_5 일상 _ NOUN NN _ 6 compound _ Translit=.il.sang|LTranslit=_6 생활 _ NOUN NN _ 3 conj _ Translit=.saeng.hwal|LTranslit=_7 사이의 사이 NOUN NN+CM Case=Gen|Polite=Form 8 nmod:poss _ MSeg=사이-의|Translit=.sa.i.yi|LTranslit=.sa.i8 유사점을 유사점 NOUN NN+CM Case=Acc|Polite=Form 9 obj _ MSeg=유사점-을|Translit=.yu.sa.jeom.eul|LTranslit=.yu.sa.jeom9 찾을 _ VERB VV Form=Adn 10 acl:relcl _ Translit=.chaj.eul|LTranslit=_10 수 _ NOUN NNB _ 11 nsubj _ Translit=.su|LTranslit=_11 있습니다 _ ADJ JJ Mood=Ind|VerbForm=Fin 0 root _ SpaceAfter=No|Translit=.iss.seub.ni.da|LTranslit=_12 . . PUNCT . _ 11 punct _ Translit=.|LTranslit=_ Code-switching (CS) and Transliterate (Tlit) task Code-switching: 여러 언어가 한 문장에 등장하는 경우 ex) I thought मौसम different होगा बस fog है Tlit: 음차 표기 ex) I thought mosam different hoga bas fog hy M-BERT 의 cross-lingual transferability 는 vocab overlap 때문일까? → NO vocab overlap: fine-tuning dataset (train) 의 word piece 와 evaluation dataset (test) 의 word piece 간의 overlap \\[overlap = {| E_{train} ∩ E_{eval} |}{| E_{train} ∪ E_{eval}|}\\] 검증 방식: NER task 중 16개의 언어에 대한 in-house 데이터셋으로 가능한 언어쌍 (16 * 15 = 240 개) 에 대해 overlap을 구하고, trasfer score (F1) 를 report (결과) M-BERT는 vocab overlap 과 무관하게 generally 성능이 좋다. vocab overlap 이 0인 언어쌍에 대해서도 최소 40%의 F1 score 를 보인다. 반면 EN-BERT 는 vocab overlap 에 굉장히 많이 영향을 받는다. M-BERT의 cross-lingual transferability 는 언어의 typological 특징 때문일까? → YES 근거: POS accuracy of ur → hi (91%) while en→ ja (49.4%) (둘 다 다른 script 를 사용하는 언어쌍, a.k.a vocab overlap ~= 0) typological features[2] (결과) 공통된 typological feature 의 개수가 많을수록 transferabiltiy 향상 (결과) 여러 typological features 중에서 SOV order 와 AN order의 영향을 비교했을 때 전자가 더 영향이 큼 M-BERT의 cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? → CS (YES) / Tlit (NO) CS 실험 목적: Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would beneﬁt to an even larger degree from a shared multilingual representation. Tlit 실험 목적: Generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-BERT was not pre-trained on text that looks like the target. (결과) M-BERT는 CS text 에 좋은 성능을 보임 (90.56% ⇒ 86.59%). 하지만 Tlit 은 이러한 종류의 데이터에 학습되지 않고서는 trasferability를 기대하기 어려움 (85.64% ⇒ 50.41%) 4. M-BERT의 feature space WMT16 병렬 코퍼스를 사용해서 언어쌍 간의 NN accuracy 측정 (결과) 중간 layer에서 linguistic information을 공유하고 이는 언어에 관계없이 비슷하게 나타남 My Thoughts on the Results vocab overlap 실험에서 EN-BERT와의 비교는 정당한가? 제 추측으로는, vocab overlap 실험에서 M-BERT 만을 보았을 때, 이 경향성이 overlap에 영향을 받는 것인지 아닌지 판단하기 어려웠기 때문에 상대 비교를 할만한 결과가 필요해서 EN-BERT의 실험 결과를 넣은 것 같다. 아래 이미지에서 corr 을 보면 어느 정도 유의미해 보이는 양의 상관관계가 나올 것 같고, 그렇다면 영향을 받는다고 해석해야할 수도 있지만, vocab overlap이 0임에도 40%의 성능을 보이므로 영향을 받는다고 하기도 애매한 상황이 아니었을까? 그렇지만 EN-BERT와의 비교가 정당하다고 생각하긴 어려운 것 같다. NER 예측 task 는 sent -&gt; model (either M-BERT or EN-BERT) -&gt; last activation -&gt; add.layer -&gt; NER prediciton 로 진행되는데, sent 가 영어가 아닌 경우 tok 단계에서부터 unk으로 인식될 가능성이 높기 때문에 transfer는 고사하고 fine-tuning도 어려울 수 있다. 이 때문에 논문에서 EN-BERT와 XLM을 비교하는데, Indo-european 인 (de, nl, es) 에 대해서만 나와있다. (Table 3) 영어와 alphabet 이 비슷하면, unk 이 나오지 않을 가능성이 높다고 생각해서 위의 결과가 EN-BERT로 얻어진 것에 대해 크게 거부감이 들지 않았고, 오히려 CJK 에 대해서 보였어야 하는 것 아닌가 하는 의심이 들었다. 그래서 🤗 로 간단하게 tokenize 결과를 비교해보았다. sent 가 es 인 경우 sent: Por su parte , el Abogado General de Victoria , Rob Hulls , indicó que no hay nadie que controle que las informaciones contenidas en CrimeNet son veraces . M-BERT tok: Por su parte , el Ab ##oga ##do General de Victoria , Rob Hull ##s , ind ##ic ##ó que no hay nadie que controle que las informa ##ciones conte ##nida ##s en Crime ##Net son vera ##ces . EN-BERT tok: Po ##r su part ##e , el A ##bo ##gado General de Victoria , Rob Hull ##s , in ##dic ##ó que no hay na ##die que control ##e que las inform ##ac ##ione ##s con ##ten ##idas en Crime ##Net son ve ##race ##s . sent 가 ko 인 경우 sent: 언어(言語)에 대한 정의는 여러가지 시도가 있었다. M-BERT tok: 언 ##어 ( 言 語 ) 에 대한 정 ##의 ##는 여러 ##가지 시 ##도가 있었다 . EN-BERT tok: [UNK] ( [UNK] [UNK] ) [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] . 위의 결과로 미루어보아, vocab overlap 이 낮으면서 성능도 낮았던 점들의 언어쌍에는 EN-BERT에서 unk이 많이 나왔던 언어가 포함되어 있지 않을까하는 생각이 들었고, 비교가 공정하지 않다는 생각이 들었다. 또한 tlit. 을 M-BERT 가 못하는 이유로, pre-train step에서 tlit. corpus 가 없었기 때문이라고 언급하였는데 EN-BERT 또한 같은 이유로 성능이 낮을 수 밖에 없었을 것이라고 생각. SOV order 가 중요한 점이었을까? 논문에서는 아래 표에서 SVO -&gt; SVO (81.55) &gt; SVO -&gt; SOV (66.52) 라는 점 때문에 SOV order 가 가장 중요하다고 주장한다. 하지만 반대로 SOV -&gt; SOV (64.22) &gt; SOV -&gt; SVO (63.98) 의 차이가 적게 나는 점은 설명할 수 없다. 제 추측으로는, 오히려 각 그룹을 구성하는 언어와 그 언어들이 Wikipedia 에서 차지하는 비율, 즉 M-BERT 학습에 영향을 많이 끼친 언어가 중요한 역할을 했을 수도 있을 것 같다. SVO languages: Bulgarian, Catalan, Czech, Danish, English, Spanish, Estonian, Finnish, French, Galician, Hebrew, Croatian, Indonesian, Italian, Latvian, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese (European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish, and Chinese. SOV Languages: Basque, Farsi, Hindi, Japanese, Korean, Marathi, Tamil, Telugu, Turkish, and Urdu. Urdu -&gt; Hindi 의 성능이 91% 였던 점을 고려하면 이 중 어떤 언어쌍에서 굉장히 낮은 성능을 보였을 것으로 예상 (평균이 64.22 이어야하므로) 그룹의 평균치를 보지 않았다면 다른 해석이 가능했을 수도..? SVO order 가 비슷하면 -&gt; transfer 가 잘된다! 라는 주장을 하고 싶었다면 비교하려는 대상 언어쌍들간에 SVO order 빼고는 조건을 동일하게 만족시켰어야 하지 않을까하는 아쉬움이 남는다. Feature space MLM은 보통 중간 layer 에서 semantic 한 성질이 가장 두드러지게 나타나는 것 같다. credit) BERTScore: Evaluating Text Generation with BERT 논문의 분석 내용 논문에서 분석한 결론이 지나친 일반화가 아닐까하는 생각도 든다. 일단 M-BERT가 zero-shot transferability 가 높은 이유는 어쩌면 같은 내용의 Wikipedia 문서로 학습했기 때문일 수 있다. 만약에 한국어는 동일 내용에 대한 번역된 문서가 없는 (e.g., 네이버 블로그) 로 학습하고, 영어도 영어 나름대로의 번역문이 없는 문서로 학습된 BERT 였더라도 같은 결과가 도출되었을지는 모르겠다. M-BERT 의 학습 데이터의 특징에서 기인한 특징이 얼마나 되는지도 궁금하다. 오히려 여기에서 영향을 많이 받았을 수도 있을 것 같다. References 1.https://www.aclweb.org/anthology/P19-1493.pdf ↩︎ 2.https://www.aclweb.org/anthology/P12-1066.pdf ↩︎","link":"/2020/06/20/How-multilingual-is-multilingual-BERT/"},{"title":"Machine Learning Yearning 요약: Ch.13~19","text":"목적에 맞는 Dev와 Test set을 구축했다면, 이제 모델이 얼마나 잘하고 있는지, 못한다면 그 이유는 무엇인지에 대한 분석을 할 수 있다. 그래서 이번에 다룰 주제는 Basic Error Analysis이다. Ch.13: Build your first system quickly, then iterate Ch.14: Error analysis: Look at dev set examples to evaluate ideas Ch.15: Evaluating multiple ideas in parallel during error analysis Ch.16: Cleaning up mislabeled dev and test set examples Ch.17: If you have a large dev set, split it into two subsets, only one of which you look at Ch.18: How big should the Eyeball and Blackbox dev sets be? Ch.19: Takeaways: Basic error analysis 이해의 용이성을 위해, 내가 속한 팀이 사용자들이 올린 이미지를 classification하는 모델을 서비스한다고 가정해보자. 팀원들은 모델의 classification accuracy를 향상시키고 싶을 것이고, 이를 위해 적절한 training, dev, test set을 구축하였다. 학습시킨 모델의 정확도는 90%이다. 하지만 이 정도로는 서비스에 반영하기 어렵다. 자, 이제 우리는 어떻게 해야할까? 일단, 10%의 에러가 어떤 데이터에 대해서 발생하는지 분석(=error analysis)해야 한다. 좋은 분석은 적절한 가정들을 바탕으로 이루어진다. 우리 팀이 생각한 모델의 문제는 “개(dog) 이미지를 잘못 분류한다”이다. 지난 Chapter에서 dev set이 error analysis를 위한 data set임을 설명했다. 이 중 모델이 misclassify한 데이터에서 100개를 sampling한다. 그리고 우리의 가설-개 이미지를 잘못 분류-과 일치하는 case를 센다. 만약 5개였다면, 에러율 10%의 5%가 해당 케이스인 것이므로 모델이 앞으로 “개 이미지를 제대로 분류”한다고 하더라도 10% * 0.05 = .5% 정도의 향상을 기대할 수 있다. 좀 더 효율을 높여보자. 우리는 지금 가설 1개에 대해서 100개의 sampling된 데이터를 살펴보았는데, 이 작업은 가설 여러개에 대해서 동시에 진행할 수 있다. &gt; 가설1: 개 이미지를 잘못 분류 &gt; 가설2: 고양이과(사자, 표범 등) 이미지를 잘못 분류 &gt; 가설3: 흐린 이미지를 잘못 분류 Andrew Ng은 헷갈리지 않도록 아래 그림처럼 spreadsheet을 만들어서 작업을 진행한다고 한다. % of total을 통헤 현재 모델이 가장 critical하게 개선해야 하는 점이 무엇인지를 바로 파악할 수 있다. 모델의 성능이 좋지 않을 때 항상 염두에 두어야 하는 가설이 있다. 바로, “mislabeled data의 비율이 얼마나 되는가?”이다. 데이터의 양이 많을수록 수집과정에서 예상치 못한 에러가 발생한다. mislabeled data가 발견된다면 training data의 mislabel은 어쩔 수 없으니 내버려두고, dev set과 test set은 수정하는 것을 권장한다. (dev와 test set의 distribution은 같아야 하므로) 지금까지 소개한 error analysis 방법은 misclassify된 data 중에서 100개에 적용되었다. 이 말은 모델의 성능이 95%의 accuracy라고 가정했을 때, dev set의 크기가 100/0.05 = 2,000 정도라는 뜻을 함유하고 있다. 만약 나의 dev set이 이 정도의 규모보다 작다면, 혹은 크다면 어떻게 해야할까? Andrew Ng이 제안한 100개의 error data 개수는 이 정도가 모델의 major error source로서 매우 좋은 인사이트를 발견할 수 있다고 생각했기 때문이다. ~100개: 매우 좋은 인사이트 발견 ~50개: 좋은 인사이트 발견 ~20개: 개략적인 인사이트 발견 ~10개: 불충분. 하지만 적은 dev set을 가지고 있다면 없는 것보다는 나은 정도 그래서 dev set이 작다면 모든 데이터를 error analysis를 위해 사용하는 것이 좋다. 만약 dev set이 크다면, eyeball dev set과 blackbox dev set으로 나누어 사용한다. eyeball dev set은 error analysis를 하면서 계속 모델이 잘하거나 못하는 것을 살펴볼 용도로 사용하는 dev set이고, blackbox dev set은 모델의 파라미터를 튜닝하기 위한 용도로만 사용하는 dev set이다. 만약 eyeball error rate &lt;&lt; blackbox error rate 이라면 모델이 eyeball dev set에 overfitting하고 있으므로 새로운 eyeball dev set을 구축하는 것으로 그 위험을 방지한다. 여기에도 예외가 있는데, 사람조차 풀기 어려운 과제를 위한 모델을 만들고 있다면 eyeball dev set은 전혀 도움이 되지 않을 것이므로 eyeball dev set을 만들지 않아도 좋다. 그리고 여기까지 소개된 일련의 과정-데이터 수집 -&gt; 모델 구성 -&gt; 학습 -&gt; 결과 분석 pipeline-이 최대한 빨리 iterate하는 것이 중요하다. 처음부터 완벽한 것은 없다. 일단 시작하자!","link":"/2019/01/01/Machine-Learning-Yearning-summary-Ch.13~19/"},{"title":"Machine Learning Yearning 요약: Ch.1~4","text":"Deep learning으로 예전에는 풀지 못했던 문제들을 풀게 되면서 다양한 기업에서 자신들의 서비스에 Deep learning을 활용하려는 시도가 많아졌다. 그러나 생각보다 Deep learning을 서비스에 적용하는 과정은 간단하지 않다. 연구 목적으로 사용되는 것보다 데이터가 훨씬 크고, 이 때문에 한번 모델을 학습시키는데 소요되는 시간이 더 길다. 더욱이 서비스로 배포되기 위해서는 매우 정확해야 하므로 (서비스의 특징에 따른 차이는 있겠지만 대체적으로) 모델의 검증과 재학습의 iteration이 더 많이 이루어진다. 그러므로 연구실에서보다 회사에서 더 전략적인 판단의 과정이 필요하다. 우리 팀도 NMT를 서비스에 반영하면서 다양한 고민에 부딪혀왔다. 그래서 Andrew Ng이 쓴 &lt;&gt;을 같이 읽고 있는데, 책 구석구석 고민해왔던 문제들에 대한 Andrew Ng 만의 해결책이 적혀있어 속이 뻥 뚫리는 기분을 맛보고 있다. (아무래도 Andrew Ng 은 Geoffrey Hinton, Yann Lecun, Joshua Bengio와 다르게 산업적인 측면에서의 Deep Learning에 더 관심이 많은 분인 듯하다.) 책 자체가 쉽게 쓰여져 있고, 각 챕터가 1~2장 정도밖에 되지 않아 부담은 적지만 큰 주제 별로 요약하면 더 많은 사람들이 쉽게 내용을 이해하고, 각자의 분야에 접목시킬 수 있을 것 같아서 이 글을 쓰게 되었다. Chapter 1~4는 “왜 이 책이 필요한지, 어떻게 이 책을 활용하면 좋을지”에 대한 내용이 담겼다. 목차를 보면 다음과 같다. Ch.1: Why Machine Learning Strategy Ch.2: How to use this book to help your team Ch.3: Prerequisites and Notation Ch.4: Scale drives machine learning progress Chapter 2는 “이 책은 한 챕터가 짧으니 인쇄해서 팀원에게 보여줘라”로 요약할 수 있고, Chapter 3은 “Supervised Learning과 Neural Network에 대한 지식Andrew Ng의 MOOC를 들어라”으로 요약가능하다. Chapter 1과 4를 종합하면 아래의 내용이다. (서비스에 적용하기 위해 도달해야 할) 높은 정확도의 모델은 많은 데이터를, 이를 감당할 수 있을만큼 큰 Neural Network에 학습시킬 때 얻을 수 있다. 그러나 이 과정은 (당연히) 한 번에 이루어지지 않고, 다양한 시행착오가 수반된다. 모델의 성능이 좋지 않을 때, 우리가 택할 수 있는 전략은 다양하다. 더 많은 데이터를 얻는다. 다양한 training set을 구한다. 더 오래 학습시킨다. 더 큰 Neural Network를 학습시킨다. 더 작은 Neural Network를 학습시킨다. Regularization을 추가한다. Neural Network 구조를 바꾼다. … 이 중 무엇을 우선적으로 시도해 보아야할까? 그 답은 현재 내가 학습시킨 model에 있다. 그리고 이 책은 model이 남긴 단서들을 어떻게 활용해서 답을 찾을 수 있는지에 대한 내용을 담고 있다. Andrew Ng은 바쁜 직장인들도 쉽게 내용을 이해할 수 있도록 책에 다양한 배려를 심어두었는데, 그 중 하나가 Chapter 들을 소주제로 묶어둔 것이다. Setting up development and test sets Basic Error Analysis Bias and Variance Learning curves Comparing to human-level performance Training and Testing on different distributions Debugging inference algorithms End-to-end deep learning Error analysis by parts 이 주제를 보면 책의 큰 흐름을 알 수 있을 것이다. 다음 글은 Setting up development and test sets에 대한 내용이다.","link":"/2018/12/25/Machine-Learning-Yearning-summary-Ch.1~4/"},{"title":"좋아하는 일은 어떻게 찾을 수 있을까?","text":"잇다라는 온라인 멘토링 플랫폼에서 멘토로 활동하면서 가장 많이 받는 질문은 “좋아하는 일을 어떻게 찾을 수 있나요?” 이다. 좋아하는 일, 어떻게 찾을 수 있을까요? 주변에서 좋아하는 일을 하라고들 많이 말하는데 막상 저에 대해 생각해보면 진짜 제가 뭘 좋아하는지 모르겠습니다. 뭘 좋아하는지부터 알아야 앞으로 나아갈 수 있을 것이라고 생각이 드는데 어떤 방법으로 찾을 수 있을지 궁금합니다. 나를 탐색하는 방법이나 추천하고 싶은 방법을 알려주세요. 진로는 어떻게 찾아가셨나요? 사실 지금 직장은 나이때문에 부랴부랴 들어간 곳이라 보람감 혹은 성취감 없이 다니고있습니다,, 제가 정말 성장하고 흥미를 느낄만한 분야를 찾고싶은데 사실 진로를 찾고 발을 내딛기가 엄두도안나고 막막하기만 하네요, 멘토님은 자신의 진로를 어떻게 찾아가셨나요? 지금 이 시점에 돌이켜보니 그저 소속된 곳에서 흘러가는대로 살아왔던 것 같고, 이제 무소속이 된 지금 이 시점에 어떤 직업을 가져야 할지 내가 과연 원하는 것이 무엇일지 너무 답답해서 (중략) 혹시나 멘토로서 조언해주실 것이 있다면 어떤 피드백이라도 주시면 감사하겠습니다. 좋아하는 일은 많고 하기에는 두려워요. 기계공학과를 오긴 했지만 제가 정말 하고싶은 일이 기계를 다루는 일일까라는 걱정도 많이 되고... 멘토님은 좋아하는 일을 어떻게 찾으셨는지 궁금합니다! 멘티들의 질문을 마주하고 글 이면에 숨겨진 각자의 어려움을 상상하다보면, 나의 대학시절의 모습이 많이 떠오른다. 무비판적으로 지식과 이론을 습득하기에 바빴던 고등학생이 대학생이 되어 스스로의 생각으로 걸어나가기 위해 무수히 많이 넘어졌던 시절이. 나에 대해 깊게 고민해 본 시간이 없었기 때문에 내가 무언가를 좋아한다는 것이 무엇인지조차 알 수 없었던 시절이. 나의 생각보다 타인의 생각을 더 높게 평가했기 때문에 다른 사람들이 하는 선택과 남들이 좋다고 생각하는 것에 이리저리 휘둘리기도 했던 시절이. 이미 본인의 길을 걸어가고 있는 친구들을 보며 부러움과 동시에 나의 길이 과연 세상에 존재는 하는 것일지에 대한 불안감을 느꼈던 그 시절이. 이 모든 혼란스러움과 불안의 시간을 거쳐, 나는 끝내 내가 기꺼이 걷기로 선택한 나만의 길을 찾을 수 있었다. 멘티들의 상황이 모두 나의 예전과 같다고는 할 수 없지만 내가 비슷한 시기에 스스로에게 질문을 던지고 이에 답했던 내용들이 조금이라도 멘티들에게 힌트가 되길 바라며 이 글을 쓰게 되었다. 우리는 왜 좋아하는 일을 찾으려고 할까? 인생에서 대부분의 시간을 일을 하며 보내기에 그 시간을 행복하게 쓰고자 하기 위함이다. 그래서 나에게 “좋아하는 일을 어떻게 찾을 수 있나요?”라는 질문은 “내게 행복을 주는 일은 어떻게 찾을 수 있나요?”와 같다. 이 질문에 대한 답을 하기 위해 나는 스스로에게 다음의 세 질문을 던졌다. 내가 행복했던 경험들은 무엇이고 그 경험들의 어떤 특징이 나를 행복하게 만들었을까? 내 행복의 요인을 지속적으로 제공할 수 있는 일은 무엇일까? 그 일을 선택하게 됨으로써 내가 부딪힐 수많은 어려움을 나는 감당할 자신이 있을까? 내가 행복했던 경험들은 무엇이고 그 경험들의 어떤 특징이 나를 행복하게 만들었을까? 이 질문에 답하기 전에 \"행복\"이라는 것에 대해 한 번 짚고 넘어갈 필요가 있다. 우리가 행복이라는 말을 통해 의미하는 것은 대개 잠시의 쾌감에 가깝다. 행복이란, 온천물에 들어간 후 10초 같은 것. 그러한 느낌은 오래 지속될 수 없다. 오래 지속될 수 없는 것을 바라다보면, 그 덧없음으로 말미암아 사람은 쉽게 불행해진다. 김영민한국일보 칼럼 새해에 행복해지겠다는 계획은 없다 짧은 순간에 머무르는 말초적이고 자극적인 쾌락을 행복이라고 정의해선 안된다. 또한 행복의 원인이 타인이나 주위의 환경(주변의 기대, 동경, 인정 등)에 의존적이라면, 다시 생각해보아야 한다. 그래서 나는 행복했던 경험을 찾기 위해 순간의 감정에 집중하기보다 오랜 기간 지속해온, 혹은, 해왔던 활동들에 집중했다. 사람은 저마다 각자의 행복을 위해 산다[1]. 그리고 삶은 유한하다. 주어진 시간을, 사람들은 저마다의 행복을 최대화할 수 있는 선택들으로 채워나간다. 그렇기 때문에 수많은 선택지들 중에서 하필 왜 \"그 선택\"을 내렸는가에 답이 들어있을 것이라 믿었다. 예를 들어보자. 나는 잇다에서 멘토로 활동하고 있다. 잇다의 멘토는 아무런 금전적 보상을 받지 않는다. 그럼에도 멘토링을 하는 이유는 내 도움을 필요로 하는 멘티들이 내 답변을 통해 도움을 얻었을 때 뿌듯하기 때문이다. 그 순간이 내게는 보상이다. 이런 나의 선택을 통해 나라는 사람에게 돈이라는 가치는 크게 중요하지 않다는 사실을 유추할 수 있다. 오히려 내게 중요한 것은 타인에게 미치는 긍정적인 영향력이다. 다른 사람들이 나로 인해 좀 더 나은 삶을 영위하게 되었을 때 행복해진다. 내 행복의 요인을 지속적으로 제공할 수 있는 일은 무엇일까? 내 행복한 경험의 특징을 더 구체적으로 생각해보자. 좀 더 나은 삶이란 무엇인가? 몸이 아픈 사람이 건강해지는 것? 모든 사람에게 동일한 기회가 제공되는 것? 나의 답은 서비스를 통해 더 편리해지고 유익해지는 삶. 그리고 획일화된 사고방식을 강요하는 교육이 아닌, 다양한 가치를 존중하고 나만의 고유함을 인정받는 교육을 제공받는 삶이었다. 사람들의 삶이 더 편리해지고 유익해질 수 있는 서비스를 직접 만들거나, 이미 만들고 있는 기업에 참여하는 과정일 것이다. 그런 서비스는 무엇일까? 나의 답은 사람들이 원하는 것을 읽어내 그 부분을 채워주는 서비스였다. 그리고 그 방법으로 떠올린 것이 “데이터 분석”이었다. 고등학교 때부터 데이터에는 힘이 있다고 믿었다. 잔반량이 문제였던 고등학교 시절, 음식의 맛과 잔뱐량의 상관관계를 데이터로 파악할 수 있다면 금방 잔반량을 최소화하는 방법을 마련할 수 있을 것이라고 생각했다. 맛없는 음식은 급식의 품질에 달려있고, 품질은 급식비와 큰 관련이 있기 때문에 적절하게 급식비를 올리면 해결될 수 있는 문제였다. 마찬가지로, 사람들이 사용하는 서비스에는 어쩔 수 없이 사람들이 남긴 흔적들이 있을 것이다. 그 흔적을 분석하여 사용자들이 원하는 바를 읽어내고, 새로이 서비스에 반영하면 사람들의 삶이 더 편리해질 것이라고 믿었다. 이 일을 하는 사람들은 데이터 분석가, 데이터 사이언티스트라는 이름으로 불리고 있었고, 현실적으로도 내가 도전해볼 수 있는 영역 안에 자리했다. 고유함을 인정받는 교육을 실현하는 과정을 구체화하는 것은 어려웠다. 현재의 교육 시스템을 바꾸어야 가능한데, 교육에 관련된 많은 사람들-학부모, 학생, 학원 강사, 학원 원장, 교육부, 학교, 교사 등-의 공감을 얻어야만 가능한 일이기 때문에 쉽게 그려지지 않았다. 교육에 관련된 일을 하고 있는 직업도 내가 하고싶은 일이라고 느껴지지 않았다. 교사와 학원 강사는 이미 현재 교육 시스템 안에서 활동하는 사람이기 때문에 혁신과는 거리가 멀었다고 생각했기 때문이었고, 교육부장관은 교육에 관련된 수많은 이해관계자들 속에서 마땅한 해결책을 제안할 수 없을 뿐더러 top-down 방식으로는 절대 교육의 혁신이 이루어지지 못할 것이라고 생각하였기에 제외하였다. 오히려 내가 직접 학교를 세워 그 안에서 자유롭게 내가 생각하는 교육을 실현하는 방식을 꿈꾸었다. 그러나 당시 내가 처한 현실-화학생물공학 전공의 3학년 대학생-에서는 길이 보이지 않았기 때문에 마음 속에만 간직해둔 채로 언젠가 다가올 기회만을 기다리기로 했다. 그 일을 하기로 결정하게 됨으로써 앞으로 부딪힐 수많은 어려움을 나는 감당할 자신이 있을까? 여기까지 구체화되었다면, 남은 것은 목표를 향해 움직이는 것 뿐이었다. 먼저, 데이터 사이언티스트는 어떻게 해야 될 수 있는지를 알아보아야 했다. 그 당시만해도 지금처럼 이 직업이 핫하지는 않았다. 그래서 어떤 회사에 들어가야 데이터 분석가로 일할 수 있는지, 어떻게 해야 데이터 분석가가 될 수 있는지 자세히 알기 어려웠다. 더욱이 나의 전공은 이 분야와 관련이 없어서 주변에 데이터 분석가로 일하고 있는 선배를 소개받기도 어려웠다. 또한, 데이터 분석과 관련된 공부를 시작해야했다. 내가 원하는 바와 가장 가까웠던 연구실-산업공학 데이터마이닝 연구실-을 찾았고, 그 연구실에 입학하고 싶기도 했고, 교수님의 수업을 듣고 싶었다. 그렇게 산업공학 부전공을 시작하게 되었다. 내 주변에서는 아무도 선택하지 않았던 길이라 많이 두려웠다. 다른 사람들에 비해 출발이 늦었다고 생각했기 때문에 불안하기도 했다. 그렇지만, 지금이라도 늦지 않았다고 나를 다독였다. 하고 싶은 것을 향해 나아가는 과정에서 더 많은 어려움이 있을 것을 직감했다. 현실적인 제약으로 수많은 좋은 방법을 포기해야 하기 때문에, 전혀 다른 분야여서 내가 배워야할 것이 정말 많기 때문에, 다른 사람들에 비해 뒤쳐져 있기 때문에, 그런 와중에도 같은 목표를 향해가는 수많은 사람들 속에서 나만의 고유함을 계속 갈고 닦아야 하기 때문에 정말 많이 넘어지고, 깨지고, 좌절하는 경험이 많을 것이었다. 그럼에도 불구하고, 나는 데이터 사이언티스트가 되고 싶었다. 그래서 내 선택에 이어질 모든 순간들을 받아들이기로 결심했다. 여기까지 생각이 정리되자, 내게 무언가를 할 용기가 생겼다. 스스로에게 끊임없이 물었던 시간들만큼 나의 결정에 무게감이 실렸다. 실제로 그 이후의 많은 시간들은 힘겨웠지만 포기하진 않았다. 그 때 버텨냈던 경험 덕분에 지금의 나는 행복할 수 있었고, 그 경험으로 당시의 나와 같은 고민을 하고 있을 멘티들에게 도움을 줄 수 있었다. 마지막으로, 여기까지 글을 읽었을, 스스로에 대한 고민이 많을 사람들에게 전해주고 싶은 문장을 인용하며 마무리 지으려고 한다. 실망이라는 향유. 실망은 불행이라고 간주되지만, 이는 분별없는 선입견일 뿐이다. 실망을 하지 않는다면 우리가 무엇을 기대하고 원했는지 어떻게 발견할 수 있으랴? 또한 이런 발견 없이 자기 인식의 근본을 어떻게 알 수 있으랴? 그러니 실망이 없이 자기 자신에 대한 명확함을 어떻게 얻을 수 있으랴? 리스본행 야간열차 [잇다에서 보기] 1.문유석, 개인주의자 선언 ↩︎","link":"/2018/11/24/How-to-find-my-job/"},{"title":"답은 언제나 나에게 있다","text":"삶을 살아나간다는 것은 모두에게 주어진 같은 24시간을, 나의 선택들로 채워나가는 것이다. 하지만 고등학교 때까지 나에게 주어진 대부분의 시간은 다른 누군가에 의해 정해졌다. 부모님에 의해, 학교에 의해, 학원에 의해. 특히 고등학교 때는 7시까지 11시까지 정해진 수업시간, 야자시간으로 채워져 있어 내가 자유롭게 쓸 수 있는 시간은 더욱 없다. “대학 입학”이라는 공동의 목적을 향해 “성적을 올리는 것”을 공동의 목표로, 우리 모두는 정해진 시간 속에 살아간다. 하지만 대학 입학 이후, 환경은 갑자기, 극적으로 달라진다. 갑자기 공동의 목표가 사라지고, 갑자기 선택의 자유가 생기고, 갑자기 결정에 대한 책임이 주어진다. 갑자기 방향을 잃어버린 학생들에게 갑자기 “네가 좋아하는 것을 하고 살라”고 한다. 그래서 소심하게 내가 하고 싶은 것을 찾아 시간을 조금씩 채워본다. 바이올린을 배우고 싶어서 오케스트라 동아리에 들고, 독립이 하고 싶어서 자취를 시작한다. 술자리를 가지고, 미팅에 나가보고, 주점을 열고, 학교 축제에도 참여해본다. 그러다 보면 어느새, 졸업을 한 이후의 나의 모습을 고민해야 할 시기가 다가온다. 주변을 둘러보기 시작한다. 선배들은 어떤 진로를 택했는지, 친구들은 어떤 진로를 택하려고 하는지를 살핀다. 지도 교수님, 부모님 혹은 선배의 조언을 구한다. 다양한 충고, 조언이 쏟아져 나온다. “확실히 대기업이 좋긴 좋다. 복지도 좋고, 월급도 많고. 대기업 와, 괜찮아.” “사회에 나와보니까 왜 의사, 의사 하는지 알겠더라. 너는 꼭 기회가 남아있을 때 의전 가라.” “4차 산업 혁명 때문에 난리다. 우리 회사에서도 그쪽 관련된 전공자들 엄청 뽑으려고 하더라. 너도 그쪽 공부해두는 게 좋을 거야.” “그 전공으론 취직 어려울 거다. 차라리 공무원 준비를 하든지, 로스쿨 가.” “대학원 절대 오지 마라.” 진로 고민을 덜어내고자 만났던 사람들이지만 오히려 고민의 무게만 늘어난다. 대기업에 취직할까 싶다가도, 의전이나 로스쿨에 지원하는 것이 좋아 보이다가도, 아직 사회에 나갈 준비가 안됐다는 불안감에 대학원에 갈까 싶다가도, 익숙하게 고시 공부를 하는 것이 맞아 보이기도 한다. 하지만, 그들의 경험과 ‘나’의 경험은 다르다. 그들의 성향과 ‘나’의 성향은 다르다. 그들이 잘하는 것과 ‘내’가 잘하는 것은 다르다. 그들의 관심사와 ‘나’의 관심사는 다르다. 그들의 조언은 어디까지나 그들의 입장에서의 이야기일 뿐이다. 그래서 그 이야기에 좌지우지될 필요가 없다. 대신, '나'의 이야기는 꼭 들어보아야 한다. 미래에 대한 불안감과 두려움은 단지, 어떤 선택을 함으로써 닥쳐올 미래의 일들을 내가 얼마나 잘 감당할 수 있고, 더 나아가 행복할 수 있을지 알지 못하는 “나에 대한 무지함” 때문에 비롯된 것이기 때문이다. 스스로에게 물어보자. 그리고 솔직하게 대답해보자. 복지가 좋고, 월급이 많은 것이 나에게 중요한가? 남들이 동경하는 직업에, 개인 병원을 차리면 돈을 많이 벌 수 있다는 사실이 나에게 중요한가? 4차 산업 혁명이 무엇인지 체감이나 하고 있는 것일까? 그래서 그게 왜, 얼마나 중요한지도 나는 알고 있을까? 나는 한 분야에 대한 좀 더 전문적인 지식을 알고 싶은데, 내가 대학원에 가지 말아야 할 이유란 대체 무엇일까? 성적이 좋았기 때문에 주변 사람들 모두 의사가 되길 기대했고, 스스로도 의사가 되고 싶다고 생각해서 의과 전문대학원에 진학하기 위해 화학생물공학부를 전공으로 택한 학생을 만난 적이 있다. 원하는 대학, 원하는 전공에 합격했지만 어딘가 모르게 답답하고 ‘이게 맞는 걸까?’라는 생각이 자꾸만 든다고 했다. 주변에 이런 불안감을 토로해도 ‘네가 피곤해서 그런 걸 거야.’, ‘살다 보면 그럴 때가 있지.’라는 가벼운 대답만 돌아왔다고 했다. 처음에는 가볍게 넘겼지만, 학년이 올라갈수록 ‘이건 아니다’라는 생각이 더 강하게 들었다고도 했다. “지금은 의사가 되고 싶지 않은 건가요?” 그런 것 같아요. “왜 그런 것 같나요?” 어렸을 땐, 의사라는 직업에 대한 이해가 부족했어요. 막연히 어른들이 좋다고 하니까, 그리고 나는 뭐든 할 수 있는 사람이니까, 게다가 아무나 꿈꾸는 게 아니었으니까 그냥 하고 싶었나 봐요. 그런데 지금은 아니에요. 의사라는 직업이 현실적으로 다가오면서 나와 맞지 않는 것이 조금씩 보여요. 과에서 생물학을 배우는데, 정말 안 외워져요. ‘이거 알아서 어디에 쓰지?’라는 생각 때문에 동기부여가 되질 않아요. 또 의사는 매번 아프고 힘든 사람을 만나잖아요. 저는 사람들을 통해 힘을 많이 얻고, 주변 사람의 영향을 많이 받는데 항상 만나는 사람들이 환자라면 견디지 못할 것 같아요. 돈에도 큰 관심이 없고요. “스스로에 대한 이해가 좋은 편인 것 같아요. 지금 듣기에는 의사를 하고 싶지 않은 것이 명확한데, 어떤 고민이 있나요?” 이제야, 내가 하고 싶은 것이 생긴 것 같은데 지금 전공과 너무 달라서 불안하고 두려워요. 잘할 수 있을지, 이래도 되는 것인지 모르겠어요. “하고 싶은 게 뭔데요? 무엇이 불안한가요?” 저는 사실 고등학교 때부터 데이터에는 힘이 있다고 믿었어요. 어떤 문제 상황이 있을 때, 그 상황에 대한 데이터만 있다면 모를게 하나도 없다고 생각했으니까요. 대학생활을 하다 보니까 저는 남들보다 그런 상황을 더 많이 발견하고, 그래서 해결해보고 싶다는 욕구를 많이 느끼더라고요. 다양한 앱을 사용하는 것을 좋아하는데요, 앱의 업데이트되는 방향을 보면 어떨 때는 ‘와 진짜 잘한다’하는 것이 있고 ‘이걸 왜 했지? 사용자들의 마음을 이렇게나 모르나?’ 싶을 때가 있어요. 후자의 경우에 제가 막 데이터를 보고 싶어 져요. 인터넷에 검색을 해봤더니 이런 일을 데이터 마이닝이라고 하더라고요. 더 찾아보니까 산업공학과에서 제가 생각한 일을 하는 연구실이 있었고, 다행인지 불행인지 학부만 졸업해서는 이런 일을 할 수 없대요. 그런데 산업공학과를 부전공으로 하는 사람은 제 주변에서 보지 못했고, 또 저한테는 너무 생소하고 이제 처음 시작하는 건데 제가 얼마나 잘할 수 있을지 모르겠어서 두려워요. 처음부터 이 학생이 이렇게 대답했던 것은 아니었다. 남에게 보이고 싶은 대로 말하기도 했고, 고민해본 적이 없어서 시간을 두고 답을 하기도 했다. 하지만 점점 스스로에게 솔직하게 이야기를 하고, 다양한 각도에서 스스로에게 묻는 질문들에 대답을 하는 과정 속에서 자신이 얼마나 데이터에 관심이 있었는지를 오히려 깨닫게 되었다. 그리고 더 이상 주변의 말에 휘둘리지 않는 굳건함을 얻게 되었다. 화학생물공학에서 산업공학으로 진로를 틀었을 때 주변에서 모두들 ‘왜? 화학생물공학을 졸업했을 때 더 돈을 많이 벌 수 있었을 텐데.’라고 이야기했지만, 그 전공을 택한 결정의 이면에는 ‘나’의 생각, ‘나’의 경험, ‘나’의 느낌이 강하게 있었기 때문에 흔들리지 않았다고 했다. 스스로를 어렵게 설득한 후에 결정을 내린 만큼 그 결정에 후회가 없을 수 있도록 많은 노력을 했고, 지금은 네이버에서 데이터를 다루는 데이터 사이언티스트로 일하고 있다. 인생은 언제나 우리에게 쉬운 질문을 던지는 법이 없다. “어떻게 살 것인가?”, “행복이란 무엇인가?”처럼 갈수록 더 어려운 질문이 우리를 기다리고 있다. 그렇다고 회피하거나, 두려워할 필요 없다. 답은 언제나 내 안에 있다.","link":"/2019/01/20/I-always-had-an-answer/"},{"title":"Naver News Comment Analysis (1)","text":"올초(3월)부터 같은 팀의 재명님과 네이버 뉴스 댓글 데이터로 사이드 프로젝트를 시작했다. 직접 크롤링하신 데이터였는데, 그 양이 방대해서 \"이 정도 데이터가 있으면, 뭔갈 해볼 수 있겠지!\" 라는 가벼운 마음으로 사이드 프로젝트 제안을 덥석 받아물었다. 그리고 여느 사이드 프로젝트가 그렇듯 그 과정은 결코 생각만큼 가볍지는 않았더랬다... 마침 작년 사내 Hackday에서 Abuser Detection 분석으로 좋은 성과를 얻었던터라 어뷰저 분석을 해보고 싶었고, 그 결과로 나름 재밌는 것들이 발견되었다. 하지만 좋은 발표 자리(이를테면 파이콘이라든지,,,)에 등록할 시기를 놓쳐서 논문을 arXiv에 올려두듯이 블로그에 댓글 분석한 내용을 공유하고자 한다. 내용은 크게 뉴스 댓글 수집과 뉴스 댓글 분석 파트로 나뉘며, 전자는 재명님이 후자는 내가 주로 담당해서 정리하였다. 이번 글은 뉴스 댓글 분석 1편이다. Data 수집 기간 2006.04.26 ~ 2018.05.25 (수집 시점: 2018.10) 수집 내용 네이버 뉴스의 6개 분야별(정치, 경제, 사회, 생활/문화, 세계, IT/과학) 가장 많이 본 뉴스 30건 같은 기사이지만 2-3일 동안 랭킹뉴스에 오를 수 있으므로 중복 기사를 제거해주었다. * 중복 제거 전 기사 #: 751,751 (약 75만) * 중복 제거 후 기사 #: 643,226 (약 64만) 분석에 사용한 필드 기사: 기사 id, 기사 제목, 기사 입력 시각, 기사 내용, 언론사, 기사 감정 댓글: 댓글 작성 기사id, 작성자 hashed id, 댓글 작성 시각, 댓글 내용, 공감수, 비공감수 Basic Statistics 중복 제거된 기사에 대해, 기사 작성 시점을 기준으로 한 달 단위로 기사에 달린 코멘트를 집계해서 그래프를 그리면 다음과 같다. 네이버 뉴스 개편 history[1] 와 엮어서 이 그래프를 해석하면 재밌어진다. 2009년 2009년 개편 때는 메인 페이지 뉴스 박스 편집권을 신문사에 넘겼고, 기사를 클릭하면 바로 신문사 링크로 연결되게 바뀌었다. 이로 인해 네이버 뉴스의 트래픽이 감소하게 되었고 예전과 비교해서 리플 개수나 조회수가 상당히 줄어들었다. 2010년 2010년대 초반에 뉴스 스탠드가 도입되면서 메인화면 뉴스 편집권을 포기하게 된다. 기사를 클릭하면 기본적으로 네이버 페이지가 아닌 언론사 사이트로 연결된다. 모바일로 댓글을 달 수 없었다. 또한 댓글 형태가 댓글 제목을 클릭해야만 내용을 볼 수 있는 형태라서 결과적으로는 당시 뉴스 댓글 란은 지금보다 훨씬 폐쇄적인 모양새였다. 2012년 검색과 지식인의 인기를 바탕으로 네이버가 2012년 1일 방문자 1800만 명을 기록할 정도로 성장하는 동안, 네티즌의 뉴스 읽기 방식도 달라졌다. 종이신문을 읽거나 신문방송의 홈페이지를 찾아가는 대신, 네이버나 다음 등 포털의 뉴스캐스트를 통해 여러 언론사 기사를 한꺼번에 읽는 사람들이 크게 늘어난 것이다. 이 때문에 뉴스 편집 기능을 수행하는 포털을 언론사로 봐야 할 것이냐 아니냐 하는 논쟁이 언론관련 심의기구 등에서 벌어지고 있기도 하다.[2] 2012년 중반부터 모바일로도 댓글을 달 수 있게 되었다. 네이버 아이디로 로그인하지 않아도 트위터나 페이스북 등의 SNS 계정으로 댓글을 달 수 있게 되었다. 이 때문에 네이버 영화 평점 조작처럼 추천수 조작하기도 쉬워졌다. 네이버, 미투데이, 트위터, 페이스북, 다음으로 한 번씩만 로그인해도 공감 및 비공감 5개를 줄 수 있다. 2016년 10월, JTBC에서 최순실의 태블릿 pc를 발견하였고 최순실 게이트 사건의 포문이 열리기 시작했다. (트래픽 측면에서 네이버 뉴스는 최순실에게 감사하는 마음이 없지 않아 있을 것이다...) 그리고 동시에, 드루킹의 댓글 조작 사건도 시작[3]되었다. 2018년 5월 말, 특검법이 통과된 이후에 댓글이 줄었다는 기사[4] 가 보도되었다. 2018년 6월 이후의 댓글이 있었다면 그간 댓글부대의 위력이 어느 정도였는지 가늠해볼 수 있었을 것이다. 결론적으로 2016년 후반부 이후 폭발적인 댓글 수의 증가는 정치 및 사회 영역의 엄청난 트래픽 덕분이었을 것이다. 가설 검증 차원에서 섹션 별로 나누어 같은 방식으로 댓글을 집계해 보았다. News Sentiment Analysis 네이버 뉴스는 기사에 \"좋아요\" 를 시작으로 \"훈훈해요\", \"슬퍼요\", \"화나요\", \"후속기사 원해요\" 의 label을 달 수 있게 만들었다. - \"좋아요\": 2014년 초 시작 - \"훈훈해요\", \"슬퍼요\", \"화나요\", \"후속기사 원해요\": 2017년 초 시작 \"좋아요\" 만 있을 때와 다섯 가지의 감정이 있을 때의 추이가 또 재밌다. \"좋아요\" 외의 다른 감정이 허가된 순간 이후로 \"화나요\" 가 급격히 증가한다. 정치 참고: 사드배치 (2017.03), 문재인 당선 (2017.05)[5], 평창 동계 올림픽 (2018.02), 이명박 수감 (2018.03)[6] 경제 사회 문화 2018년 2월에는 무슨 일이.. (추운 날씨, 성추행 등의 사건 때문으로 추측됨) IT 세계 Conclusions 여기까지는 기초적인 데이터 탐색 작업이었다. 간단히 시간 순으로 댓글 수를 집계하기만 해도 재미있는 분석 결과를 얻을 수 있었다. (가령, 박근혜-최순실 게이트가 얼마나 큰 이슈였는지, 뉴스는 대부분 우리를 열받게 하는 내용이라든지 등) 이 다음 분석은, 의심하기만 했던 댓글 어뷰저 집단이 실제로 존재하는지에 대해 다룰 예정이다. 마침 댓글 수집 기간과 드루킹의 댓글 조작 기간이 맞물려 있어서 분석해 볼 수 있는 데이터가 손에 쥐어졌다. 최대한 선입견없이 담백한 분석을 해보려고 노력했다. 정말인지 아닌지 다음 글에서 확인해보자. References 1.https://namu.wiki/w/네이버_뉴스 ↩︎ 2.https://www.wikitree.co.kr/main/news_view.php?id=71675 ↩︎ 3.https://www.mk.co.kr/news/society/view/2018/05/294952/ ↩︎ 4.https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001 ↩︎ 5.https://ko.wikipedia.org/wiki/2017년_대한민국 ↩︎ 6.https://ko.wikipedia.org/wiki/2018년_대한민국 ↩︎","link":"/2019/07/25/Naver-News-Comments-Analysis-(1)/"},{"title":"Machine Learning Yearning 요약: Ch.5~12","text":"이번에 요약할 Chapter 5~12 의 소제목은 Setting up development and test sets 이고, 아래와 같은 제목으로 구성되어 있다. Ch.5: Your development and test sets Ch.6: Your dev and test sets should come from the same distribution Ch.7: How large do the dev/test sets need to be? Ch.8: Establish a sinlge-number evaluation metric for your team to optimize Ch.9: Optimizing and satisficing metrics Ch.10: Having a dev set and metric speeds up iterations Ch.11: When to change dev/test sets and metrics Ch.12: Takeaways: Setting up development and test sets Machine Learning / Deep Learning 모델을 전략적으로 향상시키기 위해서는 Idea를 내고, Code로 구현하고, 검증하고, 재학습시키는 일련의 iteration이 정확하고 빠르게 진행되어야 한다. 코드를 통해 구현된 모델을 실험한 후, 올바른 방향으로의 재학습이 이루어지기 위해서는 error analysis가 필수적이다. 신뢰도 있는 Error analysis는 신뢰도 있는 dev/test set과 evaluation metric을 통해 얻어진다. 그래서 이번 Chapter는 좋은 dev/test set 그리고 evalutaion metric에 대한 조건에 대해 서술하고 있다. Dev/Test set 먼저, training set, dev set, test set에 대해 정의해보자. Training set: 우리가 알고리즘을 학습할 때 사용하는 데이터 Dev set: 알고리즘의 parameter를 튜닝하고, feature를 선택하는 등의 결정을 내리기 위한 데이터 Test set: 알고리즘의 성능을 평가하기 위한 데이터 즉, 모델이 잘하고 있는지 아닌지의 여부는 test set에 달려있다. 그리고 “잘한다”라는 기준은 서비스에 대해서는 당연히 “사용자의 만족도”이다. 그러므로 test set은 “사용자의 만족도를 향상시키기 위해 세운 팀의 목적”과 부합해야 하며, “서비스가 잘 하고 싶은 영역”과 “서비스 사용자를 통해 들어올 법한 데이터”를 반영해야 한다. 그리고 dev set은 반드시 test set과 같은 distribution을 가져야 한다. 왜냐하면 모델의 세부사항은 dev set을 통해 결정되기 때문에 만약 distribution이 다르다면 “죽 쒀서 개 준 꼴”이 날 것이다. 뿐만 아니라 추후 dev set에서 error analysis를 통한 모델의 진단이 어려워진다. 적절한 Dev set의 크기는 학습한 모델들의 차이를 인지할 수 있을 정도이어야 한다. 보통 1,000에서 10,000개 정도가 적당한데, 이미 기본적인 모델의 성능이 높고, 0.01%의 차이에 민감해야 한다면 10,000개보다 커야 한다. Test set의 경우에는 모델의 성능에 대해 높은 confidence를 줄 수 있을 정도의 사이즈가 적당하다. 교과서에서는 전체 데이터의 30%를 쓰라고 하지만, 빅데이터 시대에는 그보다 비율이 작아도 된다. Evaluation metric 2개 이상의 metric을 쓰는 것보다 단일 metric을 쓰는 것이 좋다. A와 B의 측면에서 평가해야하는데 A를 버리라는 뜻이 아니다. A와 B를 적절히 섞은 (평균, 가중 평균 등) 단일 수치를 만들라는 것이다. 단일 metric이어야 빠르게 모델의 성능을 비교하고 결정할 수 있다. 그러나 A와 B와 같은 metric을 적절히 섞기 어려운 경우가 있을 수 있다. 아래 그림의 정확도와 running time처럼 (Accuracy - 0.5*RunningTime와 같은 단일 metric이 부자연스럽다). 이럴 땐, satisficing metric과 optimizing metric을 나누어서 생각하면 된다. 총 N개의 metric이 존재할 때, N-1 개의 satisficing metric과 1개의 optimizing metric으로 나눈다. 위의 예제의 경우, Running time은 satisficing metric으로, Accuracy는 optimizing metric으로 두었을 때, Running time은 특정 기준을 넘기기만 하면 되고, Accuracy는 그 안에서 최대인 모델이 가장 좋은 모델이라고 평가할 수 있다. Dev/Test set 및 evaluation metric의 업데이트 위의 조건을 만족하는 dev/test set과 evaluation metric을 결정하는 것은 쉽지 않을 것이다. 하지만 완벽하진 않더라도 빠르게 무엇이라도 구축하고, 업데이트하는 것이 좋다. dev/test set이 더이상 실제 사용자의 distribution을 대표하지 않을 때, dev set에 너무 overfit 되어버렸을 때, metric이 팀의 방향을 대변하지 못할 때, 과감히 변화를 시도하자.","link":"/2018/12/26/Machine-Learning-Yearning-summary-Ch.5~12/"},{"title":"Naver News Comment Analysis (2)","text":"NOTICE: 앞으로 소개될 내용은 NAVER와 무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은 중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을 알립니다. TL;DR 2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를 확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다. 타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다. 발생하기 어려운 패턴을 보여야 한다. 이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서 369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고 269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다. Abuser, who are you? Introduction \"정말 2016년 4분기부터 정말 댓글 조작을 했던 사용자들이 있었을까?\" 라는 단순한 의문과 궁금증에서 분석을 시작하게 되었다. 다만, 데이터에 을 눌렀던 interaction 정보가 누락되어 있기에 (이 데이터는 네이버 뉴스 측에서 제공해주지 않는 이상 얻을 수 없다) 적어도 댓글을 한 번이라도 남겼던 사용자에 대해서만 어뷰저로 의심해 볼 수 있었다. label이 없는 상황에서 어뷰저를 특정짓는 것과 그 사용자가 어뷰저임을 다른 사람에게 설득하는 것은 어려운 일이다. 또한 무죄추정의 원칙에 의거해 댓글 작성자를 함부로 어뷰저라고 단정지을 수도 없었다. 그래서 이번 분석에서는, \"모든 작성자는 어뷰저가 아니다.\" 라는 가정을 기반으로 특정 패턴이 등장할 확률을 계산해서 어뷰저였을 가능성을 간접적으로 추측하는 방식을 취했다. 이 과정에서 누군가는 그 정도 확률로는 어뷰저라고 단정짓기 어렵다고 판단할 수도 있고, 아닐 수도 있다. 또 추가적인 분석 결과가 있다면 어뷰저 가능성이 더 높아질 수도 있다. 후자라면 언제든 댓글로 추가 분석할 내용을 요청했으면 하는 바람이다. Abuser Criteria 어뷰저는 어떤 존재일까? 이에 답하기 앞서, 어뷰징의 목적과 어뷰징이 문제가 되는 상황에 대해 먼저 정리해보았다. 어뷰징의 목적 어뷰저들의 목표는 네이버의 댓글 정렬 기준에 맞추어 10위권 내에 드는 것이다. 네이버 뉴스의 UI 상, top 10 내에 들면 그 기사를 읽는 누구나 쉽게 그 댓글의 내용에 접하게 되기 때문이다. 그리고 그 내용이 대중을 대표한다고 생각하기 때문에 쉽게 타인의 생각에 영향을 미칠 수 있다. 어뷰징이 문제가 되는 상황 어뷰징이 문제가 되었던 이유는 공정하고 자연스러운 방식으로 집계되었다고 믿었던 top 10 댓글이 실제로는 어떤 세력에 의해 의도를 가지고 조작되었기 때문이었다. top 댓글이 특정 집단에 의해 조작되었다면, 그것들이 과연 네이버 뉴스 플랫폼에 참여하는 사용자들의 생각을 대표하는 댓글이라고 볼 수 있을까? 그래서, 이 글에서 이야기 할 어뷰저의 criteria는 다음과 같다. 어뷰징의 목적을 달성해야 한다. 즉, 타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다. (안습) 자연적으로 발생하기 어려운, 확률이 낮은 패턴이 등장해야 한다. Data preprocessing 에서 사용했던 데이터에서 추가로 필터링이 필요했다. 크롤링한 댓글 데이터에 hashing 된 아이디가 포함된 것이 2015년 12월 이후였기 때문이다. 사용한 댓글 데이터 기간: 2015.12.08 ~ 2018.05.25 Analysis 먼저, 정치 분야에서 댓글이 top 10 내에 들었던 횟수를 작성자 별로 집계한 후, 횟수가 높은 순서대로 정렬하였을 때의 추이를 살펴보았다. mean stdev max 75% med min 2.240442 4.607621 369 2 1 1 대부분의 작성자는 1~2번 정도 댓글이 top 10 내에 드는 반면, 일부 사용자들은 100번 이상 순위권 내에 든다. 이 그래프만 본다면 자주 top 10에 드는 사용자들 모두가 의심스러울 수 있지만 이런 skewed graph는 대부분의 사회과학 데이터에서 발견되므로 이들을 어뷰저로 속단하긴 이르다. 검증을 위해 다른 섹션(사회, 경제, 문화, IT, 세계)에 대해서도 마찬가지 방법으로 그래프를 그려보았다. 갓 파레토... 자주 순위권 내에 드는 댓글을 작성한 사용자를 top user 라고 했을 때, 다른 분야에서도 top user는 쉽게 찾아볼 수 있었다. 어쩌면 이들은 (어뷰저가 아닌 이상) 네이버 뉴스 플랫폼에서 높은 \"공감수-비공감수\"를 받을 수 있는 전략이 학습된 것은 아닐까? 기사가 나오고 얼마 지나지 않아 댓글을 남기거나, 그 당시 분위기에 맞는 댓글의 내용을 남기거나, 사실로 보여지는 데이터와 함께 댓글을 작성하거나 하는 등 자신만의 전략이 있을 것이다. 그러나 이 전략들이 100%의 확률로(=항상) 통하지는 않았을 것이다. 때로는 일찍 댓글을 작성했음에도 뒤늦게 작성한 댓글이 폭발적인 공감을 이끌어내서 top 10에 들지 못했을 수도 있고, 당시의 전반적인 분위기에 탑승하는 댓글을 남겼음에도 다른 댓글 중에 두드러지지 못해 공감을 받지 못했을 수도 있다. top user 간에 일반적인 top 10 성공률이 존재할 것이고 이는 normal distribution을 따른다는 가설을 바탕으로 \"top user가 작성한 전체 댓글 수 대비 top 10에 들었던 댓글 수(=top 10 성공률)\"를 계산해보았다. 정치 top users userId top comment # total comment # top 10 성공률 (%) user 1 369 386 95.60 user 2 339 380 89.21 user 3 269 289 93.08 user 4 178 610 29.18 user 5 178 1090 16.33 user 6 175 424 41.27 user 7 174 818 21.27 user 8 155 316 49.05 user 9 143 950 15.05 user 10 141 583 24.19 경제 top users userId top comment # total comment # top 10 성공률 (%) user 11 289 1185 24.39 user 12 226 2935 7.70 user 13 219 1656 13.22 user 14 183 1636 11.19 user 15 173 1378 12.55 user 16 161 989 16.28 user 17 160 654 24.46 user 18 157 2589 6.06 user 19 139 1514 9.18 user 20 127 742 17.12 사회 top users userId top comment # total comment # top 10 성공률 (%) user 21 366 953 38.41 user 22 308 1636 18.83 user 23 271 935 28.98 user 24 241 1254 19.22 user 25 233 1656 14.07 user 26 204 328 62.20 user 27 191 719 26.56 user 28 168 625 26.88 user 29 149 1190 12.52 user 30 148 1489 9.94 문화 top users userId top comment # total comment # top 10 성공률 (%) user 31 373 1636 22.80 user 32 367 935 39.25 user 33 301 1417 21.24 user 34 243 890 27.30 user 35 220 1656 13.29 user 36 188 1943 9.68 user 37 178 3245 5.49 user 38 172 2738 6.28 user 39 164 200 82.00 user 40 151 719 21.00 IT top users userId top comment # total comment # top 10 성공률 (%) user 41 714 3123 22.86 user 42 572 3886 14.72 user 43 442 2287 19.33 user 44 399 1468 27.18 user 45 231 810 28.52 user 46 234 3010 7.77 user 47 275 1622 16.95 user 48 317 1493 21.23 user 49 346 2349 14.73 user 50 364 1185 30.72 세계 top users userId top comment # total comment # top 10 성공률 (%) user 51 237 1636 14.49 user 52 214 1432 14.94 user 53 145 615 23.58 user 54 148 1709 8.66 user 55 155 611 25.37 user 56 156 1076 14.50 user 57 162 864 18.75 user 58 165 2778 5.94 user 59 165 1254 13.16 user 60 175 575 30.43 top user의 top 10 성공률을 확률 변수 X라고 했을 때의 histogram과 모든 유저가 전략을 바탕으로 활동하는 그룹이라고 가정했을 때 Gaussian distribution으로 추정한 확률 분포이다. (Gaussian Mixture Model로 distribution fitting한 결과는 Appendix A 참고) 정치 섹션에서만 유일하게 ratio &gt; 90% 인 top user(user 1, user 3)가 존재했으며 이들의 top 10 전략 성공률은 다른 top user 대비 발생하기 어려울 정도로 (0.0053%, 0.0079%) 높다고 해석할 수 있다. 숫자 이면의 패턴을 보기 위해 전체 정치면 기사들의 댓글 수와 user 1, user 3의 전체 댓글 수, top 10 내에 든 댓글 수를 그래프로 시각화 해보았다. c.f. 2016.3 ~ 2018.5 까지 굵직한 이슈들[1] [2] [3] 이세돌 vs. 알파고 (2016.3) 옥시 (2016.4) 최순실 태블릿 pc (2016.10) 박근혜 탄핵 소추안 (2016.12) 사드배치 / 박근혜 수감 / 세월호 인양(2017.3) 19대 대통령 선거 (문재인 당선) (2017.5) 이대목동 신생아 사망 (2017.12) 평창 동계 올림픽 (2018.2) 이명박 수감 (2018.3) 드루킹 (2018.4) 남북1차정상회담 @판문점 (2018.4) 남북2차정상회담 (2018.5) user 1 (ratio: 96%) 회색 line이 정치면 기사 댓글, 파란색 line이 작성자가 쓴 전체 댓글 수, 초록색 line이 작성자가 쓴 댓글 중 top 10 내에 들었던 댓글 수를 나타낸다. user 1이 주로 활동했던 시기는, 최순실 태블릿 pc 사건, 박근혜 탄핵 및 19대 대통령 선거, 평창 동계올림픽 및 MB 다스 사건과 맞물려 있었다. 댓글 내용을 시기 별로 뜯어보면, 다음과 같다. title article date user top comments 朴대통령, '29일까지 대면조사' 檢 요청에 사흘째 묵묵부답 2016-11-25 15:12:00 대통령인 자가 자신의 관할하에 있는 검찰을 부정한다면 곧 국가도 부정하겠다는 의미다. 이런 대통령은 더이상 대한민국 대통령이 아니다. 차은택 변호인 \"차씨, 최순실 지시로 김기춘 실장 공관서 면담\" 2016-11-27 16:04:00 김기춘의 진두지휘하에 박근혜 정권의 모든 불법들이 자행되었다. 정말 악마같은 인간이다. 변호인 \"차은택, 崔 지시로 김기춘 만나…우병우 장모와 골프도\"(종합) 2016-11-27 16:46:00 김기춘의 진두지휘하에 박근혜 정권의 불법들이 자행되었다. 구속수사해서 감옥에서 못나오게 만들어야 한다. 공작정치부터 공안탄압 정경유착의 죄를 물어야 한다. 청문회장도 지배한 '촛불민심'…與野 '재벌 봐주기' 없었다 2016-12-06 12:34:00 뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까... 강요죄는 확실할듯... 재벌 총수들 \"청와대 거절 어려워\"…하나같이 대가성 부인 2016-12-06 12:45:00 뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까... 강요죄는 확실할듯... 박한철 前소장 한표, '캐스팅보트' 될뻔한 아슬아슬 상황 나올까 2017-03-04 08:00:00 탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다. 85시간 재판, 속기록 3000쪽…탄핵심판 이번주 결론날까 2017-03-05 09:00:00 탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다. [리얼미터] 다자 文 42.6% vs 安 37.2%…양자 文 47.6% vs 安 43.3% 2017-04-10 09:15:00 여론몰이에 흔들릴 국면이 아니다. 무쏘의 뿔처럼 나아가면 야합은 흩어지고 굳건함이 승리할 것이다. 文 \"김부겸 동지 미안하다…꼭 국민통합 해내겠다\" 2017-04-22 08:01:00 김부겸의 진심이 느껴지고 그를 위로하고 뜻을 같이 하는 문재인의 진심도 느껴진다. 남자들에게 이런 동지애는 죽음도 불사하게 만드는 마력과도 깉다. 조~오타!!! 文 대통령 \"내게 반대하라\" 파격적 수석회의 시동(상보) 2017-05-25 12:56:00 요새 대통령의 행동과 지시사항을 보면 정말 준비된 겸손한 사람이란게 진솔하게 느껴진다. 대한민국 국민인게 자랑스럽고 행복해진다. 文대통령 \"사드 임시배치, 현재 정부가 취할 수 있는 최선의 조치\"(종합) 2017-09-08 21:13:00 국가의 지도자는 자신의 굳은 신념까지도 국가와 국민을 위해 잠시 접어야할 용기가 필요할 때가있다. 그 지도자라고 왜 자신의 신념을 꺾음에 자괴감과 고민이 없겠는가? 그는 자신을 지지하는 사람들만의 지도자가 아니라 대한민국의 지도자이기 때문이다. 그의 고뇌찬 결단을 위로하며 지켜보고 힘을 실어주고 싶다. 與 \"안철수, 나라다운 나라 만드는 일 폄훼 말라\" 2017-11-04 16:10:00 명버기 구하기에 혈안이 된 명바기 아바타!!! 김정은 위원장 \"이른 시일내 만날 용의\"…문 대통령에 방북 요청(종합) 2018-02-10 15:56:00 남북 정상회담애서 허심탄회하게 모든 할 말 다해서 기필코 한반도 비핵화와 평화를 이루어야 한다. [현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표 2018-03-09 09:11:00 한반도 평화가 세계 평화다. 이런 평화 모드가 얼마만인가... 같은 내용의 댓글이 top 에 오른 것도 확인할 수 있었다. 다른 섹션에 같은 댓글을 남기고도 top에 오른 적도 있다. (아래 표 참고) title section article date top user comments [현장영상] 박근혜 前 대통령 법원으로 출발 society 2017-03-30 10:18:00 최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다. 구속 갈림길에 선 박근혜 '웅변 대신 침묵' 선택 politics 2017-03-30 10:22:00 최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다. user 3 (ratio: 93%) user 3 의 주 활동 시기는 사드배치, 평창 동계올림픽 및 북미회담과 맞물려있었다. 댓글을 자세히 보면 아래와 같다. title article date user top comments 정상회담 돌발 변수는 '사드'…청 \"모든 가능성 준비\" 2017-06-25 20:20:00 국익과국가안보가최우선입니다~~~~ 송영무 \"사드, 비준 아닌 국회 검증…고액연봉·음주운전 송구\"(종합) 2017-06-28 12:18:00 이유미녹취록에맛짱구치고놀아난언론은× 이유미-이준서 중 한 명은 거짓말…윗선 수사 불가피 2017-06-28 20:52:00 이유미녹취록에맛장구치고놀아난언론은~~~~???? 軍, 송영무 인사청문회서 공개된 '군사기밀 유출' 조사 착수 2017-06-29 12:24:00 자유당놈들답다도둑놈들 文대통령, 내일 트럼프와 만난다…취임 후 첫 韓美정상회담 2017-06-29 13:54:00 국익과국가안보가최우선입니다부디좋은결과있으시길~ [단독] '제보 조작' 수사망 좁혀오자 安 독대한 이준서…왜? 2017-06-29 20:19:00 철수야~ 깜빵갈시간이다가오네~~~~ 트럼프, 文대통령 부부에 백악관 사적공간 '트리티 룸' 깜짝공개(종합) 2017-06-30 13:23:00 문재인대통령님~ 멋저부러요~♡♡♡ 한반도 이슈서 '주도권' 확보 성과…한미FTA 재협상 '숙제'(종합) 2017-07-01 10:05:00 국익과국가안보가최우선입니다부디좋은결과있으시길~~~~~~~~♡♡ 남북 \"4월말 정상회담 판문점서 개최\"…특사단 발표(종합) 2018-03-06 20:24:00 이게 실화나ㅡ ㅡㅡㅡㅡ 문 대통령 \"서울·평양·판문점 중 北이 판문점 정상회담 선택\" 2018-03-07 15:28:00 문통 지지 합니다 문 대통령 \"국외 대북 비밀접촉 없어…저쪽에 놀아나는 것 아냐\" 2018-03-07 16:53:00 문 대통 령님 지지 합니다 文대통령 \"북핵목표는 비핵화…제재완화, 지금은 불가능\"(종합) 2018-03-07 17:16:00 문대통령님 지지합니다 [현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표 2018-03-09 09:11:00 이게 실화냐ㅡㅡㅡㅡ Conclusions 어뷰저를 타인의 생각에 영향을 미치고 비정상적인 행태를 보이는 사용자로 정의하였고, 이 기준에 따라 어뷰저로 의심되는 사용자를 찾아내고자 하였다. 순공감 기준, 댓글이 10위권 내에 들었던 횟수가 많았던 작성자 중에서 작성한 댓글 수 대비 top 10 댓글 수의 비율이 일반적이지 않은 작성자 개인적으로, 1.과 2.의 기준에 드는 사용자는 user 1, user 3 라는 생각이다. 작성한 전체 댓글 수는 다른 사용자들에 비해 적은 편이었지만 top 댓글에 들었던 비율은 가장 높았고, 그 수치가 일반적이지는 않았다. 어뷰저를 찾고자 시작한 분석이었지만 데이터를 살펴보면서 네이버 뉴스 댓글이 가지는 단일하고 공개된 ranking system이 얼마나 위험한지를 오히려 인식하게 되었다. 분석한 기간에서 중복 제거한 기사의 수는 총 100,780개 였고, 만약 top 10 댓글의 작성자가 모두 다른 사용자였다면 1,007,800명이 각자의 의견을 개시했을 것이다. 하지만 실제 그 기간에 집계된 unique한 작성자는 총 308,731 명에 불과했다. 게다가 중복 댓글까지 포함하면, 그 다양성은 조금 더 떨어진다. 이 같은 면에서 네이버 뉴스 댓글은 다양성을 충분히 수용하고 있지 못하다는 생각이 들었다. \"플랫폼이기 때문에 그럴 수 있지 않을까?\" 싶지만 페이스북이나 유튜브, 레딧같은 다른 플랫폼에서의 댓글을 보면 무작정 호감순으로 정렬하지는 않는다. 이 플랫폼들의 기준이 문제가 없다는 것은 아니다. 하지만 네이버 뉴스 보다 다양한 기준으로 댓글을 정렬시키고 있으며 (최신순, 오래된 순, 공감을 많이 받은 순, relevance 순, 호감 + vote의 크기 등) 이를 통해 다양한 의견이 쉽게 노출될 수 있는 환경을 조성하였다는 점에서는 좀 더 높은 점수를 주고 싶다. 그래서 세 번째 글은, 간단하지만 다른 정렬 기준을 적용했을 때 발견되는 새로운 댓글에 대해서 다뤄 볼 예정이다. 프로젝트 초창기에, 같이 작업을 진행했던 재명님이 돌려본 결과가 있는데 이 것도 조금 다듬어서 올리면 재밌을 것 같다! 3탄은... 휴가(@Norway) 다녀오고 나서 작업해볼까 싶다. To be continued... Appendix A: Gaussian Mixture Model Fitting GMM의 n_components 최적 개수를 구하기 위해 silhouette score를 계산하였다. score가 가장 높은 n_components=2 이므로 2개의 gaussian을 가정하여 fitting 해보면 아래 그림과 같다. userId cluster 1 prob cluster 2 prob ratio (%) user 1 2.27E-16 1 95.60 user 3 2.56E-15 1 93.08 user 2 9.31E-14 1 89.21 user 39 4.96E-11 1 82.00 user 6 8.11E-01 0.188898 41.27 user 9 1.00E+00 0.000219 15.05 References 1.https://ko.wikipedia.org/wiki/2016년_대한민국 ↩︎ 2.https://ko.wikipedia.org/wiki/2017년_대한민국 ↩︎ 3.https://ko.wikipedia.org/wiki/2018년_대한민국 ↩︎","link":"/2019/08/03/Naver-News-Comments-Analysis-(2)/"},{"title":"Naver News Comment Analysis (3)","text":"TL;DR 어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은 불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은 내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까? 네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을 상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연 정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할 의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱 획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어 무의식적으로 다양한 사고에 대한 가능성을 차단받는다. 그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sorting algorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을 비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고 비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다. Introduction 모두가 다 알고 있는 사실이지만, 어뷰저는 존재한다. 드루킹과 에서 나온 결론으로도 뒷받침될 수 있지만 트위터에 m.news.naver.com/comment 라고 검색하기만 해도 아래와 같이 댓글 조작의 흔적을 쉽게 발견할 수 있다. 이렇듯 쉽게 어뷰저의 존재를 찾을 수 있음에도 네이버가 어뷰저를 잡지 않는 이유는 그 일이 생각처럼 쉬운 일이 아니기 때문이다. n초 안에 여러번 공감과 비공감을 지속적으로 받은 댓글은 어뷰징의 결과로 의심한다. 그 댓글을 지워야 할까? 만약 댓글을 쓴 유저가 어뷰저가 아니었다면 문제가 될 수 있다. 사후 분석을 통해 어뷰저로 의심되는 댓글의 내용을 지우는 방법은 어떨까? 뉴스라는 매체의 특성 상 시간이 지난 기사는 사람들이 관심있게 보지 않는다. 그러므로 이 방법은 어뷰저를 막는다고 볼 수 없다. 분석을 통해 어뷰저라고 강하게 의심되는 유저를 차단한다고 하더라도 새로운 패턴으로 어뷰징을 하는 유저들이 생겨날 것이다. 어뷰저의 기준을 세우는 것은 어려운 반면 새로운 방식으로 어뷰징을 하는 것은 좀 더 쉽기 때문에 이렇게 물고 물리는 싸움은 어뷰저에게 유리하다. 그렇다면 어뷰저를 차단하는 것에만 집중하지 말고, 어뷰징은 내버려두되 그 효과를 완화시키는 방법은 어떨까? 지금 네이버 뉴스 댓글 랭킹 방식은 그것이 미치는 영향력에 비해 너무 간단하고 단편적이다. 구글의 검색 랭킹이 신뢰도를 가지고 있는 이유는 상위에 랭크된 글이 '조작'을 통해 만들어지지 않았다는 점 때문일 것이다. 그 이유는 정보가 되는 글에 대한 정보량, 품질 기준이 보다 엄격하고 단편적인 면으로만 순위를 매기지 않기 때문이다. 만약 구글 랭킹이 웹문서의 클릭수로만 되어 있었다면 어땠을까? 많은 기업들이 본인의 홈페이지를 상위에 랭크시키기 위해 많은 조작이 일어났을 것이다. 그래서 이번 글에서는 그렇게 간단하다고는 볼 수 없는 다른 랭킹 algorithm에 대해 소개해보려고 한다. 현재 네이버 뉴스 댓글 랭킹 방식 중 순공감순, 공감비율순, 답글순의 한계점을 살펴보고 reddit과 yelp에서 신뢰도있게 쓰이는 best 랭킹과 새로운 관점의 controversial 랭킹 algorithm을 소개한다. Naver News Comment Sorting System Sorting Algorithms 2019년 9월 기준, 총 5개의 정렬방으로 서비스되고 있다. 드루킹 논란 이후 댓글 제공 여부와 정렬방식을 언론사가 선택하는 방식으로 바뀌었다. 순공감순: 공감 - 비공감[1] 공감비율순: 공감 / (공감 + 비공감) 답글순 최신순 과거순 이 중, 댓글에 대한 사용자의 인터랙션(공감, 비공감, 답글)으로 순위를 매기는 순공감순, 공감비율순, 답글순에 대한 문제점을 하나씩 짚어보고자 한다. Limitations 순공감순 순공감순은 우리의 직관과 벗어나는 랭킹이라는 점에서 한계가 있다. 우리는 절대적인 공감 수치보다, 공감비율로 댓글의 신뢰도를 평가한다. 아래의 사례는 네이버 뉴스 댓글[2]의 실제 예시이다. 첫번째 댓글은 순공감 344개(= 455 - 111) 로, 300개(= 316 - 16)의 순공감을 지니는 두번째 댓글보다 더 높은 순위에 자리한다. 하지만 각각의 댓글의 공감비율은 80.4%(= 455 / (455 + 11)) 로, 두번째 댓글의 공감비율인 95.2% (= 316 / (316 + 16)) 보다 작다. 공감비율순 앞서 설명한 것처럼 공감비율순이 좀 더 우리의 직관과 유사한 척도이다. 하지만 공감비율순은 전체 공감, 비공감 수가 적을 때 문제가 된다. 소수의 사람들에게만 노출된 댓글은 공감과 비공감의 개수가 모두 적어 100% 라는 공감비율이 쉽게 만들어지는 반면, 여러 명에게 노출된 댓글은 하나의 비공감만 달리더라도 그보다 낮은 공감비율을 지니게 되는 문제가 발생한다. 아래의 네이버 뉴스 댓글 예시[3]에서 공감수가 20, 비공감수가 0인 댓글이 비공감을 전혀 받지 않아 공감비율 100%가 되어 더 많은 사람들이 읽고 공감을 표한 공감수 1021, 비공감수 58인 댓글보다 더 상단에 위치한다. 답글순 여러 개의 답글이 달리는 댓글은 주로 일찍 남겨진 댓글 중에 인신공격이나 뉴스 외 주제에 대한 댓글인 경우가 많다. 댓글 공간에서는 명확한 내용으로 구성된 댓글에 대해서는 대댓글 보다도 공감 혹은 비공감으로 본인의 주장을 표시하는 것이 일반적이다. 그러나 감정적으로 쓰여진 댓글은 그 댓글에 자극을 받은 다른 사용자의 답글로 이어지고 되므로 답글 개수를 기준으로 댓글을 정렬하면 뉴스 내용과는 무관한 자극적인 댓글들이 우선적으로 노출된다. 또한 일찍 쓰여진 댓글일수록 더 많은 사람들에게 노출될 가능성이 있으므로 대부분 뉴스 작성 시점과 가까운 댓글이 상위에 랭크된다. 랭킹 algorithm으로 보기에는 정렬 기준이 controllable하지 않으며 댓글의 유익한 속성이 높게 평가되어 정렬되는 랭킹이라고 볼 수 없다. 아래의 네이버 뉴스 댓글 예시[4]를 보면 vote 수가 많지 않아도, 공감수가 전혀 없고 비공감만 받더라도 top 10에 위치할 수 있다. Reddit Comment Sorting Algorithms 댓글이 활발하게 생성되는 플랫폼은 비단 네이버 뉴스 뿐만은 아니다. 네이버 쇼핑, 네이버 호텔, 망고 플레이트, reddit, stackoverflow, yelp, amazon 등의 다양한 플랫폼에서 수집되며 플랫폼에서는 다시 이 데이터를 가공하여 사용자에게 유익한 정보를 제공한다. 그 중에서도 reddit의 랭킹 시스템이 앞서 비판했던 순공감순, 공감비율순의 한계를 극복한 sorting algorithm을 제공하고 있기에 자세히 살펴보려고 한다. reddit의 랭킹 방식에는 best, top, new, controversial, old, q&amp;a가 있다. top이 순공감순, new가 최신순, old가 과거순이다. Best Best ranking[5][6] 은 Wilson score[7]로 정렬한 것으로, 공감비율순의 단점으로 언급되었던, 전체 vote수가 적은 상황을 smoothing시켜준 algorithm이다. reddit뿐 아니라 yelp에서도 사용한다고 한다[8]. Wilson score는 주어진 positive와 negative vote가 binomial distribution을 따른다고 가정했을 때, positive 발생 확률을 95% 신뢰구간의 최소값으로 추정한 값이다. 동전 뒤집기 상황에서 앞면을 positive, 뒷면을 negative라고 하자. n번 던진 후 앞면이 나올 확률(\\(p\\))을 추정할 때 n이 충분히 큰 경우 central limit theorem에 의해 \\(p\\)는 normal distribution을 따른다. 따라서 95%의 신뢰도로 \\(p\\)를 추정하여 \\(p\\)의 최소값, 최대값을 구할 수 있고 이 때 최소값이 Wilson score가 된다. 자세한 수식은 Appendix A에 정리해두었다. \\[ w^- = max(0, \\frac{2n\\hat{p} + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)})=\\text{wilson score} \\] 위의 식을 함수로 구현하면 다음과 같다. import numpy as np# ref: http://www.evanmiller.org/how-not-to-sort-by-average-rating.htmldef best(up, down): try: z = 1.96 # 95% confidence level n = up + down p_up = up / n p_down = 1 - p_up denominator = 2 * (n + z**2) numerator = 2 * n * p_up + z**2 - z * np.sqrt(z**2 + 4 * n * p_up * p_down) lower = numerator / denominator except ZeroDivisionError as e: lower = 0 return max(0, lower) 아래의 예시는 네이버 뉴스 댓글에 Best ranking algorithm을 적용해본 결과이다. 공감비율순 정렬이었다면 \"원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요\"는 1000개 이상의 vote를 가진 \"법대로 해라 법은 만인 앞에 평등하다\"는 댓글을 제치고 상위에 랭크되었을 것이다. 하지만 Best 정렬방식에서는 vote 수가 적은 경우 약간의 penalty를 받기 때문에 하위에 랭크되었다. MB '정치보복' 반발에 문무일 총장 \"법적 절차대로 하겠다\"[9] comments 공감수 비공감수 best score 공감비율 법대로 해라 법은 만인 앞에 평등하다 1091 55 0.938 0.952006980803 법대로 하면 사형인데 !! 562 39 0.936 0.935108153078 제발 법대로만 해주세요. 그래도 나라를 지옥으로 만든 죄는 물을 법도 없다. 이 악마야!!! 252 14 0.933 0.947368421053 지금까지 반발하고 나서 살아남은 넘을 못봤다. 565 38 0.933 0.936981757877 혓바닥몇번 낼름거릴까나했더니 찔렸나보네ㅎㅎ 595 37 0.932 0.941455696203 본인이 구린짓을 했으니까 먼저 발광하는거겠지.. 686 43 0.931 0.941015089163 법대로 하는 것보다 더 정의로운 절차는 세상에 없다 4146 317 0.926 0.928971543805 당연히 법대로 하셔야죠 296 14 0.921 0.954838709677 원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요 302 13 0.921 0.95873015873 법대로 합시다 919 51 0.92 0.947422680412 기본적으로 공감수가 많은 댓글을 상위에 랭크시키는 알고리즘이기 때문에 어뷰징 작업으로 공감수가 부풀려진 댓글이 top 10 밖으로 밀려나지는 못한다. 하지만 vote수가 적더라도 경향성을 파악해 댓글을 정렬시키기 때문에 단순한 순공감이나 공감비율순으로는 하위권에 있던 댓글이 상위권에 위치할 기회를 증가시켰다. 어뷰저 입장에서는 쉽게 계산할 수 있는 정렬방식이 아니기 때문에 조작이 어려워질 것이다. 어뷰징을 할 때 고의로 공감과 비공감을 섞어서 해당 댓글을 상위에 랭크시키는데, Best 정렬이라면 \"적당\"한 비율을 맞추기 까다로워질 것이다. Controversial controversial[10]은 이름 그대로, 공감과 비공감이 팽팽하게 맞서는 댓글을 상위에 위치시키려는 알고리즘이다. 단순히 팽팽하기만 하면 공감과 비공감이 1:1인 상황과 10:10인 상황이 같다고 생각할 수 있기에 vote수도 sorting algorithm에 포함시켜서 10:10이 1:1인 상황보다 더 controversial할 수 있도록 만들어졌다. 아래의 식에서 upvote는 공감을, downvote는 비공감을 의미한다. upvote와 downvote의 차이가 같아서 분모가 같아진 경우에는 그 크기가 큰 쪽이 높고, vote의 크기가 같은 경우에는 차이가 작은 쪽이 높다. \\[ \\text{controversial} = \\frac{match \\times log(match + 1)}{| upvote - downvote | + 1},\\text{ where }match=min(upvote, downvote) \\] python으로 구현한 식이다. import mathdef controversial(upvote, downvote): match = min(upvote, downvote) top = match * math.log(match + 1) bottom = abs(upvote - downvote) + 1 return float(top) / bottom 좀 더 직관적인 이해를 돕기 위해 가공한 아래의 예시를 보자. upvote downvote controversial score 1001 1000 3454.38 999 1000 3450.42 100 100 461.52 101 100 230.76 1000 700 15.24 130 100 14.89 100 130 14.89 1 1 0.69 1 2 0.35 upvote, downvote의 비율이 비슷한 댓글 순서로 정렬되고, 그 비율 내에서는 vote 수가 큰 댓글이 더 위에 놓이게 된다. controversial algorithm을 네이버 뉴스 댓글에 적용해보았다. 예상대로 공감과 비공감 수치가 비슷하면서도 vote수가 많은 댓글이 가장 먼저 보인다. vote수가 작은 이유는 이미 순공감 노출로 인해 vote를 받을 기회를 박탈당한 댓글들이기 때문이다. 수치와는 무관하게 top 10 댓글의 내용은 얼마나 controversial하게 구성되어 있는지 정성적으로 평가해보았다. 보도자료에 대한 찬성은 푸른색, 반대는 붉은색 , 애매한 문장은 표기하지 않았다. controversial하다면 뉴스 기사의 주제에 대해 찬성과 반대가 골고루 섞여있어야 할 것이다. 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합)[11] userId comments 공감수 비공감수 user 1 대통령 각하, ‘사드 문제’ 갖고 거품무는 중국에도 내정 간섭이라고 거침 없이 말씀해주세요 26 26 user 2 이제는 한미일군사훈련을 해야 한다. 81 85 user 3 근데 왜 중국한테는 대놓고 내정간섭 받는거죠, 대통령님? 치욕스러웠던 조선시대가 그리운건가요? 22 22 user 4 봐라 ㅋㅋㅋ 연기하지?철수 얘기나온다 백퍼 ㅋㅋㅋㅋ 베트남꼴 나는거야 ㅋㅋㅋ 정신 좀 차리자 33 31 user 5 아베만도못한 문통; 11 11 user 6 문재인 아가라 닥쳐라. 사드도 내정문제인데 중국한테는 끽소리 못 하던 색히가 어디서 주둥아리 씨부리노. 10 10 user 7 ㅋㅋㅋㅋㅋㅋ 곧 양념단와서 또 평화올림픽 울부짖겠네. 66 57 user 8 미국이 한국을 버려야 할 듯.없네. 15 14 user 9 미국을 대변하는거다.국익을 최우선으로 하는거지싫지만 아베가 똑똑하지않는냐.살자. 8 8 user 10 얼마나 답답하면 저런말을 할지 생각 안해보셨나요?? 북에서 원하는 대로 흘러가네요. 앞으로 한미군사훈련 연기뿐만 아니라 축소되고 없어지고 난리나겠네 8 8 분명 공감수와 비공감수는 controversial하지만 대부분이 당시의 여론과 반대대는 내용으로 치우쳐있다. 정성적으로 controversial한 댓글은 공감: 비공감이 1:1이 아닌 좀더 공감 비율이 높은 비율을 가진다는 사실을 유추해볼 수 있다. 공감비율과 비슷하게 controversial도 vote수가 많은 경우에 불리해진다. controversial의 분모는 upvote와 downvote의 차이값인데, vote수가 많을수록 한두개차이를 유지하기가 어려워진다. 공감 66, 비공감 57을 가진 댓글이 공감 10, 비공감 10보다 아래에 놓인다. New Sorting Algorithms reddit ranking algorithm 중에서 controversial의 문제점을 해결한 새로운 controversial algorithm과 비공감이 많은 의견도 노출하는 best anti 정렬방식을 제안하고자 한다. New controversial 앞서 지적했듯이 controversial은 공감: 비공감의 비율 재조정과 vote 수가 많은 경우 분모값의 기준을 완화시켜야하는 이슈가 있다. 공감 : 비공감 정성적으로 확인해보았을 때 공감: 비공감 = 6.5 : 3.5 정도에서 기사 내용에 대한 찬성과 반대의 댓글이 골고루 등장하였다. 때문에 new controversial에서 upvote와 downvote의 값을 조정해주어야 한다. vote수가 많은 경우 이 문제는 공감비율순과 비슷했다. upvote와 downvote의 절대치에 의존하기보다 wilson score로 도출된 값을 upvote와 downvote로 대체하면 vote수가 많고 적음을 고려하면서도 0과 1 사이의 값을 가지게 되어 upvote와 downvote의 차이에 대한 효과가 완화된다. 변경된 내용을 정리하면 다음과 같다. import mathdef controversial(upvote, downvote): p_up = best(upvote, downvote) * 3.5 p_down = best(downvote, upvote) * 6.5 match = min(p_up, p_down) top = match * math.log(match + 1) bottom = abs(p_up - p_down) + 1 return float(top) / bottom 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합) [11] userId comments 공감수 비공감수 user 11 아베한테 대하듯 똑같이 김정은하고 북한, 중국한테도 당당하게 나와라! 16 11 user 12 개~~새끼 아베 한테는 그렇게 당당하면서 김정은한테는 왜 그렇게 꼬리를 내린다냐? 핵이 무섭긴 무서운가 보다 11 7 user 13 한미 동맹도 좋다 그러나 우리 나라 스스로 강한 나라가 되어야 한다. 문대통형 수고 많으십니다 !! 9 6 user 14 아베에게 일침을 놔주신문 대통령님 지지 합니다.나대지 마시오 9 6 user 15 쪽바리 추종자들 많네!! 특히 벌레 틀딱들~~ 8 5 user 16 반대로 우리나라가 일본보고 자위대 훈련하는거 보고 참견하면 일본이 가많이 있겠냐?벌레들아! 비판을 하려면 국내 내정에 간섭하는 아베를 비판해야지 아베를 두둔하냐? 이 스레기들아... 8 5 user 17 아베가 옳은말했네 지금이라고 김정은 참수 한미연합훈련을 시작하라 빨갱이한테 이 나라를 줄 수 없다 6 4 user 18 대한민국은 다시 한번 망해봐야 정신차리지..안된다. 6 4 user 19 일본이 우방이란애들 멍청한거 아니냐 일본애들도 그렇게 생각안하는데 왜 니혼자 망상해 찐따새끼인가ㅋㅋㅋㅋㅋㅋ 6 4 user 20 문재인씨 당신의 국적은 어디입니까? 다스 실소유주를 밝히는 것보다 훨씬 더 중요한 문제입니다. 6 4 공감 비율을 조금 높여주었을 때 기사 내용에 찬성하는 댓글과 반대하는 댓글이 top 10에 골고루 섞이게 되었다. 또 wilson score로 변환한 상태에서 비율을 조정해주게되어 vote수가 높은 경우에 up과 down의 차이에 덜 민감해질 수 있었다. Best-Anti 꼭 공감수가 많은 것만 괜찮은 의견이라고 볼 수 있을까? 비공감수가 많은 의견 또한 반대 진영의 입장을 대변하는 좋은 의견이라고도 볼 수 있지 않을까? 네이버 뉴스 댓글은 대부분 당시의 여론에 따라 분위기가 흘러간다. 순공감순이든 공감비율순이든 한가지 주장을 다른 방식으로 표현하고 있는 댓글들이 top 10이 된다. 이를 보는 대중은 한쪽의 영향만 받게 되어 생각이 더욱 치우쳐진다. 정치적 다양성을 수용하는 것은 의견의 객관성을 유지하는데에 도움이 된다. 그런 의미에서 당시 여론과 반대되는 내용의 댓글 또한 보여주는 것은 댓글에 영향을 받을 다른 사용자를 위해서도, 플랫폼의 중립성을 담보하기 위해서도 중요하다고 생각한다. Best-Anti는 negative vote에 대한 Wilson score를 구한 것이다. \\[ w_{neg}^- = max(0, \\frac{2n(1-\\hat{p}) + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)}) \\] python 구현식은 다음과 같다. import numpy as npdef best_anti(up, down): try: z = 1.96 # 95% confidence level n = up + down p_up = up / n p_down = 1 - p_up denominator = 2 * (n + z**2) numerator = 2 * n * p_down + z**2 - z * np.sqrt(z**2 + 4 * n * p_up * p_down) lower = numerator / denominator except ZeroDivisionError as e: lower = 0 return max(0, lower) 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합)[11] userId comments 공감수 비공감수 user 21 평화협정후 미군철수 바랍니다 0 5 user 22 홍발정씨..트럼프도 좌파 빨갱이죠?? 0 4 user 23 늙다리 미치광이는 빠져 줄래!자주통일좀 하자! 0 4 user 24 자국당은 사형감 많던데... 미국철수 애기했다고 파면? 자국당 5월에는 문정인으로 놀고먹겠군~! 0 4 user 25 봐라. 지도자 하나가 이렇게나 세상을 바꿀 수 있다. 물론 촛불 들고, 직접민주주의를 구현한 국민 또한 위대하지. 지방선거 때 투표 잘 하자. 0 4 user 26 아직도. 미국이 인계철선이라믿고 50년대 사고방식이 존재하는구나 군사력 세계10위안에들고 1-1붙어도 안지니 너무 미군철수로 여론전말고 참신한거없어요 ? 자한당분들? 1 6 user 27 극우 자한당은 미국도 빨갱이란다 제비가 왔다고 봄은 아니람서 ㅋㅋㅋ 0 3 user 28 원샷-빅딜! 0 3 user 29 자한당분들께서 트럼프도 좌파래요.. 0 3 user 30 잊지마세요 지금도 북한은 세계 최악의 인권유린 국가입니다 이시간에도 북한 주민들은 김정은한테 총살당하거나 아오지탄광으로 끌려가고 있습니다 북한 여성들은 김정은의 성노예가 되고 있구요 대한한공 조현민의 갑질 화가나죠 미투운동으로 드러난 권력자들의 성폭력 정말 싫습니다 그런데 이것보다 수백배는 더심한 갑질과 성폭력을 일삼는게 북한 김정은입니다 0 3 Conclusions 현재의 네이버 뉴스 댓글 정렬방식은 공감수가 높은 댓글을 위주로 보여주고 있고, 기준 또한 쉽다. 조작에 들어가는 비용 대비 얻을 수 있는 효과가 큰 상황에서 조작으로 인해 이익을 볼 집단은 당연히 어뷰징을 할 수 밖에 없다. 그리고 이미 조직적인 세력이 되어버린 어뷰저들은 완벽히 차단할 수 없다. 때문에 어뷰징을 해결할 수 있는 가장 좋은 방법은 현재의 정렬 방식의 단점을 극복하면서 자연스럽게 기준이 복잡해지게 만드는 것과 사람들이 조작된 의견에 크게 흔들리지 않을 수 있도록 다양한 의견을 보여주는 것이다. 현재의 네이버 정렬 방식 중 순공감순과 공감비율순이 가지는 한계는 reddit에서 사용하고 있는 best 정렬방식으로 해결된다. 공감수에 가중치를 둔 정렬방식 외에 공감수와 비공감수가 비슷한 댓글에 가중치를 두는 방식, 비공감수에 가중치를 두는 방식을 제안하였다. 한 쪽의 의견만 듣는 것은 언제나 편향된 결과를 야기한다고 생각한다. 한 쪽이 명백히 잘못한 것처럼 보도될 때, 그 반대의 의견에도 귀를 기울일 수 있는 플랫폼이 되길 바란다. Future works 지금까지는 댓글의 contents보다는 댓글에 부과된 공감, 비공감의 interaction 데이터로 문제점과 해결방식을 제안했다. controversial로 의견의 다양성을 추구했지만 text를 보지 않았기 때문에 의견의 다양성을 간접적으로 보장하기엔 불안정할 수 있다. 쇼핑 리뷰에서 가격, 내구성, 디자인 등 다양한 측면을 보여주듯이 정치적 의견도 기사에서 언급된 중요한 단어들에 대한 사람들의 반응을 보는 방식도 생각해보면 좋을 것 같다. Appendix A: Wilson score 사실 본문에서 기술한 내용은 일반적인 Normal approximation interval이다. \\[ p = \\hat{p} \\pm z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] 여기서 \\(\\)은 Bernoulli process의 성공확률을 의미한다. Wilson score는 confidence interval을 \\(\\)가 아닌 \\(p\\)로 추정한 score interval의 최소값이다. \\[ p = \\hat{p} \\pm z\\sqrt{\\frac{p(1-p)}{n}} \\] \\(p\\)에 대해 정리하여 \\(p\\)에 대한 2차방정식을 만든다. \\[ (1 + \\frac{z^2}{n}) p^2 - (2\\hat{p} + \\frac{z^2}{n})p + \\hat{p}^2 = 0 \\] 근의 공식을 사용해 \\(p\\)를 구한다. \\[ p = \\frac{2n\\hat{p} + z^2 \\pm z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)} \\] Wilson score는 \\(p\\)의 lower bound이므로 - 에 대해 정리하면 다음과 같다. \\[ w^- = max(0, \\frac{2n\\hat{p} + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)}) = \\text{wilson score} \\] 95%의 신뢰도로 고정하는 경우 z에 1.96을 대입할 수 있다. 그리고 이 경우 본문의 python 함수에서 구현한 best가 된다. References 1.2017년 11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의 호감순처럼 호감도를 “공감-비공감”으로 계산하게 되었다. ↩︎ 2.홍준표 “나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버 뉴스 ↩︎ 3.재판에 넘겨진 조국 부인 정경심 교수…검찰 '소환 임박', 네이버 뉴스 ↩︎ 4.대학교수 이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버 뉴스 ↩︎ 5.https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system ↩︎ 6.http://www.evanmiller.org/how-not-to-sort-by-average-rating.html ↩︎ 7.https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval ↩︎ 8.https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is ↩︎ 9.MB '정치보복' 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버 뉴스 ↩︎ 10.https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence ↩︎ 11.아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스 ↩︎","link":"/2019/09/23/Naver-News-Comments-Analysis-(3)/"},{"title":"Pandas Dataframe의 다양한 iteration 방법 비교","text":"pandas는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에 없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만 오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어 있지 않은 편이다. 이번 글에서는 pandas의 여러 기능 중에서 iteration하는 여러 방법을 속도와 사용성 측면에서 비교해본 내용을 아주 간단하게 정리해 보았다. Summary rank method time iterrows 대비 속도 1 itertuples 7.7ms x8.1 2 at / iat 15.8ms x4 3 loc / iloc 24.6ms x2.5 4 iterrows 62.7ms x1 번외 values 7.1ms x8.8 번외 apply + to_dict 9.91 ms x6.3 Introduction 실험에 사용한 데이터는 아래와 같이 id, text, title 정보로 이루어진 위키피디아를 처리한 table 형식의 데이터이다. text는 위키피디아 문서를 일정 길이 단위로 잘라서 가공한 문장들이고, title은 해당 문장이 속한 위키피디아 문서의 제목을 의미한다. id는 각 문장들의 고유 번호이다. 데이터의 row 별로 iteration을 하면서 처리할 내용은 1) 아래의 cut_text를 통해 text의 길이를 줄이고, 2) table 의 내용을 list_of_dict 형식으로 변환하는 것이다. def cut_text(text, max_len: int = 100): return ' '.join(text.split()[:max_len]) 실험할 함수는 크게 iterrows, loc/iloc, at/iat, itertuples, 그리고 속도 면에서는 장점이 있으나 약간의 단점이 있는 values, 그리고 이번 task 에 overfitting 된 apply + to_dict 가 있다. 하나하나 살펴보도록 하자! iterrows 많이 사용되는 함수이지만 가장 성능이 좋지 않다. %%timeitresult = []for i, row in data.iterrows(): short_text = cut_text(row['text']) instance = { 'id': row['id'], 'text': short_text, 'title': row['title'] } result.append(instance) 62.7 ms ± 729 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) loc / iloc iterrows 다음으로 많이 사용되는 방식이다. iterrows에 비해 2.5배 정도 빠른 속도를 보인다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.loc[idx, 'text']) instance = { 'id': data.loc[idx, 'id'], 'text': short_text, 'title': data.loc[idx, 'title'] } result.append(instance) 24.6 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) :warning: 다만, loc을 잘못 쓰게 되면 iterrows를 썼을 때보다도 더 오랜 시간이 소요된다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.loc[idx]['text']) # diff instance = { 'id': data.loc[idx]['id'], 'text': short_text, 'title': data.loc[idx]['title'] } result.append(instance) 261 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) :warning: 미리 row를 받으면 조금 더 빨라지지만, 그럼에도 iterrows대비 느리다. %%timeitresult = []for idx in data.index: row = data.loc[idx] short_text = cut_text(row['text']) # diff instance = { 'id': row['id'], 'text': short_text, 'title': row['title'] } result.append(instance) 99.4 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) at / iat loc / iloc 과 유사하지만, 특정 column과 row에 해당하는 값을 받고 싶을 때 사용한다. at 함수에 대한 상세한 설명은 pandas 공식 문서에서 확인할 수 있다. iterrows에 비해 4배 정도 빠른 속도를 보인다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.at[idx, 'text']) instance = { 'id': data.at[idx, 'id'], 'text': short_text, 'title': data.at[idx, 'title'] } result.append(instance) 15.8 ms ± 49.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) itertuples iterrows와 유사하지만, Series가 return되는 iterrows와는 다르게 NamedTuple이 return 된다. column에 대응되는 값에 접근하기도 쉽고, 속도도 8배 이상 빠르다. %%timeitresult = []for row in data.itertuples(): short_text = cut_text(row.text) instance = { 'id': row.id, 'text': short_text, 'title': row.title } result.append(instance) 7.7 ms ± 21.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) values 여기서부터는 번외 느낌인데, values는 속도가 가장 빠르다는 장점이 있지만 column에 대응되는 값을 불러올 때 불편한 점이 있다. 이 점을 감안해서 써도 무관하다면 가장 좋은 선택이 될 것 같다. %%timeitresult = []for value in data.values: short_text = cut_text(value[1]) instance = { 'id': value[0], 'text': short_text, 'title': value[2] } result.append(instance) 7.1 ms ± 43.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) apply + to_dict for 문 안에서 처리할 내용이 복잡하지 않은 이번 태스크같은 경우에 쓰기 적합한 방식이다. 새로운 dataframe 혹은 새로운 column을 생성해야 해서 메모리 측면에서 오는 단점은 있지만, 코드가 짧고 깔끔하다는 장점이 있다. %%timeitresult = data.copy()result['text'] = result['text'].apply(lambda x: cut_text(x))result = result.to_dict(orient='records') 9.91 ms ± 19.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) References https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html","link":"/2021/02/04/Pandas-Dataframe-iterations/"},{"title":"PyTorch의 view, transpose, reshape 함수의 차이점 이해하기","text":"최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래 구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서 의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는 transpose 와 view 의 기능을 헷갈려했기 때문이었다. 두 함수의 차이는 contiguous 를 이해해야 알 수 있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를 바탕으로 정리해보았다. contiguous 란 matrix 의 눈에 보이는 (advertised) 순차적인 shape information 과 실제로 matrix 의 각 데이터가 저장된 위치가 같은지의 여부이다. 아래의 예시에서 t 는 contiguous 하다. 왜냐하면 t[0][0][0] → t[0][0][1] → t[0][1][0] ... 의 데이터 포인터 위치 (0 → 1 → 2 ... ) 또한 연속적이기 때문이다. 아직 이해가 되지 않아도 괜찮다. 예시를 좀 더 보자! t = torch.tensor([[[0, 1], [2, 3], [4, 5]], \\ [[6, 7], [8, 9], [10, 11]], \\ [[12, 13], [14, 15], [16, 17]], \\ [[18, 19], [20, 21], [22, 23]]]) # (4, 3, 2) print(t) >foldedtensor([[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]], [[18, 19], [20, 21], [22, 23]]]) view view 를 통해 t 라는 tensor의 shape를 변경시켜보았다. tv = t.view(4, 2, 3) print(tv) >foldedtensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) shape이 (4, 2, 3) 으로 잘 바뀐 것을 확인할 수 있다. 그리고 tv[0][0][0] → tv[0][0][1] → tv[0][0][2] ... 의 데이터 포인터 위치 (0 → 1 → 2 ... ) 또한 연속적이기 때문에 tv 는 contiguous 하다. tv.is_contiguous()---True 데이터의 tensor index 순서대로 tensor를 flatten 시켜주는 함수를 통해 t 와 tv 를 비교하면 동일하게 나오는 것을 확인할 수 있다. t.flatten() == tv.flatten()---tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]) 또한 t 와 tv 의 데이터는 pointer 값이 동일하여 한 쪽의 tensor data 를 수정하면 다른 쪽의 값도 동시에 변경된다. t.storage().data_ptr() == tv.storage().data_ptr() # data pointer 값이 일치함---True # Modifying view tensor changes base tensor as well.t[0][0][0] = 99tv[0][0][0]---tensor(99) transpose transpose 를 통해 t 라는 텐서의 shape을 변경시켜보았다. shape은 tv와 동일하나, 구성이 조금 다른 것을 확인할 수 있다. 참고로, 보통 (batch_size, hidden_dim, input_dim) 을 (batch_size, input_dim, hidden_dim) 으로 바꿔주는 작업을 할 때에 transpose 를 사용한다. tt = t.transpose(2, 1) # (4, 2, 3) print(tt) >foldedtensor([[[ 0, 2, 4], [ 1, 3, 5]], [[ 6, 8, 10], [ 7, 9, 11]], [[12, 14, 16], [13, 15, 17]], [[18, 20, 22], [19, 21, 23]]]) 앞선 예시에서 t 의 데이터 포인터는 0 → 1 → 2 ... 순서대로 저장된 것을 알 수 있었다. t와 tv 모두 데이터 포인터의 물리적 순서와 shape 상에서의 데이터 순서가 같았기 때문에 contiguous 했다. 하지만 tt 의 경우 0 → 1 → 2 ... ≠ tt[0][0][0] → tt[0][0][1] → tt[0][0][2] ... 이기 때문에 contiguous 하지 않다. tt.is_contiguous()---False tt 를 flatten 시키면 물리적 순서 (0 → 1 → 2 ... ) 와 shape 상의 순서가 다른 것을 확인할 수 있다. tt.flatten()---tensor([ 0, 2, 4, 1, 3, 5, 6, 8, 10, 7, 9, 11, 12, 14, 16, 13, 15, 17, 18, 20, 22, 19, 21, 23]) contiguous 그렇다면 강제로 물리적 위치를 연속적으로 만들어버리면 어떻게 될까? 겉보기에는 tt 와 별 차이가 없어보인다. tt.contiguous() == tt---tensor([[[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]]]) 하지만 각 데이터 포인터를 비교하면 tt.contiguous() 는 0 → 2 → 4 ... 이고 tt 는 0 → 1 → 2 이기 때문에 아래의 값이 False가 나오는 것을 예상해볼 수 있다. tt.contiguous().storage().data_ptr() == tt.storage().data_ptr()---False reshape contiguous 개념을 이해했다면, reshape 과 view 함수의 차이도 알 수 있다. 쉽게 얘기하면 reshape() == contiguous().view() 와 같다. view 는 contiguous 하지 않은 tensor 에 대해서는 적용할 수 없다. tt.view(4, 3, 2) # tt.shape() == (4, 2, 3)---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-89-785954c0ff12&gt; in &lt;module&gt;----&gt; 1 tt.view(4, 3, 2) # tt.shape() == (4, 2, 3)RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. tt.contiguous().view(4, 3, 2)---tensor([[[ 0, 2], [ 4, 1], [ 3, 5]], [[ 6, 8], [10, 7], [ 9, 11]], [[12, 14], [16, 13], [15, 17]], [[18, 20], [22, 19], [21, 23]]]) 하지만 reshape 은 강제로 tensor를 contiguous 하게 만들면서 shape을 변경하기 때문에 가능하다. tt.reshape(4, 3, 2)---tensor([[[ 0, 2], [ 4, 1], [ 3, 5]], [[ 6, 8], [10, 7], [ 9, 11]], [[12, 14], [16, 13], [15, 17]], [[18, 20], [22, 19], [21, 23]]]) tt.reshape(4, 3, 2).is_contiguous()---True Summary view : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치할 때 (contiguous) shape을 재구성한다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. transpose : tensor 에 저장된 데이터의 물리적 위치 순서와 상관없이 수학적 의미의 transpose를 수행한다. 즉, 물리적 위치와 transpose가 수행된 tensor 의 index 순서는 같다는 보장이 없으므로 항상 contiguous 하지 않다. reshape : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. References https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2","link":"/2021/03/03/PyTorch-view-transpose-reshape/"},{"title":"Positional Encoding in NLP","text":"Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게 sequential information을 얻지 못하는 경우에 대해 정보를 강제하는 방식이다. 보통 sequential data를 Recurrent Neural Network (RNN) 외의 다른 모델로 다루고 싶을 때 많이 사용된다. 이번 글에서는 Convolutional Neural Network (CNN), End-to-End Memory Network (MemN2N), Transformer에서 sentence embedding을 위해 사용된 positional encoding에 대해 소개하려고 한다. Introduction 일반적으로 NLP 모델은 각 문장을 구성하는 token을 one-hot vector가 아닌 distributed vector로 표현한다. 그 이유는 distributed representation이 1) 비슷한 의미지만 다른 lexical form을 가진 token을 더 잘 표현할 수 있기 때문이고, 2) embedding dimension을 감소시킬 수 있기 때문이다. 문장의 embedding은 문장을 이루는 각 token의 embedding을 조합하는 방식으로 얻어진다. 이 때 position에 대한 정보가 없다면 모델은 handful of chocolate과 chocolate of handful을 같은 의미로 인식하게 된다. RNN은 모델 구조 자체에 time information이 녹아져 있다. 그래서 handful --&gt; of --&gt; chocolate 의 순서가 담긴 sentence embedding을 자연스럽게 얻을 수 있다. 반면 CNN이나 Attention 기반의 Transformer는 순서에 대한 정보를 강제해야 하고, 이 때 positional encoding이 사용된다. Positional encoding (PE) 은 token embedding vector에 곱해지는 정보로, sentence에서 해당 token이 어디에 위치해 있는지를 나타낸다. J개의 token \\(t_j \\in \\mathbb{R}^d\\)으로 구성된 sentence \\(s = [t_1, t_2, ..., t_J]\\)가 있다고 하자. PE \\(\\in \\mathbb{R}^{J \\times d}\\) 의 row \\(j\\)마다 다른 값을 가지도록 하여 문장 맨 처음의 handful과 맨 뒤의 handful을 다르게 인식하도록 한다. PE를 구성하는 방식에는 크게 두 종류가 있다. 하나는 학습기반, 다른 하나는 position과 dimension을 입력으로 한 함수를 이용하는 방법이다. Learned Positional Embeddings 학습기반의 PE를 구성하는 방식은 Convolutional Sequence to Sequence Learning (ConvS2S)[1]에서 사용되었다. 평균 0, 표준편차 0.1을 따르는 normal distribution으로 initialize되고 학습을 통해 position 정보를 배우길 기대한다. PE를 encoder와 decoder 모두에 사용한 경우, encoder에만 사용한 경우, decoder에만 사용한 경우, 아예 사용하지 않은 경우로 나누어 번역 task에 실험해보았을 때의 결과는 다음과 같다. BLEU를 기준으로 분석해보면 encoder에서의 PE역할이 decoder보다 조금 더 중요하다. PE를 아예 쓰지 않을 때의 점수가 가장 낮지만 점수 차이를 생각해보면 모델 성능에는 크게 영향을 미치지 않는다고 해석해 볼 수 있다. 학습 기반이므로 학습 시 다루지 않았던 길이의 문장이 입력으로 들어온 경우, 외삽이 불가능하다는 단점이 있다.[5] Function-based Positional Encoding 함수 기반의 PE는 문장에서 몇 번째에 위치한 토큰인지, 토큰의 embedding dimension이 무엇인지를 정해주면 값이 정해진다. 이 때, 다른 위치의 정보가 같은 값으로 mapping되지 않아야 한다. 어떻게 구현할 수 있을까? The Intuition 0부터 15까지의 숫자를 2진법으로 나타내보자. 다른 색으로 구분지어 표현한 2진수의 자리수마다 다른 주기를 가지는 것을 볼 수 있다. 붉은색은 주기가 1이고, 노란색은 주기가 2, 초록색은 주기가 4, 파란색은 주기가 8이다. 위 예시에서의 자리수를 embedding dimension이라고 생각해보면 PE에도 같은 원리를 확장시켜볼 수 있다. In MemN2N End-to-End Memory Network (MemN2N)[4]에서는 아래의 함수를 사용했다. \\[PE_{k j}=(1- \\frac{j}{J})-\\frac{k}{d}(1- \\frac{2j}{J})\\] \\[j \\in {1, ..., J}\\] \\[k \\in {1, ..., D}\\] 임의의 문장 \"The same representation is used for questions, memory inputs and memory outputs.\"에 적용되는 PE를 시각화해보면 다음과 같다.[5] 여기서는 dimension에 관계없이 같은 주기를 가지지만 시작값이 전부 다르다. 결과적으로는 position마다 다른 vector를 곱하게 되어 position 정보를 전달할 수 있다. 다른 문장 길이를 가지는 경우에 대해서 적용해보면 어떨까? 이번에는 \"We therefore propose a second representation that encodes the position of words within the sentence.\"에 대해 시각해보았다.[5] position이 늘어난만큼 position encoding 값의 변화도가 줄었다. J는 문장마다 달라지므로 첫번째, 두번째의 절대적인 위치보다는 각 순서를 구분짓기 위한 목적에 치중하였다. ConvS2S에서와는 달리 MemN2N에서 PE의 효과는 꽤나 컸던 것으로 보인다. In Transformer Attention is all you need[5]에서 사용된 PE는 주기함수로 유명한 sin 함수와 cos 함수를 기반으로 한다. (a.k.a, sinusoidal functions) \\[ \\begin{aligned} P E_{(\\text {pos, 2k} )} &amp;=\\sin \\left(\\text {pos} / 10000^{2 k / d}\\right) \\\\ P E_{(\\text {pos,2k+1})} &amp;=\\cos \\left(\\text {pos} / 10000^{2 k / d}\\right) \\end{aligned} \\] 잠시 고등학교 때 배운 수학을 떠올려보자. \\(sin(ax + b)\\) 의 주기는 \\(\\frac{2\\pi}{|a|}\\) 이다. 따라서 PE의 특정 position vector 값의 주기는 \\(2\\pi \\cdot 10000^{2 k / d}\\) 와 같다. MemN2N에서의 PE와는 달리, position vector의 주기가 vector의 dimension마다 변화한다. 전체 벡터 크기(\\(d\\))가 128이라고 가정할 때, \\(k\\)가 작을수록 주기가 짧고 \\(k\\)가 클수록 주기도 길어진다. (아래 그림 참고) 왜 Transformer에서는 MemN2N과 다르게 sinusoidal 함수를 썼을까? 논문에서 그 이유를 짧게 기술하고 있다. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k\\), \\(P E_{pos+k}\\) can be represented as a linear function of \\(P E_{pos}\\). sinusodial 함수의 특징을 이용해 첫번째, 두번째마다 같은 position 정보를 주면서도 \\(n + k\\)번째 vector가 \\(n\\)번째 vector와 관계가 있을 때 이를 학습할 수 있는 여지를 남겨주기 위함이다. (참고로 이에 대한 수학적인 증명은 이 article에 기술되어 있다.) 또한 PE vector 간의 distance는 대칭적이고 거리에 따라 일정한 비율로 감소한다. Transformer의 self-attention 연산에서 빛을 발하는 특징이다. Conclusions PE는 크게 학습을 통해 정해질 수 있고 미리 지정한 함수로 정해질 수도 있다. 학습을 통한 방식은 학습시 보지 않았던 새로운 길이가 등장했을 때 외삽이 불가능하지만 함수 기반의 PE는 가능하다. 함수도 어떤 함수를 쓰느냐에 따라 종류가 구분되는데, 절대적인 위치에 따라 같은 값을 가지면서도 상대적 위치의 관계도 학습할 수 있는 \\(sin\\)과 \\(cos\\) 기반의 함수가 가장 좋은 방법이라고 생각된다. References 1.https://arxiv.org/abs/1705.03122 ↩︎ 2.https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ ↩︎ 3.https://arxiv.org/abs/1503.08895 ↩︎ 4.https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb ↩︎ 5.https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf ↩︎","link":"/2020/01/26/Positional-Encoding/"},{"title":"유쾌한 슬럼프","text":"어렸을 때의 나는 무언가 새롭게 ‘시작’하는 것을 좋아했다. 무언가를 빨리 배우는 편이었고, 새롭게 어떤 환경이나 개념에 적응하는데에 드는 시간이 적게 들었기 때문에 ‘시작’을 즐길 수 있었기 때문이었던 것 같다. ‘시작’이 주는 그 몰입감과 성취감은 권태로움에서 나를 꺼내주는 좋은 처방전이었다. 하지만 동시에, 내가 ‘시작’했던 많은 자잘한 일들은 소위말해 ‘꿀만 빨고’ 그만 둘 수 없었다. 처음이 주는 신선함에 어느 정도 익숙해지고나면 이 분야의 ‘탁월함’이 보이기 시작하고, 나는 한참 밑에 자리하고 있다는 사실을 인지하게 된다. 그리고, 그때쯤 항상 그만두고 싶어졌다. 이 시기를 견디지 못하는 이유는, 아마 기준이 낮았기 때문이었을 것이다. 내가 성취하고자 하는 것은 굉장히 낮은 수준의 것이었고 그 수준을 빠르게 달성하고 나면 더 이상의 열정이 생기지 않았다. 오히려 ‘왜 해야하지?’ 라는 생각이 나를 합리화시키면서 금방 포기하게 만들었다. 그래서 참 많은, 잡다한 무언가를 하지만 어느 것 하나 제대로 마치지 못하는 나를 발견하곤, ‘잘’하는 것에 좀 더 집중해보기로 했다. 그 이후로 시작에 대한 두려움이 커졌다. 시간과 실력의 관계를 그래프 형태로 나타내면, ‘계단’형이라고 할 수 있겠다. 기나긴 슬럼프 끝에 오는 어느 순간의 깨달음으로, 실력이 갑자기 반등한다. 내게 두려운 기간은 바로 이 ‘슬럼프’의 시기다. 이 또한 지나가리라는 것을 알면서도, 그 순간들은 언제나 고통스럽다. 마치 근력운동을 할 때 정말 힘들지만, 이 순간을 견뎌내야 근육이 붙는다는 사실을 인지하는 느낌이다. 그 때에 나는 고민한다. 새로운 시작을 할 때인가 아니면, 버텨내야할 때인가를. 잘하고 싶기에 버텨내야 한다는 답을 내리지만, 이게 무슨 의미가 있지? 라고 물었을 땐, 쉽게 답하기가 어려울 때가 많다. 왜 버텨내야 하는지, 스스로를 격려하지 않으면 자꾸만 과거의 습관이 튀어나와 새로운 시작을 시도하려고 한다. 안타까운 건, 이젠 새로운 시작마저도 그 과정에서 다시금 슬럼프를 마주한다는 사실을 인지하고 있기 때문에 잘 하려고 들지 않는다는 것이다. 그래서, 차악으로, 내가 겪고 있는 슬럼프의 시기를 버티게 되는 요즘이다. 당신의 슬럼프는 안녕하신가요? 유쾌한 슬럼프란, 존재할 수 없는 개념일까요?","link":"/2019/05/15/Joyful-slump/"},{"title":"나, 다시 쓰는 자기소개서","text":"매달 여는 연울림 모임이지만 특히나 지난 달에 했던 가치 워크샵은 내게 많은 고민을 남겨준 시간이었다. 200여 개의 가치들 중, 내가 중요하게 여기는 가치를 선택하는 과정 속에서 종착점이 생각보다 명확하다는 인상을 받았는데, 그 지점을 향해 나는 제대로 가고 있는 것인지에 대한 의문이 생겼기 때문이었다. 내가 골랐던 가치는 공헌, 능력, 리더십, 이타주의, 지혜였고, 지혜로우면서도 능력있는 사회적 문제를 해결하는 기업가로 요약되었다. 반면, 내가 지금 발 딛고 있는 현재는 데이터 분석가이자 모임 기획자로 간략하게 소개해 볼 수 있었다. 이 자리에서 어떻게 사회적 기업가로 나아갈 수 있을지, 왜 현재의 그 일을 하고 있고, 내가 풀고 싶은 사회적 문제와 어떻게 연관되는지 등을 다시 한 번 현재의 나에게 물어야 할 때가 왔다는 생각이 들었다. 그래서 간략하게 나를 소개해보는 문장을 적어보았다. 저는 파파고라는 인공지능 번역 서비스를 함께 만들어나가고 있는 머신러닝 엔지니어입니다. 대학에 입학했을 때의 전공은 화학생물공학으로 현재 하고 있는 일과 큰 관련이 없는 전공이었습니다. 사람들은 저마다의 가치와 추구하는 바가 있고, 이를 최대한으로 발현할 수 있는 사회를 만들고 싶습니다. '날리다'라는, 더 많은 사람들이 자기이해를 할 수 있는 모임을 기획하는 단체에서 '연울림'이라는 모임을 기획하고 운영하고 있습니다. 제 삶의 마지막 단계에서는 소모적인 경쟁이 없고, 다양한 장점을 인정하는 시스템을 만들고 싶다는 꿈이 있습니다. 한 문장, 한 문장이 서로 크게 연관이 되어 보이지는 않는 피상적인 형태의 자기소개서가 되었다. 하지만 아직 발견되지 못했을 뿐, 제각각의 문장을 관통하는 무언가가 있을 것이라는 생각이 들었다. 그 답은 스스로에게 던져진 질문에 답하는 과정에서 찾을 수 있겠지. '나'라는 사람을 설명해주는 으니까. 언제나 그렇듯, 좋은 질문은 좋은 답을 이끄는 방향키다. 당신은 어떤 사람인가?","link":"/2019/07/20/Rewriting-self-introduction/"},{"title":"PyTorch의 IterableDataset을 사용해서 데이터 불러오기","text":"PyTorch 1.2 이상부터 torch.utils.data 에서는 크게 map-style dataset (torch.utils.data.Dataset) 과 iterable dataset (torch.utils.data.IterableDataset) 의 두 종류의 데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는 IterableDataset 을 사용하는 것이 좋은데, Dataset 과는 딜리 아직 개발되어야 할 기능이 더 필요한 클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다. Map-style Dataset 1.2 이하 버전에서 사용되던 map-style dataset은 memory에 모든 데이터를 업로드할 수 있을 때 사용하는 가장 일반적인 dataset type 이다. custom dataset class를 생성하고자 할 때 torch.utils.data.Dataset 을 상속받아 __len__ , __getitem__ 을 구현하면 된다. from torch.utils.data import Datasetclass MyMapDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): return self.data['text'][index] Iterable Dataset 하지만 학습 데이터가 메모리에 다 올라가지 않는 경우가 발생할 수 있다. 이 문제를 해결할 수 있는 다양한 방법 중에 하나로, torch.utils.data.IterableDataset 을 사용하는 방법이 있다. Map-style Dataset과 비슷하게 torch.utils.data.IterableDataset 을 상속받아서 custom dataset class를 생성하고, __iter__ 를 선언하면 된다. from torch.utils.data import IterableDatasetclass MyIterableDataset(IterableDataset): def __init__(self, data_path): self.data_path = data_path def __iter__(self): iter_csv = pd.read_csv(self.data_path, sep='\\t', iterator=True, chunksize=1) for line in iter_csv: line = line['text'].item() yield line Dataset이 batch data를 생성할 때 map_dataset[index]를 사용한다면, IterableDataset은 next(iterable_dataset) 을 사용한다. 이 때문에 DataLoader를 통해 IterableDataset을 불러와서 사용하게 되면 sampler 옵션의 사용이 어렵다. 그래서 random suffling 을 하고 싶다면 미리 데이터셋을 shuffling 한 이후에 불러오는 것이 좋다. Going Parallel PyTorch 공식문서에 따르면 IterableDataset을 num_workers &gt; 0의 조건에서 사용할 때 특별히 다음을 유념할 것을 제안하고 있다. When num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset’s __iter__() method or the DataLoader ‘s worker_init_fn option to modify each copy’s behavior. 위의 문장을 이해하려면 num_workers 에 대한 이해와, num_workers &gt; 0 일 때 IterDataset 에서 어떤 현상이 일어나는지 알아야한다. num_workers는 데이터셋을 불러올 때 사용할 subprocess의 개수이다. num_workers == 0 은 main process에서 데이터를 불러오고 모델에 pass하는 작업을 모두 수행한다는 뜻이며, num_workers == 2는 subprocess 2개에서 데이터를 불러오고 main process에서는 subprocess에서 불러온 데이터를 model에 pass하는 역할만 담당한다. 따라서 num_workers &gt; 0 일 때 data loading에서의 병목이 줄어들며 gpu utilization 을 100% 가까이 끌어올릴 수 있다. 그럼, num_workers &gt; 0 일 때 어떤 현상이 발생하는지 살펴보자. Map-Style Dataset from torch.utils.data import DataLoader, Dataset, IterableDatasetimport timeclass MyMapDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): worker = torch.utils.data.get_worker_info() worker_id = worker.id if worker is not None else -1 start = time.time() time.sleep(0.1) end = time.time() return self.data[index], worker_id, start, enddata = range(16)map_dataset = MyMapDataset(data) num_workers == 0 인 경우 loader = DataLoader(map_dataset, batch_size=4, num_workers=0)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])tensor([ 8, 9, 10, 11]) tensor([-1, -1, -1, -1])tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1]) num_workers == 2 인 경우 loader = DataLoader(map_dataset, batch_size=4, num_workers=2)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])tensor([ 8, 9, 10, 11]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) 의도한대로, subprocess 별로 서로 다른 데이터를 불러오는 것을 알 수 있다. Iterable Dataset from torch.utils.data import DataLoader, Dataset, IterableDatasetimport timeclass MyIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for x in self.data: worker = torch.utils.data.get_worker_info() worker_id = worker.id if worker is not None else -1 start = time.time() time.sleep(0.1) end = time.time() yield x, worker_id, start, enddata = range(16)iterable_dataset = MyIterableDataset(data) num_workers == 0 loader = DataLoader(iterable_dataset, batch_size=4, num_workers=0)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])tensor([ 8, 9, 10, 11]) tensor([-1, -1, -1, -1])tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1]) num_workers == 2 loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([0, 1, 2, 3]) tensor([1, 1, 1, 1])tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])tensor([ 8, 9, 10, 11]) tensor([0, 0, 0, 0])tensor([ 8, 9, 10, 11]) tensor([1, 1, 1, 1])tensor([12, 13, 14, 15]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) ⚠️ worker 0과 worker 1에서 같은 데이터를 로딩하고 있다. 이 점이 공식문서에서 지적하고 있는 내용이다. 각 워커별로 같은 __iter__()를 사용하기 때문에 같은 데이터를 로딩하게 된다. 이를 방지하기 위해서는 worker_init_fn 에서 직접 워커 별 데이터를 재분배 시켜줘야 한다. def worker_init_fn(_): worker_info = torch.utils.data.get_worker_info() dataset = worker_info.dataset worker_id = worker_info.id split_size = len(dataset.data) // worker_info.num_workers dataset.data = dataset.data[worker_id * split_size: (worker_id + 1) * split_size] loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2, worker_init_fn=worker_init_fn)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([ 8, 9, 10, 11]) tensor([1, 1, 1, 1])tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) worker_init_fn 을 통해 분배시켜준 결과 worker 0과 worker 1 에서 다른 데이터를 순차적으로 불러오는 것을 알 수 있다 🙂 Conclusions IterableDataset 은 데이터가 메모리에 올라가지 않을만큼 클 때 사용하면 좋다. Dataset과 다르게 __iter__()를 선언해서 데이터를 부른다. 하지만 이 특징 때문에 Sampler 와 함께 사용할 수 없다. 또한 num_workers &gt; 0 인 세팅에서는 각 워커에서 다른 데이터를 불러올 수 있도록 worker_init_fn을 선언해야 한다. References How to Build a Streaming DataLoader with PyTorch | by David MacLeod | Speechmatics | Medium https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html torch.utils.data — PyTorch 1.7.1 documentation","link":"/2021/02/21/PyTorch-IterableDataset/"},{"title":"스카이캐슬을 통해 본 서울대학교 학생들의 우울증","text":"기사에 따르면, 서울대학교 학생들의 47%가 우울증을 앓고 있다고 한다. 그 원인으로 사상 최악의 취업난과 과열된 학점 경쟁을 꼽고 있는데, 실상을 전혀 모르고 하는 소리다. 만약 그 이유로 우울했더라면, 진로문제나 학업문제로 상담소를 찾는 학생들이 제일 많아야 한다. 그러나, 기사에도 적혀있듯이 심리상담소를 찾는 대부분의 학생은 “정서문제”로 어려움을 겪고 있다. 서울대 학생들은 확실히 타교 학생들에 비해 취업 걱정이 덜하다. 주변에서 “취업을 하지 못할 것”으로 염려하는 학생들은 보지 못했다. 대신, “내가 무엇을 좋아하는지, 무엇을 잘할 수 있는지, 내가 선택한 모든 것들이 괜찮은 것인지, 이래도 되는 것인지, 평생에 걸쳐 이뤄내고 싶은 내 삶의 목적은 무엇인지, 나는 어떤 사람인지”를 몰라 방황하는 학생들은 많았다. 요즘 인기리에 방영 중인 드라마 “스카이캐슬”을 본 사람들이라면, 서울대생의 이런 고민에 공감할 수 있을 것이다. 서울대학교에 입학한 대부분의 학생들은 스카이캐슬의 학생들과 비슷한 학창시절을 보낸다. 운 좋게 공부에 재능을 가지고 태어나서 큰 어려움없이 초등학교와 중학교 교육과정에서 요구하는 바를 충실히 따라가고, 고등학교에 진학하면 자연스레 좋은 대학에 가는 것이 목표가 되어 버린다. 주변에서 관심을 가지는 것은 모두 “성적”에 관련된 것들 뿐이다. 지금 내가 취약한 과목은 무엇인지, 어떤 인강이, 어떤 선생님이 좋은지. 부모님도 내가 높은 성적을 받을 수 있도록 모든 방면에서 서포트를 해주신다. 마치 좋은 대학에만 들어가면 나의 인생에서 행복이 보장된 것처럼. 그리고, 학생 스스로도 자신을 기만한다. 대학교에 입학하기 전까지 이 학생들에게는 큰 고민이랄 것이 없다. 목표는 확실했고, 그 목표를 향해 충실히 달려나가고 있기 때문에. 문득 떠오르는, “왜 나는 좋은 대학에 가야하지?”라는 질문이 가장 큰 방해물이고, 그런 생각은 일단 좋은 대학에 붙은 이후에 하는 것으로 여겨진다. 그렇게 인생의 중요한 문제를 고민하는 시기를 미루게 된다. 서울대학교에 입학하고 나서 이들은 과연 행복했을까? 그리고, 인생의 근본적인 문제들-삶의 이유, 목적, 나의 존재 이유-이 해결이 되었을까? 전혀 그렇지 않다. 대학교 입학 자체가 목적이었기에 이를 달성한 순간에는 행복했겠지만, 그 이후의 삶에 대해서는 크게 고민해본 적이 없었기 때문에 대학 입학 이후의 시간들은 이 학생들에게 너무나 큰 짐으로 다가왔을 것이다. 고등학교 때 나에 대한 진지한 고민없이 선택한 전공이 나와 맞지 않음을 깨닫기 시작하고, 그렇다면 나에게 맞는 미래는 무엇인지를 묻게 된다. 하지만 경험이 적다보니 그 질문에 대한 답을 찾는 것이 어렵고, 답이 문득 떠올랐더라도 항상 제일 좋은 선택지를 택해왔고, 정답이 있는 선택을 해왔기 때문에 ‘이래도 되는걸까?’라는 생각과 함께 스스로 결정을 내리지 못하고 머물러있다. 이들은 그동안 살아왔던 방식과 전혀 다른 방식으로 살아가길 도전받는다. 그래서 서울대학교 학생들은 두렵다. 하지만 그 마음을 누군가에게 잘 털어놓지 못한다. 부모님은 ‘우리 아이는 스스로 알아서 잘 할 것’이라고 생각하기에 털어놓지 못하고, 고등학교 친구들은 ‘서울대가 고민은 무슨 고민’이라고 생각하기에, 같은 대학교 친구들은 속을 쉽게 털어놓을만큼 깊은 관계가 아니기에 이런 이야기를 하기 어렵다. 마음 속에 부채처럼 쌓이는 이야기들이 어느 순간 스스로를 곪게 만든다. 나의 이야기를 편견없이 들어줄 수 있는 사람을 원하게 된다. 그래서 상담소를 찾게 되고, 그 곳에서 오히려 주변 사람들에게 하지 못했던 나의 깊은 이야기를 털어놓게 된다. 그리고, 그 것만으로도 마음이 치유된다. 내가 느끼는 모든 것들이 이상하지 않고, 지금은 넘어지고 다칠 수 밖에 없는 시기라는 사실에 위안받는다. 남의 이야기처럼 썼지만, 약 70%정도는 나의 이야기이다. 그래서 요즘 스카이캐슬을 보면 마음이 아프다. 스카이캐슬 입주 가족 중 어느 누구도 대학 이후의 삶에 대해서는 이야기하지 않는다. 어떤 사람이 되고 싶은지, 나의 죽음은 어떻게 기억되고 싶은지, 왜 그 대학에 가고 싶은지, 그 대학에 가서 제일 하고 싶은 것은 무엇인지 등. ‘서울의대 합격’이라는 증명서가 전부인 세상. 그 안에서 이미 곪을대로 곪아버린 영재를 떠올려본다.","link":"/2018/12/23/Skycastle/"},{"title":"한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기","text":"약 4-5개월동안 사이드로 진행했던 혐오 댓글 프로젝트[1]가 성공적으로 마무리되었다. 같은 문제의식을 가진 사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이 무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는 활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고 원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의 상호보완적인 역량 덕분이 아니었을까 싶다. 사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한 이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작 작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던 부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는 사람들에게도 좋은 팁이 되지 않을까? 어노테이션 왜 편견과 혐오인가? 어노테이션 가이드라인에도 나와있듯이 우리는 크게 편견과 혐오라는 두 가지 aspect에 대해서 label을 수집했다. 처음에는 성에 관련된 편견 및 혐오와 그 외의 편견 및 혐오로 나누었는데, 이보다는 편견과 혐오로 구분하는 것이 낫다는 판단을 했다. 가이드라인 작성을 위해 댓글을 직접 태깅하다 보니 편견만 존재하는 댓글과 혐오만 존재하는 댓글이 존재했다. 항상 혐오가 편견으로부터 시작되지는 않았고, 편견이 있음을 부끄러워하지 않고 세상의 진리인 것처럼 이야기하는 댓글이 보였다. 혐오가 편견으로 시작된 경우는, 무리하게 개인의 특성을 집단의 특성으로 확장해서 그 집단에 대한 혐오를 개인에게 표출할 때였다. 그래서 이 둘의 관계를 데이터로 파악할 수 있도록, 또 편견과 혐오를 구분지어 생각할 수 있도록 편견에 관련된 label과 혐오에 관련된 label을 구분짓기로 했다. 언어학에 관심있는 사람들이라면 label을 바탕으로 댓글을 분석하는 것으로도 재밌는 연구가 될 것 같다. 표현의 자유와 혐오의 경계 이 둘을 구분짓는 좋은 threshold를 결정하는 것은 무슨 목적으로 활용하냐에 달려있다. 우리의 목적은 혐오 댓글의 피해자가 줄어들기를 바라는 것이었으므로 익명인의 표현의 자유보다는 기사의 대상이 되는 사람의 기분을 좀 더 신경쓰기로 했다. 그래서 태깅을 할 때에 당사자의 입장에서 생각하도록 가이드했다. 어노테이션이 어려웠던 댓글 예전에 데뷔작에서 수영복입고 수중씬 기억난다 진짜 섹씨했는데 연예인이라는 직업이 가지는 특수성 때문에 판단하기 어려웠던 경우이다. 특히 여자연예인에 대해서는 외모에 대해 품평하는 댓글이 많았는데, 스스로가 연예인이었던 적이 없으니 감정이입을 해서 이를 모욕이라고 봐야할지도 모르겠고, 만약 의도적으로 외모를 부각해서 유명세를 얻은 경우라면 모욕이라고 보기가 더 어렵다고 생각했다. 결국 각자의 판단에 맡겨서 majority voting을 했지만 정말 어려웠던 케이스였다. 신천지? \"일반적으로 비난받을만한 행위로 인한 혐오는 어떻게 판단해야할까?\" 를 고민하게 만든 댓글이었다. 신천지 교도로 인해 코로나가 빠르게 퍼졌던 사건 이후로 \"신천지\"는 부정적인 이미지로 굳어져 버렸는데, 이 맥락을 고려해서 위의 댓글을 혐오라고 태깅하면 \"신천지\"라는 가치 중립적인 단어가 혐오로 태깅되기에 굉장히 고민이 많았습니다. 살빠진 마닷같애 위와 비슷한 케이스로 이 댓글 또한 판단하기 무척이나 어려웠다 ㅠㅠ offensive로 판단하자니 마닷은 뭐가 되냐는... Other biases 라는 label 현재는 bias label이 gender bias, other biases, none 의 세가지로 구성되어 있다. 이렇게 할 수 밖에 없었던 가장 큰 이유는 예산 문제였다 ㅠㅠ 돈이 많았다면 gender 외에도 정치, 지역, 인종 등에 대한 편견도 label을 수집할 수 있었을텐데 하는 아쉬움이... 인당 150만원 이상은 부담하고 싶지 않아서, 그리고 연예 도메인은 성 편견이 가장 많은 비중을 차지하고 있어서 이런 결정을 하게 되었다. 그러다보니 others 라는 label 은 온갖 종류의 편견이 모두 모아져 있다. 아마도 모델이 곧장 others 를 예측하는 것은 쉽지 않을 것이라고 생각한다. 이 task는, 논문에 적혀있듯이, 2-step classification 문제를 푸는 방식이 낫지 않을까라고 생각한다. 먼저 gender / no-gender 를 예측하고, 그 이후에 bias 유 / 무 를 예측하면 gender, others, none 을 좀 더 쉽게 예측할 수 있을 것이라고 생각한다. 어노테이션 작업 시 context 미제공 댓글에 포함된 편견 및 혐오를 더욱 정확하게 판단하기 위해서는 댓글이 작성된 뉴스 기사에 대한 정보가 필요하다. 하지만 현실적인 이유들로 포기했었다. \"작업자가 기사를 읽어야 하는 번거로움을 감수할까?\" \"태깅 플랫폼에서 이 기능을 제공해줄까?\" \"뉴스 기사의 내용에 대한 저작권은 우리에게 없기 때문에 공개 데이터셋에 포함할 수 없고, 그럴거라면 태깅을 컨텍스트 없이 하는게 좋지 않을까?\" 등의 질문들에 대해 명쾌한 답변을 내리지 못했고, 결국 댓글의 내용만 보고 판단하는 방식을 가져갔다. 지나고나니 아쉬움이 남는 건 어쩔 수 없는듯하다. Testset 구성 현재 testset은 함께 작업했던 저와 조원익, 이준범이 직접 작업한 라벨이 달려있다. 우리의 의도와 부합하는, 가장 어노테이션이 잘 되었다고 보장할 수 있는 데이터셋이라고 볼 수 있다. 하지만 지나고나니 \"시간 순으로 train, validation, testset을 구성했다면 어땠을까?\" 하는 아쉬움이 남았다. 댓글에는 많은 사회적 배경지식이 녹아져있다. 특히 인물의 이름이 가지고 있는 정보가 있는데, 우리가 수집한 기간에는 승리와 정준영 등의 연예인이 얽혀있던 단톡방 사건이 포함되어 있었다. 그래서 \"승리\"가 포함된 댓글은 부정적인 맥락 속에서 판단되었다. 예를 들어 \"승리가 뭘 잘못했다고 난리들인지...그냥 승리 부럽고 베알꼴린 애들이 화난거로밖에 안보임ㅎ\" 라는 댓글에서 \"승리\"를 제거하면 성편견이 없는 것으로 태깅되었겠지만, \"승리\"가 포함되었기 때문에 성편견이 존재하는 것으로 태깅된다. Generalization을 잘 하는 모델이 진짜 잘하는 모델이라고 했을 때, 학습 데이터에 \"승리\"가 없어도 위의 댓글에 달린 라벨을 잘 예측할 수 있는 모델을 판별할 수 있게 testset을 구성했다면 더 좋았을 것 같다. KoBERT tokenization baseline으로 CharCNN, BiLSTM, BERT를 사용한 모델의 결과를 논문[2]에 첨부했다. 여러 task 모두 BERT가 가장 좋은 성능을 보였다. 댓글은 맞춤법을 지키는 문장과는 거리가 멀고, 줄임말, 신조어, 연예인명, 그리고 그 외의 고유명사 등이 많이 등장한다. 그래서 BERT tokenization 결과를 보면 한 글자 씩 분리되는 경우가 빈번했다. 페지해라 가세연. 페지가 답이다 아님말고식 증거도없이 유재석 언급하노['▁페', '지', '해', '라', '▁', '가', '세', '연', '.', '▁페', '지가', '▁답', '이다', '▁아', '님', '말', '고', '식', '▁증거', '도', '없이', '▁유재석', '▁언급', '하', '노'] god 박준형이 이 기사를 싫어합니다.`['▁', 'go', 'd', '▁박', '준', '형', '이', '▁이', '▁기사', '를', '▁싫어', '합니다', '.'] koBERT 학습 데이터에 자주 등장했던 연예인 이름은 원본 그대로 보존되는 반면, 그렇지 못한 연예인은 이름이 쪼개진다. 조개갖고 개ㅈㄹ하는 태콩이나 개촐싹대는조샌징들이나 ㅈㄴ웃김ㅋㅋㅋ ['▁O', 'O', 'O', '▁조', '개', '갖', '고', '▁개', 'ᄌᄅ', '하는', '▁태', '콩', '이나', '▁개', '촐', '싹', '대', '는', '조', '샌', '징', '들이', '나', '▁', 'ᄌᄂ', '웃', '김', 'ᄏ', 'ᄏ', 'ᄏ'] ㅅㅂ 더럽게 메갈어로 제목뽑는거 봐라['▁', 'ᄉᄇ', '▁더', '럽', '게', '▁메', '갈', '어', '로', '▁제', '목', '뽑', '는', '거', '▁봐', '라'] \"ㅈㄴ\", \"ㅈㄹ\", \"ㅅㅂ\" 같은 단어가 tokenization에서는 쪼개지지 않는다. 어려웠던 작업이었고, 완벽했다고는 할 수 없지만 좋은 시작점이 될 수 있는 프로젝트였다고는 생각한다. 이번에 해결할 수 없었던 여러 한계점들을 극복하는 다른 좋은 결과들이 많이 나올 수 있길 :) 이제 진짜 끝! References 1.https://github.com/kocohub/korean-hate-speech ↩︎ 2.https://arxiv.org/abs/2005.12503 ↩︎","link":"/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/"},{"title":"내가 보내는 시간에 부여하는 나만의 의미","text":"개인적으로, PUBLY 박소령 대표님의 인스타계정을 좋아한다. 특히 본인이 읽었던 책에 대해 소개하는 피드를 애정한다. 공감하는 문장이 비슷할 때가 많고, 그 글을 읽고 난 뒤의 생각을 엿보는 재미가 있다. 최근, 회사에 연차를 이틀정도 내고 무엇을 할지 고민하던 중에 그 피드에 있었던 “일하는 마음”이라는 책이 떠올랐다. ‘옳다구나!’ 하고 집어든 책을 이제서야 거의 다 읽었는데, 그 중에 마음에 턱 걸렸던 부분을 소개하려고 한다. 전문성이 한 가지 이름의 직업과 결부되는 것이라면, 탁월성은 일을 바라보는 접근법, 다양한 분야로 확대할 수 있는 중심 기술과 연결된다. \"중심 기술\"은 사실 하나의 서사이자 이름 붙이기다. 기자였다가 번역가이자 작가로 일하고, 또 비영리단체의 옹호부장에서 사업본부장을 거친 김희경 작가는 자신의 중심 기술이 “정보를 구조화하는 것”이라고 말했다. 직업과 직위는 계속 바뀌었지만, 정보를 구조화하는 것이 언제나 자신의 일이었다는 것이다. … 전문성의 필요 조건은 두 가지다. 하나는 ‘오랜 기간’을 보냈다는 것이고, 다른 하나는 그 오랜 기간이 ‘시스템이 인정하는 내부에서 보낸 기간’이어야 한다는 것이다. … 그에 반해 탁월성은 누구에게나 열려 있지만, 그럼에도 더욱 가지기 어려운 것이다. 탁월성은 또한 자신이 해온 일, 하고 있는 일을 어떻게 반추하며 자신만의 시각으로 해석하는가의 문제이기도 하다. 같은 일을 해도 그 일의 경험을 통해 써내려갈 수 있는 이야기는 사람마다 다르다. 얼핏 보아 파편적이고 불연속적인 경험을 통해서도 일관되고 의미 있는 이야기를 써내려갈 수 있는 사람은 자기 기준을 가지고 있고, 그 기준에 맞춰 자기 일의 경험을 스스로 해석할 수 있는 사람이다. 일하는 마음전문성이 아닌 탁월성 지금 나는 서로 다른 성격의 두 가지 일 (파파고와 날리다) 을 병행하고 있다. 둘 다 잘하고 싶은 마음은 크지만 능력의 부족으로 어느 것 하나 제대로 하고 있지 못하고 있었는데, 일에 치일 때마다 ‘하나만 해도 버거운데, 나는 왜 이렇게 살고 있지?’ 라는 생각이 들면서 아무것도 하고 싶지 않은 느낌이 강하게 들곤 했다. 연차를 썼던 그 때의 나에게 필요했던 건, 내가 시간을 투자하는 일에 대한 나만의 의미를 되찾는 것이었다. 다행히 휴가 기간 내에, 불완전하고 미완성인 해석일지라도 새롭게 의미를 부여했다. 그리고 그제서야, 다시 시작할 수 있는 힘을 얻을 수 있었다. 내가 하고 있는 두 일의 중심에는 사람이 있었다. 나는 타인이 해결하고 싶은 문제를 나의 문제로 쉽게 투영시킨다. 누군가의 문제와 이를 둘러썬 상황을 이해하고, 문제를 해결해서 그 사람을 만족시킬 때 큰 기쁨을 느낀다. 그래서 쉽게 드러나지 않는 문제 상황을 더 잘 이해하고자 데이터 분석을 시작했고, 내가 해결책을 제시할 수 있다고 생각하는 문제에 대해서는 더 많은 사람이 나의 해결 방식을 통해 도움을 받게 하고 싶어서 날리다에 속해 프로그램 기획을 하고 있다. 다른 사람들은 어떤 마음으로 자신이 해온 일, 하고 있는 일을 해석하고 있을까? 남에게 나를 소개하기 쉬운 방식 - data manifold의 geometrical insight에 관심있는 로봇자동화 연구실 대학원생, HCI 연구실 대학원생, deep metric learning에 관심있는 대학원생, 전동킥보드 쉐어링 스타트업 개발자, (전) 인액터스 의장, 서울대 수학과 박사, google swe, 건축학도, 파파고 개발자 - 이 아닌, 본인 스스로에게 부여하는 이름은 무엇일까?","link":"/2019/04/04/Through-my-times/"},{"title":"Social Bias in NLP Models","text":"한 스타트업에서 개발한 인공지능 채용솔루션(a.k.a. AI 면접관)을 벌써 여러 기업에서 사용하고 있다는 뉴스기사를 접하게 되었다. 해당 기업은 \"성별이나 학력 등에 따른 차별 방지와 정확한 역량 추정\"을 위해 5만 2천여명의 데이터를 확보하여 학습했다고 말한다. 5만 2천여명의 데이터와 차별 방지가 어떤 관련이 있는지는 모르겠지만, 많은 양의 데이터를 사용한다는 걸 내세우고 싶었다면 대량의 데이터가 편향성을 줄이는 것과는 무관하다고 말하고 싶다. 5만 2천개보다 더 많은 데이터로 학습한 Language Model 도 편향성 문제가 있으며 이 이슈는 아직도 연구자들에 의해 활발히 연구되고 있다. Introduction 2020년 6월 말에 다음과 같은 트윗이 올라왔다. 오바마를 모자이크한 이미지를 넣었는데 백인의 특징을 가진 이미지가 생성이 되었다는 것이다. 이 트윗은 흘러흘러 Yann LeCun의 귀에 들어간다. 그리고 이는 다시 조경현 교수님을 통해 Alice Oh 교수님의 AI &amp; Ethics 특강 발표자료[9]에서 아래의 문구와 함께 다시 한번 인용된다. Too much blame on data curation, too little blame on algorithms 조경현 교수님의 이야기를 좀 더 들어보자. \"물론 데이터도 문제가 있지만 알고리즘의 잘못이 전혀 없는 것은 아니다\" 라고 주장한다. 아래의 장표에서는 ML 알고리즘의 solution space를 구획별로 나누어서 총 4가지로 구분하고 있다. Training solutions: 학습 데이터셋에서 좋은 성능을 보이는 solution space Shortcut solutions: 학습 및 주어진 테스트셋에서 좋은 성능을 보이는 solution space Intended solutions: 학습 및 주어진 테스트셋과 o.o.d 테스트셋 모두에서 좋은 성능을 보이는 solution space. 여기야말로 우리가 원하는 진짜 general knowledge가 있는 space Learnable solutions: 알고리즘이 학습을 통해 도달할 수 있는 solution space 우리가 원하는 것은 intended solution space와 learnable space가 만나는 점에 학습 모델이 수렴하게 만드는 것이다. 하지만 이는 쉽지 않다. data bias 가 있기 때문에 model 이 학습과정에서 data 내에 존재하는 bias 를 함유할 수 밖에 없는 것은 맞지만, 그럼에도 불구하고 이 모든 것이 data bias 의 잘못때문만은 아니라는 것이 조경현 교수님이 이야기하고 싶었던 내용이다. 아래의 장표는 random seed 를 바꿈에 따라 learnable 모델의 학습 결과가 위치하는 solution space를 보여준다. seed 만 바꾸어도 solution space가 달라지는 것으로 미루어보아, 충분히 모델의 학습 과정을 잘 tuning 하면 원하는 solution space 쪽으로 학습할 여지가 있다고 보여진다. Undesirable solution 의 이유는 무엇일까? 여러 이유 중 하나로, shortcut solution 중 일부는 잘못된 correlation 을 학습하기 때문이라는 점이 있다. \"Correlation does not imply Causality\" 라는 유명한 명제가 있다. 눈에 드러나는 X와 Y variable 의 관계에 모두 영향을 미치는 confounding factor Z 가 있고 hidden varirable 인 경우 우리는 실제 인과관계를 놓치고 상관관계를 인과관계처럼 학습할 위험이 있다. 가장 유명한 예로, \"초콜렛의 소비(X)가 많은 나라에서 노벨상(Y)을 많이 받는다.\" 가 있다. 실제로는 \"고등교육에 투자하는 물질적, 시간적 여유가 많을수록(Z) 초콜렛 소비(X)가 많다.\" 와 \"고등교육에 투자하는 물질적, 시간적 여유가 많을수록(Z) 노벨상(Y)을 많이 받는다.\"의 인과관계에서 Z variable 을 무시한채 해석하면 잘못된 정보를 학습할 수 있다는 것을 보여준다. 모델도 마찬가지다. 학습에 노출되는 데이터 분포에서는 Z가 명시적으로 보이지 않을 수 있다. 백인 남성이 상대적으로 경제적인 여유가 더 있어서 인터넷에 사진을 업로드하는 경우가 많았고, 이 때문에 웹에서 수집한 데이터 중의 대다수가 백인 남성일 수 밖에 없다고 했을 때, 이 사실은 데이터의 분포만 파악하는 모델의 입장에서는 학습하기 어려운 정보일 수 있다. 모든 bias 가 나쁠까? 꼭 그렇지는 않다. 때론 우리는 모델이 inductive bias 를 학습하길 바란다. 예를 들어, \"나비가 코끼리를 포식하지 않는다\" 는 정보는 모델 입장에서는 필요한 bias 이다. 그러나 social bias 와 같은 종류의 정보는 학습하지 않기를 기대한다. NLP에서는? 앞에서는 이미지를 예로 들었지만, 여러 논문에서도 NLP 모델이 social bias 를 학습하고 있다는 사실을 직/간접적적으로 보여주고 있다. 모델이 학습한 word embedding 을 분석해서 bias를 학습했음을 통해 보이거나[1][2], 학습 dataset 자체에 social bias 가 있음을 보이거나[3], bias 를 측정하는 task와 metric을 제안하여 얼만큼 bias를 학습했는지 를 수치적으로 평가[2][4][5][6]한다. 새로운 bias 측정 task를 제안할 때 수반되는 dataset이 있는데, 여기에도 구축 방식에 따라 두가지 종류로 나눌 수 있다. 하나는 templated-based dataset[4], 다른 하나는 crowdsourced dataset[5][6] 이다. 모델의 embedding space 분석 이 방법론은 PLM 이전, word2vec 이 많이 사용되었던 시기에 등장했다. word2vec을 통해 학습된 단어의 관계를 분석했을 때 social bias 가 얼마나 반영되었는지를 판단한다. 예를 들어, man - woman ~= computer programmer - homemaker 의 관계가 성립한다면 이 모델은 social bias 를 함유하고 있다고 볼 수 있다. Man is to computer programmer as woman is to homemaker? debiasing word embeddings[1]에서는 word2vec의 geometric bias에 대해 분석하였다. Figure 1의 왼쪽에는 she / he 와 가까이에 있는 직업 단어 목록이 있다. 그리고 각 직업 단어들을 crowd worker 에게 female-stereotypic, male-steretotypic, neutral 한지를 물었을 때의 결과와 corrleation을 구했고, 그 결과 0.51 정도의 moderate 한 결과가 나왔다. Figure 1의 오른쪽에는 she-he 의 관계와 유사한 단어들의 관계 목록을 보여주고 있다. 이 중에서 Gender stereotypical 한 analogy와 gender appropriate한 analogy의 수량을 비교하였고, 150 개의 관계 중 29개가 gender stereotypical 하다고 판단되었다. 데이터셋 자체에 함유된 Bias 학습한 모델을 분석하는 방법이 아닌, 학습하는 대상이 되는 데이터셋 자체를 model-agnostic 하게 분석하는 방법도 제안되었다. Social Bias in Elicited Natural Language Inferences[3]에서는 SNLI 데이터셋에 존재하는 bias 를 PMI 와 Likelihood ratio test of independence를 통해 통계적으로 분석하였다. 아래의 Table 1은 SNLI 데이터에서 gender, age, race/ethnicity/nationality 에 해당하는 단어들 각각에 대해 PMI가 높은 단어들을 나열해 놓은 것이다. woman &amp; man, girls &amp; boys, white man &amp; african american 등 차이가 없어야할 개념들에 대해 PMI 상 가까운 단어들을 비교해보면 차이가 있다. Table 2는 inference type 별로 gender words 들에 대해 PMI가 높은 순서대로 정렬한 결과다. 당연히 비슷할 수 밖에 없는 결과 - women &amp; woman, females - 를 제외하고 gender stereotypical 한 단어들을 살펴보면 gender bias가 사뭇 드러난다. women - chat, smile 은 entailed 관계인 반면, women - dicussing, politics 는 contradiction 관계에서 자주 등장한다. Bias evaluation task 앞서 소개한 두 방법은 모델 학습결과의 embedding space에서의 단어 분포와 데이터 자체의 특성을 이용한 통계학적 관점을 활용하였다. 이와 다르게 모델의 bias를 확인할 수 있는 방법으로 task based evaluation 이 있다. bias 를 판단할 수 있는 task 즉, dataset과 metric 을 설계해서 점수를 내고 비교하는 방식이다. 이는 다시, template based dataset 과 crowdsource-based dataset을 활용한 task로 나눠볼 수 있다. Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods[4]에서는 winograd schema 를 바탕으로 bias를 측정할 수 있는 task를 제안하였다. Winograd Schema[8]란, Terry Winograd 의 이름을 딴 schema 로, 아래의 예시와 같은 문장 형식을 일컫는다. The city councilmen refused the demonstrators a permit because they [feared/advocated] violence. 이 task의 정답을 맞추기 위해선, 대명사가 무엇을 가리키는지에 대한 이해가 필요하고, 이 이해는 councilment과 demonstrator의 관계에 대한 지식을 바탕으로 한다. 아래의 두가지 타입을 따르는 문장들이 bias를 테스트하기 위한 dataset으로 제안된다. [entity1], [entity2]는 Male 혹은 Female entity 들이고, [pronoun]은 he/she 의 대명사이다. 문장 내에서 [pronoun]과 [entity]의 관계를 보고, pro-stereotypical 한 관계를 anti-stereotypical 한 관계보다 선호한다면 bias가 있는 모델로 판단한다. StereoSet: Measuring stereotypical bias in pretrained language models[5] 은 4개의 bias domain - gender, profession, race, religion - 에 대해서 crowdsource 방식으로 수집된 데이터와 bias 측정 metric 을 제안하고 있다. crowdsource 방식은 template 기반 방식 대비 real-world 에 더 가깝게 데이터셋을 수집할 수 있다는 장점이 있다. StereoSet에서는 LM이 배워야 할 지식과 배우지 않길 기대하는 지식 (social bias) 을 구분해서 학습했는지 여부를 테스트한다. Context Association Test (CAT) 라고 명칭한 Test는 Intrasentence와 Intersentence 두 종류의 테스트로 구성되어 있다. 그리고 모델이 각 test에 대해 답변한 결과를 ranking problem (option 1을 선택한 비율이 option 2를 선택한 비율보다 많았는가, 적었는가) 으로 pose 시켜서 최종 bias score를 낸다. 앞서 언급했듯이, LM이 배워야 할 지식과 배우지 않길 기대하는 지식 (social bias) 을 구분해서 학습했는지 여부를 테스트하기 때문에 lms 와 ss 를 구분하였고, 이 두 점수를 종합해서 최종 CAT score 를 도출한다. Language Modeling Score (lms) model has to rank the meaningful association higher than meaningless associaton score: percentage of instances in which a language model prefers ideal: 100 Stereotype Score (ss) score: percentage examples in which a model prefers a stereotypical association over an anti-stereotypical association ideal: 50 Idealized CAT Score (icat) combine both lms and ss \\[icat = lms *\\frac{min(ss, 100-ss)}{50}\\] ideal: 100 (when lms: 100 and ss: 50) fully biased: 0 (when lms: 0, ss: 100 or 0) random model: 50 (when lms: 50, ss: 50) CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models[6]은 EMNLP 2020에 억셉된 논문으로, stereoset과 마찬가지로 template 기반이 아닌 crowndsourced 데이터셋으로 구성되어 있다. 크게 9가지의 bias에 대해 문장 하나는 stereotyping한 것, 다른 하나는 덜 stereotyping 한 것으로 구성한다. 두 문장의 거리는 매우 가까워야 한다. 평가의 경우, 차이가 있는 token을 제외한 나머지 token (=unmodified token)을 순차적으로 masking 해서 각각의 log-likelihood를 구한 뒤 그 합을 최종 점수로 가진다. Limitations on \"Bias in NLP\" researches Language (Technology) is Power: A Critical Survey of “Bias” in NLP[7]에서는 기존의 146개 \"bias in NLP\" paper에 대한 survey를 진행하면서 이전 연구들에 대한 비판과 이를 극복할 수 있는 방향으로의 연구를 제안하였다. 비판의 포인트는 크게 3가지이다. \"bias\"의 정의에 대한 논의 부재 \"bias\"로 인해 발생하는 문제에 대한 고민 부족 (movitations are often vague, inconsistent, and lacking in normative reasoning) 기존 학계에서 논의되고 있는 \"bias\"와의 약한 연결성 예를 들어, \"racial bias\"에 대한 다음 기존 연구들을 살펴보자. 다루고 있는 주제는 racial bias 지만 실제로는 보다 협소한 African-American English (AAE) 에 대해서 다루고 있다. (그래놓고 제목에 racial bias를 적어둔 것은 뭘까? Asian 차별에 대해서는?) 그리고 같은 AAE에 대해 다음과 같이 다양한 방식으로 bias 가 존재한다고 주장하고 있다. African American과 연관된 이름은 pleasant words보다 unpleasant words에 더 가까움 POS tagger, Language Identification, dependency parser 에서 AAE에 연관된 term이 포함되는 경우 덜 정확함 toxicity detection system 이 AAE 와 연관된 feature 가 있는 트윗을 더 offensive 하다고 판단내리는 경향이 있음 기존 연구들 중 어떤 것도 AAE 혹은 racial hierarchies in the US에서의 racial hierarchies, raciolinguistic 분석을 언급하지 않았다. 또, AAE를 누가 이야기하는가에 따라서도 bias 여부를 다르게 판단할 수 있으나 이에 대한 고려는 없이 오직 text 만 놓고 판단하였다. 어떤 맥락에서 AAE 가 문제가 될 수 있고, bias 되었다고 판단할 수 있는지에 대한 고려는 없었다. 개인적으로도 단순 데이터/모델 결과 분석의 방법론이 애매하다는 생각이 든다. bias가 있는 문장/단어에 대해 다른 문장/단어와 다른 결과를 도출한다고 이야기하고 있지만, 임의의 특성들에 대해서도 심층적으로 분석해보면 똑같은 결과가 나올 것 같다. 그리고 Crowdsourcing 으로 만들어진 dataset이 bias 가 없음을 보장하는 내용도 부족했다. US 에 살고 있는 사람들이 annotator 로 참여했지만 보수적인 주의 사람들이 더 많았다거나, bias에 대한 지식이 부족한 사람들이 더 많았다면 문제가 될 여지가 있다. So? 올바른 방식으로 bias in NLP 주제를 tackle 하기 위해서는 언제 Biased 모델이 문제를 일으키는지를 우선 고민해봐야할 것 같다. Biased model은 언제 문제가 될까? (Open Question) Bias 의 범위가 넓으므로 gender bias 에 국한해서 생각해보자. 만약 gender bias detection model 을 간단하게 KcBERT를 korean-hatespeech-dataset (gender bias) task에 finetuning[10] 한 것으로 사용한다고 했을 때, 모델이 틀리는 gender-biased 예문이 o.o.d test set 에서 많이 등장한다면 문제가 될까? 혹은 정답은 제대로 맞추었더라도 잘못된 단어를 queue로 받아서 맞추는 것이라면 문제이지 않을까? 문제가 된다면, 이 문제는 무엇으로부터 기인할까? KcBERT 가 학습한 데이터로부터 오는 문제 KcBERT를 finetuning 하는 방법으로부터 오는 문제 finetuning dataset 의 문제 제작 시 \"gender bias\"를 annotator 의 직관에 맡기기보다는 guideline을 바탕으로 tagging 되었다고 볼 수 있다. 따라서 annotator의 bias를 최소화했다고 보여진다. dataset 이 작기 때문에 real world 를 충분히 반영하지 못해서 나타나는 문제 Human-in-the-loop 으로 풀어볼 수 있지 않을까? static benchmark 가 쉽게 stale 해지는 것을 막기 위해 제안된 DynaBench[11]라는 dynamic benchmark framework을 참고하면 어떨까? DynaBench는 user들이 특정 task를 푸는 목적으로 학습된 모델이 틀리는 예문을 생성하고, 이를 바탕으로 다시 모델이 학습해서 지속적으로 발전할 수 있도록 만들어졌다. 간단하게 학습시킨 KcBERT finetuning model 을 통해 예측한 결과를 보았을 때 주어진 testset 과 생각해볼 수 있는 간단한 예문에 대해서는 나쁘지 않은 결과를 보임을 확인하였다. DynaBench를 benchmarking 한 웹사이트를 만들어보면 재밌는 결과를 수집할 수 있지 않을까? References 1.Man is to computer programmer as woman is to homemaker? debiasing word embeddings ↩︎ 2.Semantics derived automatically from language corpora contain human-like biases ↩︎ 3.Social Bias in Elicited Natural Language Inferences ↩︎ 4.Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods ↩︎ 5.StereoSet: Measuring stereotypical bias in pretrained language models ↩︎ 6.CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models ↩︎ 7.Language (Technology) is Power: A Critical Survey of “Bias” in NLP ↩︎ 8.https://en.wikipedia.org/wiki/Winograd_Schema_Challenge ↩︎ 9.https://kyunghyuncho.me/social-impacts-bias-of-ai/ ↩︎ 10.https://github.com/inmoonlight/detox/tree/master ↩︎ 11.https://dynabench.org/ ↩︎","link":"/2020/11/14/Social-bias-in-NLP-models/"},{"title":"빛의 과거","text":"새해를 맞이하자마자 작년의 목표와 기대에는 없던 일이 일어났다. 오랜만에 뵌 분께 은희경 작가님의 \"빛의 과거\"라는 책을 선물받은 것이다. 그 동안 읽었던 텍스트라고는 오직 논문이었기에 선물받자마자 들었던 생각은 \"아, 내가 과연 책을 읽을 수 있을까?\" 였다. 그 와중에 작가님의 성함이 눈에 띄었다. 은희경 작가님... 왜 이렇게 익숙한 이름인가 했더니 개인적으로 존경하고 흠모하는 언니로부터 추천받았던 작가님이었다는 것이 떠올랐다. 단지 이 작은 이유만으로 오랜만에 접하는 소설의 벽이 낮아지는 느낌이었다. 이 책을 읽을 운명이었던 것인지, 마침 다음 날은 주말이었고 미용실에 오랜만에 가기 위해 예약을 잡아두었다. 자리에 앉자마자 책을 펼쳐들었다. 그리고 단숨에 그 소설의 세계에 몰입하게 되었다. 내용 소개 그래도 공개적인 곳에 쓰는 글이니만큼, 적어도 이 글에 어쩌다 접근하게 된 누군가를 위해 줄거리를 적어두어야 할 것 같다. 주인공 '나'는 2017년 현재, 남편을 사별한 번역일을 하면서 살아가는 평범한 주부다. 1977년 대학생 시절 기숙사에서 처음으로 인연을 맺은 '친구'와 끊어질듯 끊어지지 않는 관계가 이어지고 그 '친구'와는 그렇게 친하지는 않지만 계속 연락을 하며 지낸다. 이 '친구'는 소설가다. 작가가 꿈이었던 적은 없었지만 여러 직업을 거쳐 여기에 이르렀다. 그리고 본격적인 이야기는 그 친구의 책을 읽는 것으로 시작된다. 책은 함께 기억을 공유하고 있는 대학생 시절을 배경으로 한다. 책의 구성 한 편의 영화와 같은 구성이었다. 1977년과 2017년을 오가며 이야기가 진행된다. 처음에는 \"흔한 구성\"이라고 생각했지만 읽어나가다보니 시간을 오가지 않으면 불가능했을 이야기였다. '2017년의 나'가 있었기 때문에 '1977년의 나'의 어리숙했던 부분을 어른의 시점에서 돌이켜 볼 수 있었고, '1977년의 나'가 있었기 때문에 어렸을 시절의 나의 시점에서 어리숙했을 수 밖에 없었던 일을 설득력있게 전달할 수 있었다. 그리고 1977년은 지금과는 달리 더 보수적인 시대였다. '1977년의 나'는 그 시대를 '여대생'의 시점에서 덤덤하게 이야기한다. '2017년의 나'였다면 시니컬한 톤이 묻어날 수 밖에 없지 않았을까? 마음에 닿았던 구절 회사의 관례에 따라 여성 기혼자에게 주어지는 계약직 전환 서류를 내 책상 위에 갖다 놓은 것도 그녀였다. - p.9 마음에 닿았던 구절이라고 했지, 그 구절이 감동적일 것이라는 이야기는 하지 않았다 :P '정말 이랬다고?' 라는 놀라움 때문에 바로 형광펜을 들이밀었다. 나는 이런 대우가 당연했었던 시절을 상상조차 할 수 없다. 끊어진 건 아니지만 밀착될 일도 없는, 간격이 불규칙한 점선 같은 관계였다. - p.11 '나'와 '친구'의 관계를 이야기하는 문장이다. 대학을 졸업하고 사회생활을 하는 사람들이라면 누구나 공감할 수 있는 관계일 것이다. 어떻게 이렇게 표현할 수 있을까 싶어서 밑줄을 그었다. 그러나 그녀에게는 사람을 대할 때 미묘한 권력관계를 만드는 습성이 있었다. 끊임없이 자신을 중심으로 돌아가는 관계의 자장을 만들어내고 우월감과 피해 의식을 번갈아 써가며 그것을 정당화했다. 거기에는 증인이 필요했다. 결국 나로 하여금 위성처럼 그녀의 궤도를 따라 돌며 그녀라는 일방적이고 변덕스러운 광원을 반사하도록 만들어 버리는 것이다. 나는 나대로 소심함과 자기 합리화의 조합인 어정쩡한 온검함 뒤에 숨어 그녀의 그런 태도를 순순히 받아들이곤 했다. 열정은 단호한 구석이 있어서 금세 꺾이지만 친근함은 어느 정도 안이한 감정이라서 사소한 기억의 공유만으로도 쉽게 환기되었다. - p.12 '나'와 '친구'의 관계를 묘사한 내용이다. 둘 사이의 미묘한 권력관계와 그 속에서 우월감을 느끼는 친구, 그리고 그 것이 느껴지지만 크게 동요하지 않고 순종적인 '나'의 모습이 참으로 세련되게 표현되었다. \"자신을 중심으로 돌아가는 관계의 자장\"과 \"소심함과 자기 합리화의 조합인 어정쩡한 온건함\". 부분적으로나마 모범생 흉내를 내서 그 시스템에 순종했고 그 대가로 서울의 한 여자대학에 합격하여 고향과 부모로부터 벗어날 수 있었다. - p.27 정말 나의 이야기였다. 고등학생 시절의 나도 이 책의 화자처럼 소심했다. 시스템을 거스를 힘과 배짱이 없었기 때문에 그 안에서 내가 할 수 있는 최대한의 반항을 했다. 그 곳에서 벗어나고 시스템을 바꿀 힘을 갖기 위해 좋은 대학을 가는 것. 분명 이 것은 1977년의 문장일텐데, 어째서 2010년에도 똑같은 모습이었던걸까. 그 때까지 다름이란 걸 전혀 겪어보지 못했냐고 묻는다면 그렇지는 않다. 12년간이나 중단 없이 지긋지긋했던 초중등학교 생활 속에서도 타인과 부딪힐 기회는 얼마든지 있었다. 그러나 내가 그 시절 겪없던 것은 다름이라기보다 수직적인 위계와 시비였다. 그때그때 적용되는 일관성 없는 규율이 있었고, 없으면 교사나 반장이나 힘센 애들이 만들었다. 남과 다른 것이 그대로 결격사유가 되는 단체 생활에서 내가 누군지 따위를 고민할 기회는 아무에게도 주어지지 않았다. -p. 27 이 페이지 속의 많은 문장에서 머물렀다. 나도 책의 주인공처럼 고등학교를 탈출하고 대학교에 합격을 하자마자 기숙사 생활을 했다. 6명이서 함께 사는 방이었고 한 방은 두 명이 함께 썼다. 합격과 입학이라는 들뜬 마음도 잠시, 나와 '다른' 누군가와 시간과 장소를 공유하는 것의 어려움이 크게 다가왔었다. 그 때는 이유를 몰랐지만 여기에 서술된 것처럼, 내가 고등학교 시절까지 경험했던 삶 속에서 진정한 '다름'에 대해 이해하는 시간은 전혀 없었기 때문이었던 것은 아니었을까. 혼자라는 건 어떤 공간을 혼자 차지하는게 아니라 타인의 시선에서 벗어나 익명으로 존재하는 시간을 뜻하는 거였다. -p. 84 단 한 번도 혼자 보내는 시간을 이렇게 정의해보지 않았었다. 사람들 속에 있으면서도 혼자라고 느끼는 이유는 그 들에게 난 철저히 '익명'의 누군가이기 때문이다. 이 글을 읽은 이후, 지하철에 있을 때마다 괜히 피식되게 된다. 나는 내 앞의 문을 열지 못하고 번번이 과거의 나로 굴러떨어지곤 했다. -p. 86 왜 자꾸 나의 대학생활이 떠오르는 것일까. 1977년이든, 2011년이든 청춘의 고민은 비슷했다. 그리고 여기에 적진 않았지만 행복의 순간도 비슷했다. 사람을 둘러싼 정치적, 사회적, 경제적, 기술적 상황은 너무나도 달라졌지만 사람은 결국 똑같았다. 나를 행복하게 만드는 선택과 행복을 느끼는 방법만 달라졌다. 첫인상 역시 두번째와 크게 다르지 않았다. -p. 287 이 문장은 반드시 두번 읽어야 한다. 무심결에 넘기면 평범한 문장이지만 전혀 그렇지 않다. 어떻게 첫인상이 두번째 인상 이후에 느껴질 수 있단 말인가?! 이 문장은 첫번째 만남을 기억하지 못한 채로 두번째 만남을 첫번째라고 생각했기 때문에 가능했다. 작가님의 재치에 빵- 터졌다. 개인적인 감상 작가님의 문체와 삶에서 느끼는 감각에 모두 공명할 수 있었다. 내가 느꼈던 \"그 감정\"을 거부감없이 재치있고 한편으로는 날카로운 문장으로 군더더기 없이 표현한다. 군더더기 없음은 자칫하면 82년생 김지영처럼 어떤 메시지를 품을 수 있는 배경의 소설을 담백하게 만든다. 책을 읽으면서 나는 1977년을 관찰하고 있지 않았다. 그 시대 속에 머물러 있으며 소설 속의 \"나\"가 겪어나가는 감정에 공감하기 위해 시대적 배경을 이해하려고 했다. 내가 77년대에 부자로 살았더라면, 똑똑한 남자였다면, 똑똑한 여자였다면, 소심한 남자였다면, 적극적인 여자였다면 어땠을까. 나의 기질과 나의 주어진 환경에 의해 2020년, 지금의 나와 다른 모습이었겠지. 책 속에서 사회가 남자에게 부여하는 모습은 능력있고 가족을 책임지고 이끌어나가는 가부장적인 것이었다. 반대로 여성은 자원이 부족하다면 언제든 가장 먼저 포기를 강요받은 대상이었고 남자의 내조를 위한 모습을 요구받았다. 불현듯 보수적인 할머니와 보수적인 집안에서 가족의 기대를 한몸에 받았던 아버지가, 그리고 이 보수적인 집안으로 시집와서 많은 것을 포기했던 어머니가 떠올랐다. 논문과는 다르게 소설은 \"그래서 앞으로 무엇을 하면 좋을까?\"에 대한 생각을 남기기보다는 내가 경험했지만 의식하지 않았던 회색 지대 속의 감정과 기억을 재구성하고 새롭게 의미를 붙여준다. 그래서 과거 속으로 탐험하는 기분이 들었고 특히나 대학생 시절이 많이 떠올랐다. 2020년은 내가 한국나이로 30대가 된 해이기도 하다. 개인적으로는 20대의 나를 돌이켜보기에 너무나도 좋았던 책이었다. 그 때 무엇을 어떻게 해야할지 몰랐던, 나만의 다름은 인지하지 못하고 다르다는 것이 두렵기만 했던 내가 지금은 이렇게 달라졌구나. 짜식 많이 컸구나. 수고했다.","link":"/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung/"},{"title":"노르웨이에서의 나홀로 여행: 피오르드, 브라운 치즈","text":"북유럽은 오로라를 보고 싶어서 겨울에 갈 곳으로 내심 정해두고 있었는데, 그럼에도 불구하고 한 여름에 노르웨이를 행선지로 정했던 까닭은 피오르드(Fjord) 였다. 웅장한 자연을 보길 좋아하는 편인데다가 (오로라만 봐도 알 수 있다) 덥고 습한 여름에서, 그리고 틀에 박힌듯한 답답한 삶에서 잠시 벗어나고 싶었다. 좀 더 찾아보니 노르웨이는 피오르드의 천국이었다. 대부분의 관광객이 찾는 피오르드이자 가장 길고 깊은[1] 송네피오르드(Sognefjord), 영화 &lt;&gt;의 배경이자 그 아름다움으로 UNESCO 세계 유산에 지정된 내로이피오르드(Nærøyfjord), 베르겐(Bergen)에서 플럼(Flåm)으로 가는 길에 송네피오르드를 거쳐 지나는 Aurlandsfjord[2] 등등. \"피오르드\"가 빙하로 만들어진 좁고 깊은 만을 뜻하다보니 노르웨이 곳곳에 피오르드로 끝나는 이름이 많았다. 나는 이번 여행에서 베르겐을 들르지 않고 플럼에서만 머무르기로 했기 때문에 Aurlandsfjord에서 내로이피오르드(Nærøyfjord)를 거쳐 운드레달(Undredal)에서 브라운 치즈를 체험할 수 있는 투어를 신청했다. ferry 투어와는 다르게 RIB(Rigid Inflated Boat) 투어는 야생의 투어에 가까웠다. 보트의 운전을 담당하는 가이드는 때때로 ferry가 가르고 간 물결 위를 지나며 스릴감을 주기도 하고, 피오르드 협곡에 사는 작은 고래를 만나면 근처에서 구경을 시켜주기도 했다. 이에 더해 그 지역에 대한 친절하고 자세한 설명도 함께 들을 수 있어서 노르웨이가 더 가깝게 느껴졌다. 피오르드는 정말 아름다웠다. 특히 내로이피오르드(Nærøyfjord)는 협곡의 곡선이 섬세했고 조화로웠다. 아름다움과 실용성은 반비례한다고 생각해왔는데, 피오르드도 마찬가지였다. 보기에는 아름답고 경이롭지만, 그 곳에서 살아가기 위해서는 비옥한 평지에서 살아가는 사람들에 비해 많은 노력이 필요했다. 특히 \"식\"을 해결하는 것이 가장 어려운 문제였다. 노르웨이는 경작할 수 있는 토지의 비율이 작다. 특히 피오르드 근처는 그 비율의 1/10이다. 이런 척박함 덕분에 석유의 발견으로 지금과 같이 부유한 국가가 되기 전에는 많은 사람들이 미국으로 이민을 갔다고 한다. 남은 사람들은 농업대신 낙농업으로 생계를 유지해 나갔다. 이 곳의 낙농업은 다른 곳에 비해 특이한 점이 있었다. 하나는 염소 치즈가 주를 이룬다는 점, 다른 하나는 브라운 치즈라는 독특한 치즈가 발전되었다는 점. 염소 치즈가 소 치즈보다 발달한 이유는 염소가 산을 잘 타기 때문이다. 추운 지방이다 보니 여름과 겨울에 풀을 먹을 수 있는 면적이 눈에 띄게 달라진다. 눈이 녹을 시기에는 그 기회를 십분 활용해 좀 더 위에서 풀을 먹는 것이 이득이다. 다행히 염소의 천적은 이 추위를 견디며 살 수 없어서 염소 떼를 풀어두기만 하면 알아서 산을 올라 제일 맛있는 풀을 알아서 뜯어먹고 안전히 마을로 귀가한다고 한다. 브라운 치즈는 우리가 흔히 아는 화이트 치즈의 잔여물로 만든 독특한 치즈다. 아무리 낙농업이 발달했다고 하더라도 그 양이 충분하지 않았기 때문에 사람들은 일반 화이트 치즈를 만들고 남은 것도 식량으로 활용해야 했다. 잔여물은 \"유청\"이라고 불리는 것인데 이를 충분히 졸이면 갈색으로 변하고 카라멜처럼 단 맛이 난다. 여기에 얼마 되지 않는 소의 젖으로 만든 크림을 1:1 비율로 섞어주면 브라운 치즈가 된다. 화이트 치즈를 만들 때 지방이 많이 빠져나가다보니 브라운 치즈는 지방이 덜 함유되어 있다. 브라운 치즈는 노르웨이에서 많이 파는 크래커같은 빵에 버터를 발라 그 위에 얹고, 그 지방의 잼을 발라먹는다. 마트에서 파는 브라운 치즈는 인공적인 향이 가미되어 있어 좀 더 단 맛이 많이 난다. 내로이피오르드의 끝에는 구드방겐(Gudvangen)이라는 마을이 있었고, 운드레달(Undredal)과는 달리 바이킹 족의 전통을 간직하고 있었다. 집의 지붕은 풀이 덮어져있었고[3], 어린 아이들이 (특히 남자) 낯선 이를 따라가지 못하도록 \"트롤이 너를 잡아먹는다!\"며 겁을 줬다고 한다. 놀랍게도 이 트롤은 노르웨이의 유명한 극작가 헨릭크 입센의 소설 &lt;&gt;에 처음으로 등장했다고 한다. 주인공 페르 귄트가 산 속에서 판타지스러운 여정을 할 때 초록 옷을 입은 여자가 나타나 그를 유혹했는데 알고보니 트롤의 딸이었다고 한다. 그 이후 트롤은 판타지 소설 속에서 괴기스럽게 발전되기도 하고, &lt;&gt;에 등장하는 트롤처럼 귀여운 모습으로도 발전했다. 생각보다 노르웨이 사람들의 입센 사랑은 엄청났다. 릴리함메르(Lillehammer)라는 지역에서는 매년 8월 초마다 &lt;&lt;페르귄트(Peer Gynt)&gt;&gt; 페스티벌을 연다. 노르웨이의 자연을 배경으로 야외에서 하는 연극이 가장 인기가 많다. 그리그의 &lt;&gt;으로만 접했던 페르 귄트의 이야기가 궁금해서, 그리고 그들 고유의 축제가 궁금해서, 나도 한 번 연극 티켓을 구매했다. 이에 대한 내용은 언젠가... 1.적어도 나의 가이드에 따르면, 가장 긴 피오르드는 사실 Greenland에 있는 피오르드라고 한다. 하지만 사람이 살지 않기 때문에 안 쳐준다고... ↩︎ 2.한국어로 어떻게 표기하는 것이 옳은지 몰라 영어로 남겨두었다. ↩︎ 3.나의 가이드에 따르면, 풀을 덮은 이유는 지붕을 더 단단히 엮기 위함과 아름다움(?!) 때문이라고 한다. ↩︎","link":"/2019/08/09/Travel-to-Norway-fjord-brown-cheese/"},{"title":"연울림 이야기","text":"연울림의 연은, 이야기할 연이다. 이야기의 힘을 믿기에 나올 수 있었던 기획이다. 우리, 날리다: 나를 알리다 팀은 기본적으로 모든 사람들은 본연의 색을 가지고 있다고 믿고 있다. 지금 이 순간에도 사람들은 각자의 방식으로 현상을 인식하고, 이해하고, 느끼고 있다. 그리고 나만이 경험하는 특별한 시간을 보낸다. 그 속에서 우리는 각자의 가치관과 각자의 고민을 가지게 된다. 평소에 이런 개인적인 생각들은 수면 위로 잘 드러나지 않는다. 가치관을 가감 없이 이야기하기엔 사회에서 수용 가능한 범위가 제한적이고, 고민을 숨김없이 이야기하기엔 나의 고민을 진심으로 귀 기울여줄 사람이 부족하다. 그래서 연울림이 기획되었다. 연울림의 연(讌)은 이야기할 연이고, 울림은 다양한 사람들의 이야기가 공명하는 순간을 담은 단어이다. 연(讌) 이야기를 통해 각자의 색이 드러난다. 다른 사람들에게 내가 가진 생각과 고민을 전달하는 과정에서 추상적인 형태로 남아있던 경험이 나만의 언어로 뚜렷하게 묘사된다. 그때 나는 왜 그런 생각을 하게 되었는지, 나는 왜 그렇게 밖에 할 수 없었는지를 이야기함으로써 그때의 나와 그때의 상황을 이해한다. 울림 이야기는, 그것을 진심으로 들어주는 사람들이 있을 때 힘을 가진다. 나의 가치관과 타인의 가치관이 만나 서로의 가치관에 영향을 줌으로써 확장되고, 내가 가진 고민을 다른 사람의 시선에서 바라봄으로써 새롭게 나의 고민을 바라보는 시각을 제공받을뿐더러, 나를 이해해주고 편견 없이 받아주기 때문에 위로받는다. 그래서 나의 이야기는 다시금 새로운 의미를 가진 채로 나에게 다가온다. 이야기와 이야기가 만나 공명하는 순간을 만들어가고 싶다. 모든 이야기는 저마다의 가치가 있고, 나눌수록 더 큰 힘을 발휘할 수 있다.","link":"/2019/04/01/Yeonullim/"},{"title":"Zero to One (제로투원)","text":"제로투원. 해석하면 0에서 1. 즉, 무에서 유를 창조한다는 의미다. 책의 제목에 걸맞게 주변에 스타트업에 다니는 분들에게 자주 추천받았던 책이다. 내용 소개 스타트업이 성공하기 위한 조건에 대해 이야기하고 있다. 흔히들 \"시장우위를 점해서 경쟁에서 승리한 스타트업\"을 성공한 스타트업이라고 생각한다. 하지만 저자는 할 수 있다면 경쟁은 피할수록 좋다고 말한다. 경쟁 때문에 라이벌 회사를 사용자보다 더 신경쓰게 되면 더 이상 서비스가 사용자를 향하지 못하기 때문이다. 저자에게 경쟁이란, 아무도 이윤을 얻지 못하고 의미 있게 차별화 되는 부분도 없이 생존을 위해 싸우는 것이다. 그렇다면 저자는 성공하는 스타트업을 어떤 기업으로 생각하고 있을까? 아래의 글에 그 답이 있다. 모든 기업은 남들이 할 수 없는 것을 해내는 만큼, 딱 그만큼만 성공할 수 있다. 창조적 독점이란, 새로운 제품을 만들어서 모든 사람에게 혜택을 주는 동시에 그 제품을 만든 사람은 지속 가능한 이윤을 얻는 것이다. 행복한 기업들은 다들 서로 다르다. 다들 독특한 문제를 해결해 독점을 구축했기 때문이다. 반면에 실패한 기업들은 한결같다. 경쟁을 벗어나지 못한 것이다. 마음에 닿았던 구절 대학생들은 몇몇 전공 분야에서는 고도의 전문적 기술을 습득하기도 하지만, 정작 그 능력으로 더 넓은 세상에서 무엇을 할 수 있는지에 관해서는 아무것도 배우지 못한다. 책의 주제와는 상관없지만, 이 문장에 공감하지 않을 수 없었다. 교육이 사회를 쫓아가지 못하는 것은 미국이나 한국이나 다를 바 없다는 사실이 인상깊었다. 지속적인 가치를 창출하고 또 보유하고 싶다면, 차별화되지 않는 제품으로 회사를 차리지 마라. 연울림을 런칭했을 때의 경험이 떠올랐다. 사용자에게 주고 싶은 가치가 있었지만, 나와 비슷한 생각으로 이미 사람들에게 가치를 전파하고 있는 기업이 어럿있었다. 아이디어가 비슷하더라도, 남들과 다르게 구현했다면 차별화된 제품이라고 부를 수 있었겠지만 그러기가 쉽지 않았고 끝내 경쟁 속에서 뒤쳐졌다. 사실 가벼운 동아리 같은 느낌이었기 때문에 나쁘지 않은 경험이라고 생각되지만, 다음에 무엇인가 시작하게 된다면 (사업이든 프로젝트이든) 차별성을 염두에 두어야겠다는 생각이 들었다. 신생기업에게 완벽한 표적 시장은 경쟁자가 없거나 아주 적으면서도 특정한 사람들이 적은 규모로 모여 있는 시장이다. 틈새시장을 만들어내 지배하게 되었다면, 관련 있는 좀 더 넓은 시장으로 서서히 사업을 확장해야 한다. 이 주장은 의견이 다를 수 있다고 생각한다. 컨설팅 회사에서 일해본 경험이 있는 사람이라면, 신사업 제안을 할 때의 논리와 절대적으로 반대되는 내용이라는 것을 알 것이다. \"시장의 규모가 크고, 그 시장의 경쟁상대가 누구이며 우리는 그 안에서 어떤 포지셔닝이기 때문에 시장의 n% 를 차지할 수 있다.\" 라는 논리로 이 신사업의 가능성을 타진하는 것이 일반적이다. 그런 면에서 이 문장은 신선했고, 제로투원이라는 제목에 걸맞는 주장이라는 생각이 들었다. 어려운 일은 성취할 수 있지만, 불가능한 일은 성취할 수 없다 가능성에 대해서 생각해보게 만든 글귀다. 아무도 하지 않고 있는 중요한 일을 왜 우리가 하고 있는지 설명할 수 있어야 한다. 새로운 프로젝트, 혹은 사업을 한다면 스스로에게 물어봐야 하는 질문이라는 생각이 들었다. 왜 이 문제인지, 왜 우리의 해결책이어야 하는지에 대해 나부터 납득시키지 못한다면 누구를 설득할 수 있을까? 사회를 위해서 정말로 좋은 일은 뭔가 남들과 ‘다른’ 일을 하는 것이다. 그리고 그렇게 하는 것이야말로 기업이 새로운 시장을 독점해 이윤을 만드는 방법이기도 하다. 최고의 프로젝트는 다들 떠들어대는 것이 아니라 남들에게 간과되고 있을 가능성이 크다. 가장 덤벼볼 만한 문제는 아무도 해결해보려고 하지조차 않는 문제일 때가 많다. 독자 기술은 가장 가까운 대체 기술보다 중요한 부분에서 ‘10배’는 더 뛰어나야 진정한 독점적 우위를 확보할 수 있다. 아무도 해결해보려고 하지 않는 문제는 불가능한 문제이거나, 어려운 일일 것이다. 그리고 어려운 일을 풀 수 있는 기업일수록 더 뛰어난 역량을 보유할 것이고, 남들이 쉽게 접근하지 못하는 독점적 우위에 가까운 포지션에 있지 않을까? 개인적인 감상 흔히 생각하는 성공하는 사업에 대한 조건의 틀에서 빠져나오게 만든 책이었다. 누구나 생각할 수 있는 문제와, 누구나 풀 수 있는 해결책이라면 그보다 위험한 스타트업은 없을 것 같다. \"누구나 공감하는 크고 어려운 문제\"에 대해 \"나만의 해결책\"을 가질 수 있는 사람이고 싶다.","link":"/2020/07/20/Zero-to-one-by-peter/"},{"title":"With Little Power Comes Great Responsibility","text":"요즘 등장하는 NLP model 페이퍼들은 주로 GLUE 벤치마크에 성능을 report 하면서 아주 미세한 성능 개선을 근거로 \"우리 방법론은 효과적이었다!\"를 주장하고 있다. 과연 이 결과가 실제로 그 모델이 더 나은 모델임을 주장할 수 있을만큼 근거가 탄탄할까? 이번에 소개하는 논문에서는 NLP research에서 모델의 성능 개선을 주장하는 실험 결과에 대해 그 결과가 \"정말 유의미한 모델의 성능 개선을 보장할 수 있는가?\"에 대해 분석한다. 더불어, 분석 결과를 통해 발견된 문제점을 개선할 수 있는 간단한 overview 까지 제안하고 있다. TMI 참고로 이 논문의 제목은 Spiderman의 유명한 대사에서 유래했다. 제목에서부터 느껴지는 현 시대의 NLP model evaluation에 대한 강한 부정적인 뉘양스는 논문의 교신저자가 Dan Jurafsky 이기에 가능하지 않았을까 싶다. Introduction 본격적인 분석 방법론 소개에 앞서 앞으로 등장하게 될 용어에 대해 간단하게 짚고 넘어가려고 한다. Power Power 란, 샘플 데이터에서 관측한 결과가 true distribution 데이터에 대해서도 적용될 수 있는지에 대한 확률로, \"통계적 유의미함\"과는 관련은 있으나, 다른 metric이다. 뒤에서 언급하겠지만, Power는 r 번의 반복수행 동안 통계적으로 유의미한 결과가 몇번 등장했는가를 바탕으로 측정된다. Type-S error sign 에 대한 에러. 쉬운 예를 들면, 실제로 모델 A가 모델 B에 비해 성능이 좋은데 실험 결과는 반대로 나오는 경우에 해당한다. Type-M error Magnitude에 대한 에러. 예를 들어, 실제 모델의 예측값의 차이가 적은데 관측된 결과에 따르면 예측값의 차이가 큰 경우에 해당한다. MDE (minimum detectable effect) size 유의미한 성능 차이(effect)를 보장할 수 있는 최소 데이터 사이즈. 이 논문에서 유의미한 effect는 80% Power를 의미한다. 만약 테스트셋의 사이즈가 작으면 MDE는 커진다. 이 용어들을 활용해서 다시 본 논문의 contribution을 정리하면 다음과 같다. NLP 커뮤니티에서의 실험 결과에 대한 Power를 고려하지 않는 실험 세팅과 결과 report 때문에 statistical noise와 유의미한 모델의 성능 향상을 구분하기 어렵다. Underpowered result는 실험 결과를 과장하거나, 실제 효과를 반대로 해석할 수 있는 여지가 있다. 너무 적은 샘플에 대해서는 유의미한 차이를 판단하기 어렵고, 유의미한 차이를 보이는 경우이더라도 샘플 수가 적을수록 그 효과를 과장해서 평가할 수 있다. 따라서 모델을 평가할 때에 주어진 조건들 - 테스트셋의 크기, 평가자 인원수 등 - 에 대해 결과가 유의미함을 보장하는지 생각해 볼 필요가 있다. 이를 위해 기존의 평가 framework에 대한 Power Analysis 가 필요하고, 동시에 유의미한 성능 개선을 보여줄 수 있는 조건에 대해서도 논의될 필요가 있다. 먼저, NLP 에서의 Power Analysis에 대해 알아보자. Power Analysis for NLP NLP의 task의 평가 format은 그동안 다른 과학적 방법론에서 적용되는 Power Analysis를 수행하기에 적합하지 않다. NLP 평가의 모든 시나리오를 커버할 수 없기 때문에 저자들은 최대한 일반화 가능한 simluation 기반의 power analysis를 제안한다. 저자들이 제안하는 Power Analysis 알고리즘은 다음과 같다. Simulation을 하기 위해서는 특정 parameter들이 요구된다. 저자들은 총 6개의 parameter를 제안하고 각각은 다음과 같다. \\(n\\): 평가에 사용되는 데이터의 개수 \\(e*\\): 관심있는 평가 수치 (e.g., \\(\\Delta_{acc}\\) 등) \\(h\\): 관련있는 다른 수치 (e.g., variance 등) \\(T\\): 유의미성을 판단할 수 있는 statistical test \\(\\alpha\\): 유의미성을 판단할 수 있는 threshold \\(r\\): 반복 수행 횟수 알고리즘을 간단하게 설명하면, 총 r번의 실험에 대해 n 개의 데이터에 대한 평가 결과를 생성하고 이 결과를 바탕으로 significance test를 수행해서 p-value 를 얻는다. 최종적으로 유의미한 결과 (p-value가 특정 threshold 이하인 경우)의 횟수가 r번 중 몇번인지를 바탕으로 power 를 계산한다. 아래는 위 알고리즘을 바탕으로 power analysis를 수행한 예시이다. 제일 왼쪽의 이미지는 실제 정답으로, B모델이 A모델에 비해 65% 정도 선호되는 것을 나타낸다. 두 모델의 선호도가 같음(50%)을 기준으로 약 15% 정도 더 선호된다고 볼 수 있다. 가운데 이미지는 \\(r\\)=10, \\(n\\)=100인 관측 결과이다. 샘플 수가 많기 때문에 10번의 trial 중, 유의미하게 B모델이 A모델보다 선호되는 경우가 8번으로 power는 80%라고 할 수 있다. 반면 샘플 수가 적은 마지막 이미지는 유의미한 trial은 3번으로 power는 30%라고 판단된다. 또한 샘플 수가 많은 경우에 유의미한 결과의 평균(B 모델이 선호되는 정도)과 null hypothesis(50%)의 차이가 실제 분포의 차이와 유사하다 (파란색 라인과 검은색 라인의 차이). 샘플 수가 적은 경우, 그 차이가 과장된다 (Type-M error). 이제 위의 Power Analysis가 여러 NLP 평가 시나리오에서 어떻게 수행되는지에 대한 예시를 보여준다. 크게 classfication task와 generation task, 그리고 human evaluation 시나리오를 보여주는데, 이 글에서는 그 중 classification task와 human evaluation에 대해서 중점적으로 다뤄보고자 한다. Classification Task Classification Task를 수행하는 모델들의 Accuracy 차이가 유의미한지를 판단한다. 위의 Power Analysis 시나리오를 만들기 위해서 필요한 parameter를 소개한다. \\(n\\): 100, 500, ... \\(T\\) (Significance test) classification 결과의 유의미성을 확인하는 statistical test로 가장 많이 쓰이고 있는 것은 McNemar's test로 모델 간의 불일치하는 정도를 바탕으로 두 모델의 차이에 대한 유의성을 검증한다. \\[\\chi^2 = \\frac{(p_{10} - p_{01})^2}{p_{10} + p_{01}}\\] \\(e*\\) (관심있는 평가 수치) 모델의 accuracy 차이 (\\(\\Delta_{acc}\\))와 두 모델이 같은 결과를 예측할 확률 (\\(P_a\\)) 을 제안한다. 예를 들어, 모델 A가 baseline과 비교했을 때 0.02 만큼의 accuracy 차이를 보이고, 90%의 예측 결과가 일치한다고 하자. 이는 다시 말해 10%의 결과는 다르다는 것이며, 2%라는 성능 차이에 따라서 6%는 모델 A가 맞고, 4%는 baseline의 결과가 맞을 것이다. with-little-power-table-4.png \\(\\alpha\\): 0.05 위의 파라미터에서 \\(n\\)을 100부터 5000까지 늘리면서 실험한 결과는 다음과 같다. 두 모델이 같은 결과를 예측할 확률 (\\(P_a\\)) 이 낮을수록 두 모델의 성능차이(\\(\\Delta_{acc}\\))가 적게 나는 경우는 상식적으로 발생하기 어렵다. \\(n\\)이 5000으로 많은 경우에도 80% Power를 얻는 시점의 (\\(\\Delta_{acc}\\))는 \\(P_a\\)가 높을 때가 더 낮다. \\(n\\)이 작은 경우에는 더더욱 유의미한 성능차이를 주장하기 위해 필요한 성능 폭이 증가한다. Assessing power in the literature 위의 결과를 바탕으로 다음의 질문에 대한 답을 할 수 있다. 주어진 테스트셋에 대한 baseline 대비 유의미한 성능 차이는 어느 정도일까? GLUE와 SQuAD 2.0 에 대해 결과를 report한 모델들의 baseline 대비 평균 accuracy 차이 (\\(|\\Delta_{acc}|\\)) 를 구한다. 이를 바탕으로 선형회귀 방식을 통해 구해진 \\(P_a\\)를 도출하며, 도출된 두 값과 테스트셋의 사이즈를 바탕으로 80% power를 만족(MDE; minimum detectable effect)하는 \\(\\Delta_{acc}\\)를 구한다. Est. MDE 보다 \\(|\\Delta_{acc}|\\)이 낮은 task들(WNLI, MRPC, SST-2)은 성능 개선을 주장하는 대부분의 모델이 실제로 그 task를 더 잘한다고 말할 수 없다. 평균 값이므로, 만약 어떤 모델이 Est. MDE 보다 높은 성능 개선을 보여주었다고 하더라도 데이터셋의 크기가 작기 때문에 Type-M error를 의심하게 된다. 저자들은 이 분석 결과를 두고 다음과 같이 강하게 주장하고 있다. (패기무엇) In extreme cases, such as MRPC and SST-2, it is worth considering whether it is time to retire these datasets as the basis for model comparison. Likert-Scale Human Evaluations 대화와 같은 NLG 태스크에서는 적절한 evaluation metric 이 부재하기 때문에 주로 Human evaluation 결과를 report한다. Human evaluation 결과도 다양하게 measure되는데 여기서는 그 중에서 Likert-scale 로 report하는 경우에 대해서 power analysis를 수행하였다. Meta-analysis Power Analysis에 앞서 논문에서 수행한 Human evaluation에 대해 간단한 통계량을 정리하였다. 놀랍게도 69%의 논문에서 100개 미만의 테스트셋을 사용했고, 18%만이 200개 이상의 테스트셋을 사용했다. 또한 34%의 논문에서 테스트 데이터 건당 rating 수를 제공하지 않았고, 28%의 논문에서 전체 어노테이터 수를 공개하지 않았다. 57%의 실험에서 아이템 당 3명의 어노테이터를 두었다. 정리하면, typical 한 성능 평가 시나리오는 3명의 작업자가 100개의 결과를 평가하는 것이다. Power analysis for human Likert ratings Power analysis를 위해서는 앞서 언급한 simulation을 수행해야 한다. 여기에는 작업자의 편차, 작업자 수, 평가한 문장 수 등의 정보가 필요하다. 이 parameters를 도출하기 위해 저자들은 기존 논문에서 report한 성능 평가 데이터를 기반으로 hierarchical mixed effects model을 사용한다. 그리고 도출된 parameter를 바탕으로 power analysis를 수행해서 아래의 그래프를 얻는다. 이 그래프를 통해 충분한 power를 가진 성능 평가를 위해 몇명의 어노테이터와 몇개의 평가 인스턴스가 필요한가? 에 대한 답을 알 수 있다. 분석 결과를 정리하면 다음과 같다. 대부분의 human evaluation은 underpowered 일 가능성이 높다. 가장 흔한 평가 방식은 3명의 어노테이터에게 100개의 데이터를 평가시키는 것이다. 보수적으로 작업자들의 편차가 크다고 한다면 유의미한 차이를 보이는 점수차이는 [0,1]로 normalize 시켰을 때 0.2 이상이어야 한다. 일반적인 시나리오에서 작업자들의 편차가 적더라도, 작은 성능 차이를 유의미하다고 인지하기에는 부족하다 일반적인 시나리오(3명의 어노테이터, 100개의 데이터)에서 작업자들의 편차가 작다고 가정하더라도 0.1 이상의 점수차이여야 유의미하다고 볼 수 있다. 10명의 작업자들이 500개의 데이터에 평가를 한다고 할 때에야 0.05 정도의 차이만으로도 유의미하다고 이야기할 수 있다. 대부분의 human evaluation은 제대로 결과를 report하지 않는다. 즉, 평가의 디테일이 빠져있다. 그러나 (안타깝게도) 대부분의 성능 결과에서 위의 정보를 제공하지 않아 애초에 평가의 유의미함을 제대로 판단하지 못한다. Overall Recommendations baseline 결과와 비교하기 전에 power analysis 가 선행되어야 한다. Underpowered 실험은 개선되었다고 주장하면 안된다. 새로운 데이터셋과 shared task에서 결정하는 데이터셋 사이즈는 MDE를 고려해서 결정되어야 한다. GLUE task에서 성능 개선의 유의미함을 판단할 수 없는 MRPC, SST-2의 task는 평가 대상에서 빠지거나 테스트셋을 확장시켜야 한다. Power analysis를 위해 fine-tuned model의 checkpoint가 공개되어야 한다. Anonymized human evaluation 결과가 공유되어야 하고, human evalutation을 수행함에 앞서 적절한 sample size를 도출하기 위한 power analysis가 필요하다. References https://arxiv.org/abs/2010.06595","link":"/2020/12/13/With-little-power-comes-great-responsibiltiy/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"essay","slug":"essay","link":"/tags/essay/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"quantum","slug":"quantum","link":"/tags/quantum/"},{"name":"quantum computing","slug":"quantum-computing","link":"/tags/quantum-computing/"},{"name":"LM","slug":"LM","link":"/tags/LM/"},{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"dataset","slug":"dataset","link":"/tags/dataset/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"mentoring","slug":"mentoring","link":"/tags/mentoring/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"},{"name":"book","slug":"book","link":"/tags/book/"},{"name":"data analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"news comments","slug":"news-comments","link":"/tags/news-comments/"},{"name":"social good","slug":"social-good","link":"/tags/social-good/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"hate speech","slug":"hate-speech","link":"/tags/hate-speech/"},{"name":"bias","slug":"bias","link":"/tags/bias/"},{"name":"travel","slug":"travel","link":"/tags/travel/"},{"name":"norway","slug":"norway","link":"/tags/norway/"},{"name":"evaluation","slug":"evaluation","link":"/tags/evaluation/"}],"categories":[{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"Essay","slug":"Essay","link":"/categories/Essay/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Quantum Computing","slug":"ML/Quantum-Computing","link":"/categories/ML/Quantum-Computing/"},{"name":"NLP","slug":"ML/NLP","link":"/categories/ML/NLP/"},{"name":"Ops","slug":"Ops","link":"/categories/Ops/"},{"name":"Book","slug":"Book","link":"/categories/Book/"},{"name":"Git","slug":"Ops/Git","link":"/categories/Ops/Git/"},{"name":"Data Analysis","slug":"ML/Data-Analysis","link":"/categories/ML/Data-Analysis/"},{"name":"PyTorch","slug":"ML/PyTorch","link":"/categories/ML/PyTorch/"}]}