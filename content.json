{"pages":[{"title":"Hello,  안녕하세요  👋","text":"I'm Jihyung Moon, Co-founder &amp; CTO at SoftlyAI We build AI junior employees so that people can focus on bigger problems. 💹 For investors: AILookUp | 🧑‍💻 For SaaS makers: AI Receptionist API My current interests are Building a great AI product Effective and efficient AI product development Scale Work and Education [JAN 2022 ~ ] Co-founder &amp; CTO, SoftlyAI [OCT 2020 ~ DEC 2021] NLP Research Engineer, Upstage [DEC 2018 ~ OCT 2020] NLP Research Engineer, Papago, NAVER Corporation [FEB 2018 ~ DEC 2018] ML Research Engineer, Search Modeling, Search &amp; Clova, NAVER Corporation [MAR 2016 ~ FEB 2018] M.S., Datamining Lab, Seoul National University [MAR 2011 ~ FEB 2016] B.S., Major in Chemical and Biological Engineering and minor in Industrial Engineering, Seoul National University Publications Analyzing Norm Violations in Real-Time Live-Streaming Chat. EMNLP 2023 [paper] Jihyung Moon*, Dong-Ho Lee*, Hyundong J. Cho, Woojeong Jin, Chan Young Park, Minwoo Kim, Jay Pujara and Sungjoon Park KOLD: Korean Offensive Language Dataset. EMNLP 2022 [paper] Younghoon Jeong, Juhyun Oh, Jaimeen Ahn, Jongwon Lee, Jihyung Moon, Sungjoon Park, and Alice Oh KLUE: Korean Language Understanding Evaluation. NeurIPS 2021 Datasets and Benchmarks Track [paper] [github] Sungjoon Park*, Jihyung Moon *, Sungdong Kim*, Won Ik Cho*, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho PATQUEST: Papago Translation Quality Estimation. Proceedings of the Fifth Conference on Machine Translation [paper] Yujin Baek*, Zae Myung Kim*, Jihyung Moon, Hyunjoong Kim, and Eunjeong L. Park BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection. SocialNLP@ACL 2020 [paper] [github] [slide] Jihyung Moon *, Won Ik Cho*, and Junbum Lee Revisiting Round-Trip Translation for Quality Estimation. EAMT 2020 [paper] Jihyung Moon, Hyunchang Cho, and Eunjeong L. Park Talks AI and Marginarlized Language. July 2023. ICML Panel Discussion. KLUE and XTREME. Sep 2021. XTREME Talks (Google Internal Seminar Series). KLUE: Korean Language Understanding Evaluation. Sep 2021. BigScience Episode #2. [youtube] [AI와 저작권법] 내가 만든 AI 모델은 합법일까, 불법일까?. Feb 2021. Boostcamp AI Tech. 서비스 관점에서의 AI 모델 개발. Nov 2020. 멋쟁이 사자처럼. 이 선 넘으면 침범이야, BEEP!. Sep 2020. PyCon. [slide] [youtube] BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection. July 2020. 카카오브레인. [slide] 파파고가 언어를 배우는 방법. June 2020. 인문학도를 위한 언어과학 콜로키움, 서울대학교 동아문화연구소. [slide] 온라인 뉴스 댓글 생태계를 흐리는 어뷰저 분석기. Oct 2019. 데이터야놀자. [slide] Academic Services Reviewer for EMNLP 2021, EMNLP 2022, ACL 2023, and more Patents 활동 데이터 분석을 통해 비정상 사용자 그룹을 탐지하는 방법 및 시스템 (METHOD AND SYSTEM FOR DETECTING ABUSER USING LOG DATA ANALYSIS), KR 1022091000000, filed 5 Sep 2018, issued 22 Jan 2021. [kipris] 김태욱, 문지형, 최인식, 박효균 Projects [NOV 2018] PyTorchTutorial for Beginners [github] [JUL 2017] Tensorflow Implementation of Relation Network (RN) [github] [JAN 2017 ~ FEB 2017] 구글 여성 소프트웨어 캠프 (Develop with Google) 1기 Things you don't need to know Emoticon creator, debuted with \"Daily life of SOTA, an ML Research Engineer\"","link":"/about/index.html"}],"posts":[{"title":"한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기","text":"약 4-5개월동안 사이드로 진행했던 혐오 댓글 프로젝트[1]가 성공적으로 마무리되었다. 같은 문제의식을 가진 사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이 무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는 활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고 원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의 상호보완적인 역량 덕분이 아니었을까 싶다. 사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한 이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작 작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던 부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는 사람들에게도 좋은 팁이 되지 않을까? 어노테이션 왜 편견과 혐오인가? 어노테이션 가이드라인에도 나와있듯이 우리는 크게 편견과 혐오라는 두 가지 aspect에 대해서 label을 수집했다. 처음에는 성에 관련된 편견 및 혐오와 그 외의 편견 및 혐오로 나누었는데, 이보다는 편견과 혐오로 구분하는 것이 낫다는 판단을 했다. 가이드라인 작성을 위해 댓글을 직접 태깅하다 보니 편견만 존재하는 댓글과 혐오만 존재하는 댓글이 존재했다. 항상 혐오가 편견으로부터 시작되지는 않았고, 편견이 있음을 부끄러워하지 않고 세상의 진리인 것처럼 이야기하는 댓글이 보였다. 혐오가 편견으로 시작된 경우는, 무리하게 개인의 특성을 집단의 특성으로 확장해서 그 집단에 대한 혐오를 개인에게 표출할 때였다. 그래서 이 둘의 관계를 데이터로 파악할 수 있도록, 또 편견과 혐오를 구분지어 생각할 수 있도록 편견에 관련된 label과 혐오에 관련된 label을 구분짓기로 했다. 언어학에 관심있는 사람들이라면 label을 바탕으로 댓글을 분석하는 것으로도 재밌는 연구가 될 것 같다. 표현의 자유와 혐오의 경계 이 둘을 구분짓는 좋은 threshold를 결정하는 것은 무슨 목적으로 활용하냐에 달려있다. 우리의 목적은 혐오 댓글의 피해자가 줄어들기를 바라는 것이었으므로 익명인의 표현의 자유보다는 기사의 대상이 되는 사람의 기분을 좀 더 신경쓰기로 했다. 그래서 태깅을 할 때에 당사자의 입장에서 생각하도록 가이드했다. 어노테이션이 어려웠던 댓글 예전에 데뷔작에서 수영복입고 수중씬 기억난다 진짜 섹씨했는데 연예인이라는 직업이 가지는 특수성 때문에 판단하기 어려웠던 경우이다. 특히 여자연예인에 대해서는 외모에 대해 품평하는 댓글이 많았는데, 스스로가 연예인이었던 적이 없으니 감정이입을 해서 이를 모욕이라고 봐야할지도 모르겠고, 만약 의도적으로 외모를 부각해서 유명세를 얻은 경우라면 모욕이라고 보기가 더 어렵다고 생각했다. 결국 각자의 판단에 맡겨서 majority voting을 했지만 정말 어려웠던 케이스였다. 신천지? \"일반적으로 비난받을만한 행위로 인한 혐오는 어떻게 판단해야할까?\" 를 고민하게 만든 댓글이었다. 신천지 교도로 인해 코로나가 빠르게 퍼졌던 사건 이후로 \"신천지\"는 부정적인 이미지로 굳어져 버렸는데, 이 맥락을 고려해서 위의 댓글을 혐오라고 태깅하면 \"신천지\"라는 가치 중립적인 단어가 혐오로 태깅되기에 굉장히 고민이 많았습니다. 살빠진 마닷같애 위와 비슷한 케이스로 이 댓글 또한 판단하기 무척이나 어려웠다 ㅠㅠ offensive로 판단하자니 마닷은 뭐가 되냐는... Other biases 라는 label 현재는 bias label이 gender bias, other biases, none 의 세가지로 구성되어 있다. 이렇게 할 수 밖에 없었던 가장 큰 이유는 예산 문제였다 ㅠㅠ 돈이 많았다면 gender 외에도 정치, 지역, 인종 등에 대한 편견도 label을 수집할 수 있었을텐데 하는 아쉬움이... 인당 150만원 이상은 부담하고 싶지 않아서, 그리고 연예 도메인은 성 편견이 가장 많은 비중을 차지하고 있어서 이런 결정을 하게 되었다. 그러다보니 others 라는 label 은 온갖 종류의 편견이 모두 모아져 있다. 아마도 모델이 곧장 others 를 예측하는 것은 쉽지 않을 것이라고 생각한다. 이 task는, 논문에 적혀있듯이, 2-step classification 문제를 푸는 방식이 낫지 않을까라고 생각한다. 먼저 gender / no-gender 를 예측하고, 그 이후에 bias 유 / 무 를 예측하면 gender, others, none 을 좀 더 쉽게 예측할 수 있을 것이라고 생각한다. 어노테이션 작업 시 context 미제공 댓글에 포함된 편견 및 혐오를 더욱 정확하게 판단하기 위해서는 댓글이 작성된 뉴스 기사에 대한 정보가 필요하다. 하지만 현실적인 이유들로 포기했었다. \"작업자가 기사를 읽어야 하는 번거로움을 감수할까?\" \"태깅 플랫폼에서 이 기능을 제공해줄까?\" \"뉴스 기사의 내용에 대한 저작권은 우리에게 없기 때문에 공개 데이터셋에 포함할 수 없고, 그럴거라면 태깅을 컨텍스트 없이 하는게 좋지 않을까?\" 등의 질문들에 대해 명쾌한 답변을 내리지 못했고, 결국 댓글의 내용만 보고 판단하는 방식을 가져갔다. 지나고나니 아쉬움이 남는 건 어쩔 수 없는듯하다. Testset 구성 현재 testset은 함께 작업했던 저와 조원익, 이준범이 직접 작업한 라벨이 달려있다. 우리의 의도와 부합하는, 가장 어노테이션이 잘 되었다고 보장할 수 있는 데이터셋이라고 볼 수 있다. 하지만 지나고나니 \"시간 순으로 train, validation, testset을 구성했다면 어땠을까?\" 하는 아쉬움이 남았다. 댓글에는 많은 사회적 배경지식이 녹아져있다. 특히 인물의 이름이 가지고 있는 정보가 있는데, 우리가 수집한 기간에는 승리와 정준영 등의 연예인이 얽혀있던 단톡방 사건이 포함되어 있었다. 그래서 \"승리\"가 포함된 댓글은 부정적인 맥락 속에서 판단되었다. 예를 들어 \"승리가 뭘 잘못했다고 난리들인지...그냥 승리 부럽고 베알꼴린 애들이 화난거로밖에 안보임ㅎ\" 라는 댓글에서 \"승리\"를 제거하면 성편견이 없는 것으로 태깅되었겠지만, \"승리\"가 포함되었기 때문에 성편견이 존재하는 것으로 태깅된다. Generalization을 잘 하는 모델이 진짜 잘하는 모델이라고 했을 때, 학습 데이터에 \"승리\"가 없어도 위의 댓글에 달린 라벨을 잘 예측할 수 있는 모델을 판별할 수 있게 testset을 구성했다면 더 좋았을 것 같다. KoBERT tokenization baseline으로 CharCNN, BiLSTM, BERT를 사용한 모델의 결과를 논문[2]에 첨부했다. 여러 task 모두 BERT가 가장 좋은 성능을 보였다. 댓글은 맞춤법을 지키는 문장과는 거리가 멀고, 줄임말, 신조어, 연예인명, 그리고 그 외의 고유명사 등이 많이 등장한다. 그래서 BERT tokenization 결과를 보면 한 글자 씩 분리되는 경우가 빈번했다. 페지해라 가세연. 페지가 답이다 아님말고식 증거도없이 유재석 언급하노['▁페', '지', '해', '라', '▁', '가', '세', '연', '.', '▁페', '지가', '▁답', '이다', '▁아', '님', '말', '고', '식', '▁증거', '도', '없이', '▁유재석', '▁언급', '하', '노'] god 박준형이 이 기사를 싫어합니다.`['▁', 'go', 'd', '▁박', '준', '형', '이', '▁이', '▁기사', '를', '▁싫어', '합니다', '.'] koBERT 학습 데이터에 자주 등장했던 연예인 이름은 원본 그대로 보존되는 반면, 그렇지 못한 연예인은 이름이 쪼개진다. 조개갖고 개ㅈㄹ하는 태콩이나 개촐싹대는조샌징들이나 ㅈㄴ웃김ㅋㅋㅋ ['▁O', 'O', 'O', '▁조', '개', '갖', '고', '▁개', 'ᄌᄅ', '하는', '▁태', '콩', '이나', '▁개', '촐', '싹', '대', '는', '조', '샌', '징', '들이', '나', '▁', 'ᄌᄂ', '웃', '김', 'ᄏ', 'ᄏ', 'ᄏ'] ㅅㅂ 더럽게 메갈어로 제목뽑는거 봐라['▁', 'ᄉᄇ', '▁더', '럽', '게', '▁메', '갈', '어', '로', '▁제', '목', '뽑', '는', '거', '▁봐', '라'] \"ㅈㄴ\", \"ㅈㄹ\", \"ㅅㅂ\" 같은 단어가 tokenization에서는 쪼개지지 않는다. 어려웠던 작업이었고, 완벽했다고는 할 수 없지만 좋은 시작점이 될 수 있는 프로젝트였다고는 생각한다. 이번에 해결할 수 없었던 여러 한계점들을 극복하는 다른 좋은 결과들이 많이 나올 수 있길 :) 이제 진짜 끝! References 1.https://github.com/kocohub/korean-hate-speech ↩︎ 2.https://arxiv.org/abs/2005.12503 ↩︎","link":"/2020/05/28/korean-hate-speech-dataset/"},{"title":"Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?","text":"Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는 다양한 방법들을 제공한다. 이러한 방법들을 Merge 전략이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장 많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase and Merge에 대해 소개하려고 한다. 위의 그림과 같은 상태의 commit이 생성되었다고 가정하자. feat/multiply라는 브랜치가 있고, feat/sum이라는 브랜치가 있다. 각 commit 내의 숫자는 commit의 global 순서를 나타낸다. Merge Commit 브랜치의 commit log와 merge log가 동시에 기록된다. Commit log는 commit을 행한 순서대로 기록되고, merge log는 merge가 된 순서대로 기록된다. 동시에 기록되기 때문에 commit log가 verbose해지며 commit log의 순서가 merge 순서와 다르기 때문에 history 관리 및 이해가 어렵다. Squash and Merge Merge된 순서대로 master/main 브랜치에 기록된다. 그리고 작업 완료된 브랜치의 commit은 새로운 commit 으로 모두 squash되며, 새로운 commit의 제목은 PR 제목이 되고, 합쳐진 commit의 제목은 새로운 commit의 상세 내용이 된다. 이러한 특징 때문에 master/main 브랜치의 히스토리 관리가 쉬우나, atomic commit level로 rollback 하는 것은 불가능하다. Rebase and Merge Commit 순서가 아닌 merge 순서대로 기록된다. 그래서 하나의 PR에 담긴 commit message가 다른 PR의 commit message와 섞이지 않는다. 그리고 rebase 덕분에 merge된 이후의 로그를 보았을 때 하나의 브랜치에서 연속적으로 작업한 것과 같은 로그를 확인할 수 있다. 이 때문에 얼마든지 항상 원하는 수준으로 rollback 이 가능하다. 하지만 잘 적용하기 위해서는 commit을 생성할 때부터 올바른 commit 단위로 분리해야 하며, commit message 또한 설명력을 가지고 있어야 한다. 그리고 다른 PR이 먼저 merge되는 경우, rebase 작업이 필요할 수 있고 이 때 발생할 수 있는 conflict를 잘 해결할 수 있어야 한다. Summary Merge Strategy Pros Cons Merge Commit 아직 찾지 못함 불필요한 commit message가 생기고 merge 순서와 commit 순서가 별도로 기록되어 history 관리가 어려움 Squash and Merge Commit 단위 별로 꼼꼼하게 관리하지 않아도 PR title 만 제대로 관리하면 history가 깔끔하게 정리됨 Atomic level의 rollback이 어려움 Rebase and Merge Atomic level의 rollback이 용이하며 commit 단위의 history 기록이 됨 Commit을 잘 다루지 못하는 경우, rebase에 익숙하지 않은 경우 어려움이 발생 앞서 소개한 전략들의 장/단점을 정리하면 위와 같다. 개인적으로는 아직 Merge Commit의 장점을 발견하지 못했다. 결론적으로 팀원 전체가 git을 다루는데에 굉장히 익숙해서 commit 단위, commit message, rebase 등에 어려움이 없는 경우, 혹은 atomic level의 rollback이 필요한 개발 상황에서 Rebase and Merge가 선호된다. 하지만 atomic level 까지의 rollback은 필요하지 않고 팀이 이제 막 git으로 버전관리하는 법을 배우기 시작했다면 Squash and Merge가 좋을 것이다.","link":"/2021/07/11/git-merge-strategies/"},{"title":"Naver News Comment Analysis (1)","text":"올초(3월)부터 같은 팀의 재명님과 네이버 뉴스 댓글 데이터로 사이드 프로젝트를 시작했다. 직접 크롤링하신 데이터였는데, 그 양이 방대해서 \"이 정도 데이터가 있으면, 뭔갈 해볼 수 있겠지!\" 라는 가벼운 마음으로 사이드 프로젝트 제안을 덥석 받아물었다. 그리고 여느 사이드 프로젝트가 그렇듯 그 과정은 결코 생각만큼 가볍지는 않았더랬다... 마침 작년 사내 Hackday에서 Abuser Detection 분석으로 좋은 성과를 얻었던터라 어뷰저 분석을 해보고 싶었고, 그 결과로 나름 재밌는 것들이 발견되었다. 하지만 좋은 발표 자리(이를테면 파이콘이라든지,,,)에 등록할 시기를 놓쳐서 논문을 arXiv에 올려두듯이 블로그에 댓글 분석한 내용을 공유하고자 한다. 내용은 크게 뉴스 댓글 수집과 뉴스 댓글 분석 파트로 나뉘며, 전자는 재명님이 후자는 내가 주로 담당해서 정리하였다. 이번 글은 뉴스 댓글 분석 1편이다. Data 수집 기간 2006.04.26 ~ 2018.05.25 (수집 시점: 2018.10) 수집 내용 네이버 뉴스의 6개 분야별(정치, 경제, 사회, 생활/문화, 세계, IT/과학) 가장 많이 본 뉴스 30건 같은 기사이지만 2-3일 동안 랭킹뉴스에 오를 수 있으므로 중복 기사를 제거해주었다. * 중복 제거 전 기사 #: 751,751 (약 75만) * 중복 제거 후 기사 #: 643,226 (약 64만) 분석에 사용한 필드 기사: 기사 id, 기사 제목, 기사 입력 시각, 기사 내용, 언론사, 기사 감정 댓글: 댓글 작성 기사id, 작성자 hashed id, 댓글 작성 시각, 댓글 내용, 공감수, 비공감수 Basic Statistics 중복 제거된 기사에 대해, 기사 작성 시점을 기준으로 한 달 단위로 기사에 달린 코멘트를 집계해서 그래프를 그리면 다음과 같다. 네이버 뉴스 개편 history[1] 와 엮어서 이 그래프를 해석하면 재밌어진다. 2009년 2009년 개편 때는 메인 페이지 뉴스 박스 편집권을 신문사에 넘겼고, 기사를 클릭하면 바로 신문사 링크로 연결되게 바뀌었다. 이로 인해 네이버 뉴스의 트래픽이 감소하게 되었고 예전과 비교해서 리플 개수나 조회수가 상당히 줄어들었다. 2010년 2010년대 초반에 뉴스 스탠드가 도입되면서 메인화면 뉴스 편집권을 포기하게 된다. 기사를 클릭하면 기본적으로 네이버 페이지가 아닌 언론사 사이트로 연결된다. 모바일로 댓글을 달 수 없었다. 또한 댓글 형태가 댓글 제목을 클릭해야만 내용을 볼 수 있는 형태라서 결과적으로는 당시 뉴스 댓글 란은 지금보다 훨씬 폐쇄적인 모양새였다. 2012년 검색과 지식인의 인기를 바탕으로 네이버가 2012년 1일 방문자 1800만 명을 기록할 정도로 성장하는 동안, 네티즌의 뉴스 읽기 방식도 달라졌다. 종이신문을 읽거나 신문방송의 홈페이지를 찾아가는 대신, 네이버나 다음 등 포털의 뉴스캐스트를 통해 여러 언론사 기사를 한꺼번에 읽는 사람들이 크게 늘어난 것이다. 이 때문에 뉴스 편집 기능을 수행하는 포털을 언론사로 봐야 할 것이냐 아니냐 하는 논쟁이 언론관련 심의기구 등에서 벌어지고 있기도 하다.[2] 2012년 중반부터 모바일로도 댓글을 달 수 있게 되었다. 네이버 아이디로 로그인하지 않아도 트위터나 페이스북 등의 SNS 계정으로 댓글을 달 수 있게 되었다. 이 때문에 네이버 영화 평점 조작처럼 추천수 조작하기도 쉬워졌다. 네이버, 미투데이, 트위터, 페이스북, 다음으로 한 번씩만 로그인해도 공감 및 비공감 5개를 줄 수 있다. 2016년 10월, JTBC에서 최순실의 태블릿 pc를 발견하였고 최순실 게이트 사건의 포문이 열리기 시작했다. (트래픽 측면에서 네이버 뉴스는 최순실에게 감사하는 마음이 없지 않아 있을 것이다...) 그리고 동시에, 드루킹의 댓글 조작 사건도 시작[3]되었다. 2018년 5월 말, 특검법이 통과된 이후에 댓글이 줄었다는 기사[4] 가 보도되었다. 2018년 6월 이후의 댓글이 있었다면 그간 댓글부대의 위력이 어느 정도였는지 가늠해볼 수 있었을 것이다. 결론적으로 2016년 후반부 이후 폭발적인 댓글 수의 증가는 정치 및 사회 영역의 엄청난 트래픽 덕분이었을 것이다. 가설 검증 차원에서 섹션 별로 나누어 같은 방식으로 댓글을 집계해 보았다. News Sentiment Analysis 네이버 뉴스는 기사에 \"좋아요\" 를 시작으로 \"훈훈해요\", \"슬퍼요\", \"화나요\", \"후속기사 원해요\" 의 label을 달 수 있게 만들었다. - \"좋아요\": 2014년 초 시작 - \"훈훈해요\", \"슬퍼요\", \"화나요\", \"후속기사 원해요\": 2017년 초 시작 \"좋아요\" 만 있을 때와 다섯 가지의 감정이 있을 때의 추이가 또 재밌다. \"좋아요\" 외의 다른 감정이 허가된 순간 이후로 \"화나요\" 가 급격히 증가한다. 정치 참고: 사드배치 (2017.03), 문재인 당선 (2017.05)[5], 평창 동계 올림픽 (2018.02), 이명박 수감 (2018.03)[6] 경제 사회 문화 2018년 2월에는 무슨 일이.. (추운 날씨, 성추행 등의 사건 때문으로 추측됨) IT 세계 Conclusions 여기까지는 기초적인 데이터 탐색 작업이었다. 간단히 시간 순으로 댓글 수를 집계하기만 해도 재미있는 분석 결과를 얻을 수 있었다. (가령, 박근혜-최순실 게이트가 얼마나 큰 이슈였는지, 뉴스는 대부분 우리를 열받게 하는 내용이라든지 등) 이 다음 분석은, 의심하기만 했던 댓글 어뷰저 집단이 실제로 존재하는지에 대해 다룰 예정이다. 마침 댓글 수집 기간과 드루킹의 댓글 조작 기간이 맞물려 있어서 분석해 볼 수 있는 데이터가 손에 쥐어졌다. 최대한 선입견없이 담백한 분석을 해보려고 노력했다. 정말인지 아닌지 다음 글에서 확인해보자. References 1.https://namu.wiki/w/네이버_뉴스 ↩︎ 2.https://www.wikitree.co.kr/main/news_view.php?id=71675 ↩︎ 3.https://www.mk.co.kr/news/society/view/2018/05/294952/ ↩︎ 4.https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001 ↩︎ 5.https://ko.wikipedia.org/wiki/2017년_대한민국 ↩︎ 6.https://ko.wikipedia.org/wiki/2018년_대한민국 ↩︎","link":"/2019/07/25/naver-news-comments-analysis-1/"},{"title":"Lessons learned in my first 2 years as a startup founder","text":"최근 창업 2주년을 맞았다. 지난 2년 동안 ChatGPT와 경기 침체라는 두가지 큰 tipping point를 겪었다. (공식적으로는) 5개의 제품을 출시했다가 접었고, M가지 종류의 N개 AI 모델 ckpt를 glacier로 보냈고, B2B와 B2C 모두 경험해보면서 대기업부터 SMB까지 100여개의 클라이언트와 수백명의 유저를 만났다. 그리고 지금은 투자 의사결정을 위한 기업 분석 서비스인 AILookUp과 헬스케어 서비스의 Receptionist 역할을 수행하는 AI Agent API를 만들고 있다. 지금 만들고 있는 제품이 어떤 결과를 만들어낼지는 또 모르는 일이지만 적어도 지금까지는, 이전과 다른 유저와 고객사의 반응을 체감하고 있다. 뿐만 아니라 나도 이전과는 다른 고민을 하고 있고 팀이 움직이는 방식도 이전과 달라졌다는 점에서 우리가 다음 phase로 넘어왔다는 것을 느끼고 있다. 온 몸으로 경험해내야만 했던 어려웠던 의사결정들과 그로 인한 결과들을 직면했던 지난 시간 속에서 기록하고 싶은 몇가지를 적어봤다. 사람들의 내면 깊숙한 곳에서 바라는 진짜 욕망을 이뤄줄 것 같은 제품이 팔린다. 직접 사람들을 만나서 물어본다고 발견할 수 있는 것이 아니다. 사람들 스스로 인지하지 못할 수도 있다. 원하는 것이라고 착각할 때도 있다. 원하는지도 몰랐던 욕망도 있다. 무엇보다도 물어봐서 쉽게 답변이 나오는 욕망은 스타트업이 해결할 문제가 아닐 가능성이 높다. 산업과 사람에 대한 이해가 필요하다. AI 키워드를 활용해 회사의 가치를 올리고 싶어하는 것 또한 욕망의 한 종류다. 하지만 지속적으로 팔리기 위해서는 (지속되는 욕망이라는 전제 하에) 그 욕망을 진짜로 이뤄줄 수 있어야 한다. 기대감 만으로는 retention을 만들 수 없다. 그런 욕망을 발견하는 가장 최선의 방법은 빠른 iteration 올바른 방향으로 가설을 정의하고 이를 검증할 수 있는 가장 좋은 방법을 찾고 빠르게 결과를 확인하는 과정을 무한히 반복해나가며 우리에게만 보이는 참인 가설을 뾰족하게 만들어나가는 것 알베르토 사보이아의 아이디어 불패의 법칙에서 소개된 가짜 door 방법을 통해서는 제대로 된 가설 검증과 깊이 있는 실패를 하기는 어렵다. 그 개념은 가설 검증을 위한 작은 기능을 개발하는데에 드는 시간과 노력이 많이 들었을 시기에는 유용하다고 생각하지만 요즘처럼 개발이 쉬워진 상황에서는 만들어서 검증하는 것이 훨씬 더 다양하고 제대로 된 실험을 해볼 수 있고, 깊이있는 경험을 해볼 수 있기 때문에 후회도 덜 하고 실패하더라도 다음 기회에 성공할 가능성이 더 높아지는 방법이라고 생각한다. 좋은 팀이 필요하다. 좋은 팀을 보면 뛰어난 사람이 된 개개인으로 구성되어 있지만 처음부터 뛰어난 개개인이 모여서 좋은 팀이 되는 것은 아니다. 앞으로 무엇을 할 것인지보다 지금 무엇을 하지 않을지를 결정하는 것이 더 어렵고, 더 중요하다. 누구를 뽑아야 하는지보다 지금 누구를 뽑지 말아야 하는 것을 아는 것이 더 어렵고, 더 중요하다. 제품을 팔고 싶은 대상을 나열하는 것보다, 지금 누구에게 팔지 않는 것이 좋은가를 아는 것이 더 어렵고, 더 중요하다. 더 큰 가능성을 고려하며 개발하는 것보다 지금 개발에 고려하지 않을 영역을 결정하는 것이 더 어렵고, 더 중요하다. 더 많은 일을 하는 것보다, 가장 중요한 일을 제대로 하기 위해 어떤 일을 하지 않기로 결정하는 것이 더 어렵고, 더 중요하다.","link":"/2024/01/13/lessons-learned-in-the-first-2-years-as-a-startup-founder/"},{"title":"Naver News Comment Analysis (2)","text":"NOTICE: 앞으로 소개될 내용은 NAVER와 무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은 중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을 알립니다. TL;DR 2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를 확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다. 타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다. 발생하기 어려운 패턴을 보여야 한다. 이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서 369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고 269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다. Abuser, who are you? Introduction \"정말 2016년 4분기부터 정말 댓글 조작을 했던 사용자들이 있었을까?\" 라는 단순한 의문과 궁금증에서 분석을 시작하게 되었다. 다만, 데이터에 을 눌렀던 interaction 정보가 누락되어 있기에 (이 데이터는 네이버 뉴스 측에서 제공해주지 않는 이상 얻을 수 없다) 적어도 댓글을 한 번이라도 남겼던 사용자에 대해서만 어뷰저로 의심해 볼 수 있었다. label이 없는 상황에서 어뷰저를 특정짓는 것과 그 사용자가 어뷰저임을 다른 사람에게 설득하는 것은 어려운 일이다. 또한 무죄추정의 원칙에 의거해 댓글 작성자를 함부로 어뷰저라고 단정지을 수도 없었다. 그래서 이번 분석에서는, \"모든 작성자는 어뷰저가 아니다.\" 라는 가정을 기반으로 특정 패턴이 등장할 확률을 계산해서 어뷰저였을 가능성을 간접적으로 추측하는 방식을 취했다. 이 과정에서 누군가는 그 정도 확률로는 어뷰저라고 단정짓기 어렵다고 판단할 수도 있고, 아닐 수도 있다. 또 추가적인 분석 결과가 있다면 어뷰저 가능성이 더 높아질 수도 있다. 후자라면 언제든 댓글로 추가 분석할 내용을 요청했으면 하는 바람이다. Abuser Criteria 어뷰저는 어떤 존재일까? 이에 답하기 앞서, 어뷰징의 목적과 어뷰징이 문제가 되는 상황에 대해 먼저 정리해보았다. 어뷰징의 목적 어뷰저들의 목표는 네이버의 댓글 정렬 기준에 맞추어 10위권 내에 드는 것이다. 네이버 뉴스의 UI 상, top 10 내에 들면 그 기사를 읽는 누구나 쉽게 그 댓글의 내용에 접하게 되기 때문이다. 그리고 그 내용이 대중을 대표한다고 생각하기 때문에 쉽게 타인의 생각에 영향을 미칠 수 있다. 어뷰징이 문제가 되는 상황 어뷰징이 문제가 되었던 이유는 공정하고 자연스러운 방식으로 집계되었다고 믿었던 top 10 댓글이 실제로는 어떤 세력에 의해 의도를 가지고 조작되었기 때문이었다. top 댓글이 특정 집단에 의해 조작되었다면, 그것들이 과연 네이버 뉴스 플랫폼에 참여하는 사용자들의 생각을 대표하는 댓글이라고 볼 수 있을까? 그래서, 이 글에서 이야기 할 어뷰저의 criteria는 다음과 같다. 어뷰징의 목적을 달성해야 한다. 즉, 타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번 이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다. (안습) 자연적으로 발생하기 어려운, 확률이 낮은 패턴이 등장해야 한다. Data preprocessing 에서 사용했던 데이터에서 추가로 필터링이 필요했다. 크롤링한 댓글 데이터에 hashing 된 아이디가 포함된 것이 2015년 12월 이후였기 때문이다. 사용한 댓글 데이터 기간: 2015.12.08 ~ 2018.05.25 Analysis 먼저, 정치 분야에서 댓글이 top 10 내에 들었던 횟수를 작성자 별로 집계한 후, 횟수가 높은 순서대로 정렬하였을 때의 추이를 살펴보았다. mean stdev max 75% med min 2.240442 4.607621 369 2 1 1 대부분의 작성자는 1~2번 정도 댓글이 top 10 내에 드는 반면, 일부 사용자들은 100번 이상 순위권 내에 든다. 이 그래프만 본다면 자주 top 10에 드는 사용자들 모두가 의심스러울 수 있지만 이런 skewed graph는 대부분의 사회과학 데이터에서 발견되므로 이들을 어뷰저로 속단하긴 이르다. 검증을 위해 다른 섹션(사회, 경제, 문화, IT, 세계)에 대해서도 마찬가지 방법으로 그래프를 그려보았다. 갓 파레토... 자주 순위권 내에 드는 댓글을 작성한 사용자를 top user 라고 했을 때, 다른 분야에서도 top user는 쉽게 찾아볼 수 있었다. 어쩌면 이들은 (어뷰저가 아닌 이상) 네이버 뉴스 플랫폼에서 높은 \"공감수-비공감수\"를 받을 수 있는 전략이 학습된 것은 아닐까? 기사가 나오고 얼마 지나지 않아 댓글을 남기거나, 그 당시 분위기에 맞는 댓글의 내용을 남기거나, 사실로 보여지는 데이터와 함께 댓글을 작성하거나 하는 등 자신만의 전략이 있을 것이다. 그러나 이 전략들이 100%의 확률로(=항상) 통하지는 않았을 것이다. 때로는 일찍 댓글을 작성했음에도 뒤늦게 작성한 댓글이 폭발적인 공감을 이끌어내서 top 10에 들지 못했을 수도 있고, 당시의 전반적인 분위기에 탑승하는 댓글을 남겼음에도 다른 댓글 중에 두드러지지 못해 공감을 받지 못했을 수도 있다. top user 간에 일반적인 top 10 성공률이 존재할 것이고 이는 normal distribution을 따른다는 가설을 바탕으로 \"top user가 작성한 전체 댓글 수 대비 top 10에 들었던 댓글 수(=top 10 성공률)\"를 계산해보았다. 정치 top users userId top comment # total comment # top 10 성공률 (%) user 1 369 386 95.60 user 2 339 380 89.21 user 3 269 289 93.08 user 4 178 610 29.18 user 5 178 1090 16.33 user 6 175 424 41.27 user 7 174 818 21.27 user 8 155 316 49.05 user 9 143 950 15.05 user 10 141 583 24.19 경제 top users userId top comment # total comment # top 10 성공률 (%) user 11 289 1185 24.39 user 12 226 2935 7.70 user 13 219 1656 13.22 user 14 183 1636 11.19 user 15 173 1378 12.55 user 16 161 989 16.28 user 17 160 654 24.46 user 18 157 2589 6.06 user 19 139 1514 9.18 user 20 127 742 17.12 사회 top users userId top comment # total comment # top 10 성공률 (%) user 21 366 953 38.41 user 22 308 1636 18.83 user 23 271 935 28.98 user 24 241 1254 19.22 user 25 233 1656 14.07 user 26 204 328 62.20 user 27 191 719 26.56 user 28 168 625 26.88 user 29 149 1190 12.52 user 30 148 1489 9.94 문화 top users userId top comment # total comment # top 10 성공률 (%) user 31 373 1636 22.80 user 32 367 935 39.25 user 33 301 1417 21.24 user 34 243 890 27.30 user 35 220 1656 13.29 user 36 188 1943 9.68 user 37 178 3245 5.49 user 38 172 2738 6.28 user 39 164 200 82.00 user 40 151 719 21.00 IT top users userId top comment # total comment # top 10 성공률 (%) user 41 714 3123 22.86 user 42 572 3886 14.72 user 43 442 2287 19.33 user 44 399 1468 27.18 user 45 231 810 28.52 user 46 234 3010 7.77 user 47 275 1622 16.95 user 48 317 1493 21.23 user 49 346 2349 14.73 user 50 364 1185 30.72 세계 top users userId top comment # total comment # top 10 성공률 (%) user 51 237 1636 14.49 user 52 214 1432 14.94 user 53 145 615 23.58 user 54 148 1709 8.66 user 55 155 611 25.37 user 56 156 1076 14.50 user 57 162 864 18.75 user 58 165 2778 5.94 user 59 165 1254 13.16 user 60 175 575 30.43 top user의 top 10 성공률을 확률 변수 X라고 했을 때의 histogram과 모든 유저가 전략을 바탕으로 활동하는 그룹이라고 가정했을 때 Gaussian distribution으로 추정한 확률 분포이다. (Gaussian Mixture Model로 distribution fitting한 결과는 Appendix A 참고) 정치 섹션에서만 유일하게 ratio &gt; 90% 인 top user(user 1, user 3)가 존재했으며 이들의 top 10 전략 성공률은 다른 top user 대비 발생하기 어려울 정도로 (0.0053%, 0.0079%) 높다고 해석할 수 있다. 숫자 이면의 패턴을 보기 위해 전체 정치면 기사들의 댓글 수와 user 1, user 3의 전체 댓글 수, top 10 내에 든 댓글 수를 그래프로 시각화 해보았다. c.f. 2016.3 ~ 2018.5 까지 굵직한 이슈들[1] [2] [3] 이세돌 vs. 알파고 (2016.3) 옥시 (2016.4) 최순실 태블릿 pc (2016.10) 박근혜 탄핵 소추안 (2016.12) 사드배치 / 박근혜 수감 / 세월호 인양(2017.3) 19대 대통령 선거 (문재인 당선) (2017.5) 이대목동 신생아 사망 (2017.12) 평창 동계 올림픽 (2018.2) 이명박 수감 (2018.3) 드루킹 (2018.4) 남북1차정상회담 @판문점 (2018.4) 남북2차정상회담 (2018.5) user 1 (ratio: 96%) 회색 line이 정치면 기사 댓글, 파란색 line이 작성자가 쓴 전체 댓글 수, 초록색 line이 작성자가 쓴 댓글 중 top 10 내에 들었던 댓글 수를 나타낸다. user 1이 주로 활동했던 시기는, 최순실 태블릿 pc 사건, 박근혜 탄핵 및 19대 대통령 선거, 평창 동계올림픽 및 MB 다스 사건과 맞물려 있었다. 댓글 내용을 시기 별로 뜯어보면, 다음과 같다. title article date user top comments 朴대통령, '29일까지 대면조사' 檢 요청에 사흘째 묵묵부답 2016-11-25 15:12:00 대통령인 자가 자신의 관할하에 있는 검찰을 부정한다면 곧 국가도 부정하겠다는 의미다. 이런 대통령은 더이상 대한민국 대통령이 아니다. 차은택 변호인 \"차씨, 최순실 지시로 김기춘 실장 공관서 면담\" 2016-11-27 16:04:00 김기춘의 진두지휘하에 박근혜 정권의 모든 불법들이 자행되었다. 정말 악마같은 인간이다. 변호인 \"차은택, 崔 지시로 김기춘 만나…우병우 장모와 골프도\"(종합) 2016-11-27 16:46:00 김기춘의 진두지휘하에 박근혜 정권의 불법들이 자행되었다. 구속수사해서 감옥에서 못나오게 만들어야 한다. 공작정치부터 공안탄압 정경유착의 죄를 물어야 한다. 청문회장도 지배한 '촛불민심'…與野 '재벌 봐주기' 없었다 2016-12-06 12:34:00 뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까... 강요죄는 확실할듯... 재벌 총수들 \"청와대 거절 어려워\"…하나같이 대가성 부인 2016-12-06 12:45:00 뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈 일이니까... 강요죄는 확실할듯... 박한철 前소장 한표, '캐스팅보트' 될뻔한 아슬아슬 상황 나올까 2017-03-04 08:00:00 탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다. 85시간 재판, 속기록 3000쪽…탄핵심판 이번주 결론날까 2017-03-05 09:00:00 탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측 대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에 동참하겠다는 선언일 뿐이다. [리얼미터] 다자 文 42.6% vs 安 37.2%…양자 文 47.6% vs 安 43.3% 2017-04-10 09:15:00 여론몰이에 흔들릴 국면이 아니다. 무쏘의 뿔처럼 나아가면 야합은 흩어지고 굳건함이 승리할 것이다. 文 \"김부겸 동지 미안하다…꼭 국민통합 해내겠다\" 2017-04-22 08:01:00 김부겸의 진심이 느껴지고 그를 위로하고 뜻을 같이 하는 문재인의 진심도 느껴진다. 남자들에게 이런 동지애는 죽음도 불사하게 만드는 마력과도 깉다. 조~오타!!! 文 대통령 \"내게 반대하라\" 파격적 수석회의 시동(상보) 2017-05-25 12:56:00 요새 대통령의 행동과 지시사항을 보면 정말 준비된 겸손한 사람이란게 진솔하게 느껴진다. 대한민국 국민인게 자랑스럽고 행복해진다. 文대통령 \"사드 임시배치, 현재 정부가 취할 수 있는 최선의 조치\"(종합) 2017-09-08 21:13:00 국가의 지도자는 자신의 굳은 신념까지도 국가와 국민을 위해 잠시 접어야할 용기가 필요할 때가있다. 그 지도자라고 왜 자신의 신념을 꺾음에 자괴감과 고민이 없겠는가? 그는 자신을 지지하는 사람들만의 지도자가 아니라 대한민국의 지도자이기 때문이다. 그의 고뇌찬 결단을 위로하며 지켜보고 힘을 실어주고 싶다. 與 \"안철수, 나라다운 나라 만드는 일 폄훼 말라\" 2017-11-04 16:10:00 명버기 구하기에 혈안이 된 명바기 아바타!!! 김정은 위원장 \"이른 시일내 만날 용의\"…문 대통령에 방북 요청(종합) 2018-02-10 15:56:00 남북 정상회담애서 허심탄회하게 모든 할 말 다해서 기필코 한반도 비핵화와 평화를 이루어야 한다. [현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표 2018-03-09 09:11:00 한반도 평화가 세계 평화다. 이런 평화 모드가 얼마만인가... 같은 내용의 댓글이 top 에 오른 것도 확인할 수 있었다. 다른 섹션에 같은 댓글을 남기고도 top에 오른 적도 있다. (아래 표 참고) title section article date top user comments [현장영상] 박근혜 前 대통령 법원으로 출발 society 2017-03-30 10:18:00 최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다. 구속 갈림길에 선 박근혜 '웅변 대신 침묵' 선택 politics 2017-03-30 10:22:00 최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로 직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의 감시로부터 벗어날수 있다. user 3 (ratio: 93%) user 3 의 주 활동 시기는 사드배치, 평창 동계올림픽 및 북미회담과 맞물려있었다. 댓글을 자세히 보면 아래와 같다. title article date user top comments 정상회담 돌발 변수는 '사드'…청 \"모든 가능성 준비\" 2017-06-25 20:20:00 국익과국가안보가최우선입니다~~~~ 송영무 \"사드, 비준 아닌 국회 검증…고액연봉·음주운전 송구\"(종합) 2017-06-28 12:18:00 이유미녹취록에맛짱구치고놀아난언론은× 이유미-이준서 중 한 명은 거짓말…윗선 수사 불가피 2017-06-28 20:52:00 이유미녹취록에맛장구치고놀아난언론은~~~~???? 軍, 송영무 인사청문회서 공개된 '군사기밀 유출' 조사 착수 2017-06-29 12:24:00 자유당놈들답다도둑놈들 文대통령, 내일 트럼프와 만난다…취임 후 첫 韓美정상회담 2017-06-29 13:54:00 국익과국가안보가최우선입니다부디좋은결과있으시길~ [단독] '제보 조작' 수사망 좁혀오자 安 독대한 이준서…왜? 2017-06-29 20:19:00 철수야~ 깜빵갈시간이다가오네~~~~ 트럼프, 文대통령 부부에 백악관 사적공간 '트리티 룸' 깜짝공개(종합) 2017-06-30 13:23:00 문재인대통령님~ 멋저부러요~♡♡♡ 한반도 이슈서 '주도권' 확보 성과…한미FTA 재협상 '숙제'(종합) 2017-07-01 10:05:00 국익과국가안보가최우선입니다부디좋은결과있으시길~~~~~~~~♡♡ 남북 \"4월말 정상회담 판문점서 개최\"…특사단 발표(종합) 2018-03-06 20:24:00 이게 실화나ㅡ ㅡㅡㅡㅡ 문 대통령 \"서울·평양·판문점 중 北이 판문점 정상회담 선택\" 2018-03-07 15:28:00 문통 지지 합니다 문 대통령 \"국외 대북 비밀접촉 없어…저쪽에 놀아나는 것 아냐\" 2018-03-07 16:53:00 문 대통 령님 지지 합니다 文대통령 \"북핵목표는 비핵화…제재완화, 지금은 불가능\"(종합) 2018-03-07 17:16:00 문대통령님 지지합니다 [현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표 2018-03-09 09:11:00 이게 실화냐ㅡㅡㅡㅡ Conclusions 어뷰저를 타인의 생각에 영향을 미치고 비정상적인 행태를 보이는 사용자로 정의하였고, 이 기준에 따라 어뷰저로 의심되는 사용자를 찾아내고자 하였다. 순공감 기준, 댓글이 10위권 내에 들었던 횟수가 많았던 작성자 중에서 작성한 댓글 수 대비 top 10 댓글 수의 비율이 일반적이지 않은 작성자 개인적으로, 1.과 2.의 기준에 드는 사용자는 user 1, user 3 라는 생각이다. 작성한 전체 댓글 수는 다른 사용자들에 비해 적은 편이었지만 top 댓글에 들었던 비율은 가장 높았고, 그 수치가 일반적이지는 않았다. 어뷰저를 찾고자 시작한 분석이었지만 데이터를 살펴보면서 네이버 뉴스 댓글이 가지는 단일하고 공개된 ranking system이 얼마나 위험한지를 오히려 인식하게 되었다. 분석한 기간에서 중복 제거한 기사의 수는 총 100,780개 였고, 만약 top 10 댓글의 작성자가 모두 다른 사용자였다면 1,007,800명이 각자의 의견을 개시했을 것이다. 하지만 실제 그 기간에 집계된 unique한 작성자는 총 308,731 명에 불과했다. 게다가 중복 댓글까지 포함하면, 그 다양성은 조금 더 떨어진다. 이 같은 면에서 네이버 뉴스 댓글은 다양성을 충분히 수용하고 있지 못하다는 생각이 들었다. \"플랫폼이기 때문에 그럴 수 있지 않을까?\" 싶지만 페이스북이나 유튜브, 레딧같은 다른 플랫폼에서의 댓글을 보면 무작정 호감순으로 정렬하지는 않는다. 이 플랫폼들의 기준이 문제가 없다는 것은 아니다. 하지만 네이버 뉴스 보다 다양한 기준으로 댓글을 정렬시키고 있으며 (최신순, 오래된 순, 공감을 많이 받은 순, relevance 순, 호감 + vote의 크기 등) 이를 통해 다양한 의견이 쉽게 노출될 수 있는 환경을 조성하였다는 점에서는 좀 더 높은 점수를 주고 싶다. 그래서 세 번째 글은, 간단하지만 다른 정렬 기준을 적용했을 때 발견되는 새로운 댓글에 대해서 다뤄 볼 예정이다. 프로젝트 초창기에, 같이 작업을 진행했던 재명님이 돌려본 결과가 있는데 이 것도 조금 다듬어서 올리면 재밌을 것 같다! 3탄은... 휴가(@Norway) 다녀오고 나서 작업해볼까 싶다. To be continued... Appendix A: Gaussian Mixture Model Fitting GMM의 n_components 최적 개수를 구하기 위해 silhouette score를 계산하였다. score가 가장 높은 n_components=2 이므로 2개의 gaussian을 가정하여 fitting 해보면 아래 그림과 같다. userId cluster 1 prob cluster 2 prob ratio (%) user 1 2.27E-16 1 95.60 user 3 2.56E-15 1 93.08 user 2 9.31E-14 1 89.21 user 39 4.96E-11 1 82.00 user 6 8.11E-01 0.188898 41.27 user 9 1.00E+00 0.000219 15.05 References 1.https://ko.wikipedia.org/wiki/2016년_대한민국 ↩︎ 2.https://ko.wikipedia.org/wiki/2017년_대한민국 ↩︎ 3.https://ko.wikipedia.org/wiki/2018년_대한민국 ↩︎","link":"/2019/08/03/naver-news-comments-analysis-2/"},{"title":"Naver News Comment Analysis (3)","text":"TL;DR 어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은 불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은 내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까? 네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을 상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연 정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할 의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱 획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어 무의식적으로 다양한 사고에 대한 가능성을 차단받는다. 그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sorting algorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을 비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고 비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다. Introduction 모두가 다 알고 있는 사실이지만, 어뷰저는 존재한다. 드루킹과 에서 나온 결론으로도 뒷받침될 수 있지만 트위터에 m.news.naver.com/comment 라고 검색하기만 해도 아래와 같이 댓글 조작의 흔적을 쉽게 발견할 수 있다. 이렇듯 쉽게 어뷰저의 존재를 찾을 수 있음에도 네이버가 어뷰저를 잡지 않는 이유는 그 일이 생각처럼 쉬운 일이 아니기 때문이다. n초 안에 여러번 공감과 비공감을 지속적으로 받은 댓글은 어뷰징의 결과로 의심한다. 그 댓글을 지워야 할까? 만약 댓글을 쓴 유저가 어뷰저가 아니었다면 문제가 될 수 있다. 사후 분석을 통해 어뷰저로 의심되는 댓글의 내용을 지우는 방법은 어떨까? 뉴스라는 매체의 특성 상 시간이 지난 기사는 사람들이 관심있게 보지 않는다. 그러므로 이 방법은 어뷰저를 막는다고 볼 수 없다. 분석을 통해 어뷰저라고 강하게 의심되는 유저를 차단한다고 하더라도 새로운 패턴으로 어뷰징을 하는 유저들이 생겨날 것이다. 어뷰저의 기준을 세우는 것은 어려운 반면 새로운 방식으로 어뷰징을 하는 것은 좀 더 쉽기 때문에 이렇게 물고 물리는 싸움은 어뷰저에게 유리하다. 그렇다면 어뷰저를 차단하는 것에만 집중하지 말고, 어뷰징은 내버려두되 그 효과를 완화시키는 방법은 어떨까? 지금 네이버 뉴스 댓글 랭킹 방식은 그것이 미치는 영향력에 비해 너무 간단하고 단편적이다. 구글의 검색 랭킹이 신뢰도를 가지고 있는 이유는 상위에 랭크된 글이 '조작'을 통해 만들어지지 않았다는 점 때문일 것이다. 그 이유는 정보가 되는 글에 대한 정보량, 품질 기준이 보다 엄격하고 단편적인 면으로만 순위를 매기지 않기 때문이다. 만약 구글 랭킹이 웹문서의 클릭수로만 되어 있었다면 어땠을까? 많은 기업들이 본인의 홈페이지를 상위에 랭크시키기 위해 많은 조작이 일어났을 것이다. 그래서 이번 글에서는 그렇게 간단하다고는 볼 수 없는 다른 랭킹 algorithm에 대해 소개해보려고 한다. 현재 네이버 뉴스 댓글 랭킹 방식 중 순공감순, 공감비율순, 답글순의 한계점을 살펴보고 reddit과 yelp에서 신뢰도있게 쓰이는 best 랭킹과 새로운 관점의 controversial 랭킹 algorithm을 소개한다. Naver News Comment Sorting System Sorting Algorithms 2019년 9월 기준, 총 5개의 정렬방으로 서비스되고 있다. 드루킹 논란 이후 댓글 제공 여부와 정렬방식을 언론사가 선택하는 방식으로 바뀌었다. 순공감순: 공감 - 비공감[1] 공감비율순: 공감 / (공감 + 비공감) 답글순 최신순 과거순 이 중, 댓글에 대한 사용자의 인터랙션(공감, 비공감, 답글)으로 순위를 매기는 순공감순, 공감비율순, 답글순에 대한 문제점을 하나씩 짚어보고자 한다. Limitations 순공감순 순공감순은 우리의 직관과 벗어나는 랭킹이라는 점에서 한계가 있다. 우리는 절대적인 공감 수치보다, 공감비율로 댓글의 신뢰도를 평가한다. 아래의 사례는 네이버 뉴스 댓글[2]의 실제 예시이다. 첫번째 댓글은 순공감 344개(= 455 - 111) 로, 300개(= 316 - 16)의 순공감을 지니는 두번째 댓글보다 더 높은 순위에 자리한다. 하지만 각각의 댓글의 공감비율은 80.4%(= 455 / (455 + 11)) 로, 두번째 댓글의 공감비율인 95.2% (= 316 / (316 + 16)) 보다 작다. 공감비율순 앞서 설명한 것처럼 공감비율순이 좀 더 우리의 직관과 유사한 척도이다. 하지만 공감비율순은 전체 공감, 비공감 수가 적을 때 문제가 된다. 소수의 사람들에게만 노출된 댓글은 공감과 비공감의 개수가 모두 적어 100% 라는 공감비율이 쉽게 만들어지는 반면, 여러 명에게 노출된 댓글은 하나의 비공감만 달리더라도 그보다 낮은 공감비율을 지니게 되는 문제가 발생한다. 아래의 네이버 뉴스 댓글 예시[3]에서 공감수가 20, 비공감수가 0인 댓글이 비공감을 전혀 받지 않아 공감비율 100%가 되어 더 많은 사람들이 읽고 공감을 표한 공감수 1021, 비공감수 58인 댓글보다 더 상단에 위치한다. 답글순 여러 개의 답글이 달리는 댓글은 주로 일찍 남겨진 댓글 중에 인신공격이나 뉴스 외 주제에 대한 댓글인 경우가 많다. 댓글 공간에서는 명확한 내용으로 구성된 댓글에 대해서는 대댓글 보다도 공감 혹은 비공감으로 본인의 주장을 표시하는 것이 일반적이다. 그러나 감정적으로 쓰여진 댓글은 그 댓글에 자극을 받은 다른 사용자의 답글로 이어지고 되므로 답글 개수를 기준으로 댓글을 정렬하면 뉴스 내용과는 무관한 자극적인 댓글들이 우선적으로 노출된다. 또한 일찍 쓰여진 댓글일수록 더 많은 사람들에게 노출될 가능성이 있으므로 대부분 뉴스 작성 시점과 가까운 댓글이 상위에 랭크된다. 랭킹 algorithm으로 보기에는 정렬 기준이 controllable하지 않으며 댓글의 유익한 속성이 높게 평가되어 정렬되는 랭킹이라고 볼 수 없다. 아래의 네이버 뉴스 댓글 예시[4]를 보면 vote 수가 많지 않아도, 공감수가 전혀 없고 비공감만 받더라도 top 10에 위치할 수 있다. Reddit Comment Sorting Algorithms 댓글이 활발하게 생성되는 플랫폼은 비단 네이버 뉴스 뿐만은 아니다. 네이버 쇼핑, 네이버 호텔, 망고 플레이트, reddit, stackoverflow, yelp, amazon 등의 다양한 플랫폼에서 수집되며 플랫폼에서는 다시 이 데이터를 가공하여 사용자에게 유익한 정보를 제공한다. 그 중에서도 reddit의 랭킹 시스템이 앞서 비판했던 순공감순, 공감비율순의 한계를 극복한 sorting algorithm을 제공하고 있기에 자세히 살펴보려고 한다. reddit의 랭킹 방식에는 best, top, new, controversial, old, q&amp;a가 있다. top이 순공감순, new가 최신순, old가 과거순이다. Best Best ranking[5][6] 은 Wilson score[7]로 정렬한 것으로, 공감비율순의 단점으로 언급되었던, 전체 vote수가 적은 상황을 smoothing시켜준 algorithm이다. reddit뿐 아니라 yelp에서도 사용한다고 한다[8]. Wilson score는 주어진 positive와 negative vote가 binomial distribution을 따른다고 가정했을 때, positive 발생 확률을 95% 신뢰구간의 최소값으로 추정한 값이다. 동전 뒤집기 상황에서 앞면을 positive, 뒷면을 negative라고 하자. n번 던진 후 앞면이 나올 확률(\\(p\\))을 추정할 때 n이 충분히 큰 경우 central limit theorem에 의해 \\(p\\)는 normal distribution을 따른다. 따라서 95%의 신뢰도로 \\(p\\)를 추정하여 \\(p\\)의 최소값, 최대값을 구할 수 있고 이 때 최소값이 Wilson score가 된다. 자세한 수식은 Appendix A에 정리해두었다. \\[ w^- = max(0, \\frac{2n\\hat{p} + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)})=\\text{wilson score} \\] 위의 식을 함수로 구현하면 다음과 같다. import numpy as np# ref: http://www.evanmiller.org/how-not-to-sort-by-average-rating.htmldef best(up, down): try: z = 1.96 # 95% confidence level n = up + down p_up = up / n p_down = 1 - p_up denominator = 2 * (n + z**2) numerator = 2 * n * p_up + z**2 - z * np.sqrt(z**2 + 4 * n * p_up * p_down) lower = numerator / denominator except ZeroDivisionError as e: lower = 0 return max(0, lower) 아래의 예시는 네이버 뉴스 댓글에 Best ranking algorithm을 적용해본 결과이다. 공감비율순 정렬이었다면 \"원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요\"는 1000개 이상의 vote를 가진 \"법대로 해라 법은 만인 앞에 평등하다\"는 댓글을 제치고 상위에 랭크되었을 것이다. 하지만 Best 정렬방식에서는 vote 수가 적은 경우 약간의 penalty를 받기 때문에 하위에 랭크되었다. MB '정치보복' 반발에 문무일 총장 \"법적 절차대로 하겠다\"[9] comments 공감수 비공감수 best score 공감비율 법대로 해라 법은 만인 앞에 평등하다 1091 55 0.938 0.952006980803 법대로 하면 사형인데 !! 562 39 0.936 0.935108153078 제발 법대로만 해주세요. 그래도 나라를 지옥으로 만든 죄는 물을 법도 없다. 이 악마야!!! 252 14 0.933 0.947368421053 지금까지 반발하고 나서 살아남은 넘을 못봤다. 565 38 0.933 0.936981757877 혓바닥몇번 낼름거릴까나했더니 찔렸나보네ㅎㅎ 595 37 0.932 0.941455696203 본인이 구린짓을 했으니까 먼저 발광하는거겠지.. 686 43 0.931 0.941015089163 법대로 하는 것보다 더 정의로운 절차는 세상에 없다 4146 317 0.926 0.928971543805 당연히 법대로 하셔야죠 296 14 0.921 0.954838709677 원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요 302 13 0.921 0.95873015873 법대로 합시다 919 51 0.92 0.947422680412 기본적으로 공감수가 많은 댓글을 상위에 랭크시키는 알고리즘이기 때문에 어뷰징 작업으로 공감수가 부풀려진 댓글이 top 10 밖으로 밀려나지는 못한다. 하지만 vote수가 적더라도 경향성을 파악해 댓글을 정렬시키기 때문에 단순한 순공감이나 공감비율순으로는 하위권에 있던 댓글이 상위권에 위치할 기회를 증가시켰다. 어뷰저 입장에서는 쉽게 계산할 수 있는 정렬방식이 아니기 때문에 조작이 어려워질 것이다. 어뷰징을 할 때 고의로 공감과 비공감을 섞어서 해당 댓글을 상위에 랭크시키는데, Best 정렬이라면 \"적당\"한 비율을 맞추기 까다로워질 것이다. Controversial controversial[10]은 이름 그대로, 공감과 비공감이 팽팽하게 맞서는 댓글을 상위에 위치시키려는 알고리즘이다. 단순히 팽팽하기만 하면 공감과 비공감이 1:1인 상황과 10:10인 상황이 같다고 생각할 수 있기에 vote수도 sorting algorithm에 포함시켜서 10:10이 1:1인 상황보다 더 controversial할 수 있도록 만들어졌다. 아래의 식에서 upvote는 공감을, downvote는 비공감을 의미한다. upvote와 downvote의 차이가 같아서 분모가 같아진 경우에는 그 크기가 큰 쪽이 높고, vote의 크기가 같은 경우에는 차이가 작은 쪽이 높다. \\[ \\text{controversial} = \\frac{match \\times log(match + 1)}{| upvote - downvote | + 1},\\text{ where }match=min(upvote, downvote) \\] python으로 구현한 식이다. import mathdef controversial(upvote, downvote): match = min(upvote, downvote) top = match * math.log(match + 1) bottom = abs(upvote - downvote) + 1 return float(top) / bottom 좀 더 직관적인 이해를 돕기 위해 가공한 아래의 예시를 보자. upvote downvote controversial score 1001 1000 3454.38 999 1000 3450.42 100 100 461.52 101 100 230.76 1000 700 15.24 130 100 14.89 100 130 14.89 1 1 0.69 1 2 0.35 upvote, downvote의 비율이 비슷한 댓글 순서로 정렬되고, 그 비율 내에서는 vote 수가 큰 댓글이 더 위에 놓이게 된다. controversial algorithm을 네이버 뉴스 댓글에 적용해보았다. 예상대로 공감과 비공감 수치가 비슷하면서도 vote수가 많은 댓글이 가장 먼저 보인다. vote수가 작은 이유는 이미 순공감 노출로 인해 vote를 받을 기회를 박탈당한 댓글들이기 때문이다. 수치와는 무관하게 top 10 댓글의 내용은 얼마나 controversial하게 구성되어 있는지 정성적으로 평가해보았다. 보도자료에 대한 찬성은 푸른색, 반대는 붉은색 , 애매한 문장은 표기하지 않았다. controversial하다면 뉴스 기사의 주제에 대해 찬성과 반대가 골고루 섞여있어야 할 것이다. 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합)[11] userId comments 공감수 비공감수 user 1 대통령 각하, ‘사드 문제’ 갖고 거품무는 중국에도 내정 간섭이라고 거침 없이 말씀해주세요 26 26 user 2 이제는 한미일군사훈련을 해야 한다. 81 85 user 3 근데 왜 중국한테는 대놓고 내정간섭 받는거죠, 대통령님? 치욕스러웠던 조선시대가 그리운건가요? 22 22 user 4 봐라 ㅋㅋㅋ 연기하지?철수 얘기나온다 백퍼 ㅋㅋㅋㅋ 베트남꼴 나는거야 ㅋㅋㅋ 정신 좀 차리자 33 31 user 5 아베만도못한 문통; 11 11 user 6 문재인 아가라 닥쳐라. 사드도 내정문제인데 중국한테는 끽소리 못 하던 색히가 어디서 주둥아리 씨부리노. 10 10 user 7 ㅋㅋㅋㅋㅋㅋ 곧 양념단와서 또 평화올림픽 울부짖겠네. 66 57 user 8 미국이 한국을 버려야 할 듯.없네. 15 14 user 9 미국을 대변하는거다.국익을 최우선으로 하는거지싫지만 아베가 똑똑하지않는냐.살자. 8 8 user 10 얼마나 답답하면 저런말을 할지 생각 안해보셨나요?? 북에서 원하는 대로 흘러가네요. 앞으로 한미군사훈련 연기뿐만 아니라 축소되고 없어지고 난리나겠네 8 8 분명 공감수와 비공감수는 controversial하지만 대부분이 당시의 여론과 반대대는 내용으로 치우쳐있다. 정성적으로 controversial한 댓글은 공감: 비공감이 1:1이 아닌 좀더 공감 비율이 높은 비율을 가진다는 사실을 유추해볼 수 있다. 공감비율과 비슷하게 controversial도 vote수가 많은 경우에 불리해진다. controversial의 분모는 upvote와 downvote의 차이값인데, vote수가 많을수록 한두개차이를 유지하기가 어려워진다. 공감 66, 비공감 57을 가진 댓글이 공감 10, 비공감 10보다 아래에 놓인다. New Sorting Algorithms reddit ranking algorithm 중에서 controversial의 문제점을 해결한 새로운 controversial algorithm과 비공감이 많은 의견도 노출하는 best anti 정렬방식을 제안하고자 한다. New controversial 앞서 지적했듯이 controversial은 공감: 비공감의 비율 재조정과 vote 수가 많은 경우 분모값의 기준을 완화시켜야하는 이슈가 있다. 공감 : 비공감 정성적으로 확인해보았을 때 공감: 비공감 = 6.5 : 3.5 정도에서 기사 내용에 대한 찬성과 반대의 댓글이 골고루 등장하였다. 때문에 new controversial에서 upvote와 downvote의 값을 조정해주어야 한다. vote수가 많은 경우 이 문제는 공감비율순과 비슷했다. upvote와 downvote의 절대치에 의존하기보다 wilson score로 도출된 값을 upvote와 downvote로 대체하면 vote수가 많고 적음을 고려하면서도 0과 1 사이의 값을 가지게 되어 upvote와 downvote의 차이에 대한 효과가 완화된다. 변경된 내용을 정리하면 다음과 같다. import mathdef controversial(upvote, downvote): p_up = best(upvote, downvote) * 3.5 p_down = best(downvote, upvote) * 6.5 match = min(p_up, p_down) top = match * math.log(match + 1) bottom = abs(p_up - p_down) + 1 return float(top) / bottom 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합) [11] userId comments 공감수 비공감수 user 11 아베한테 대하듯 똑같이 김정은하고 북한, 중국한테도 당당하게 나와라! 16 11 user 12 개~~새끼 아베 한테는 그렇게 당당하면서 김정은한테는 왜 그렇게 꼬리를 내린다냐? 핵이 무섭긴 무서운가 보다 11 7 user 13 한미 동맹도 좋다 그러나 우리 나라 스스로 강한 나라가 되어야 한다. 문대통형 수고 많으십니다 !! 9 6 user 14 아베에게 일침을 놔주신문 대통령님 지지 합니다.나대지 마시오 9 6 user 15 쪽바리 추종자들 많네!! 특히 벌레 틀딱들~~ 8 5 user 16 반대로 우리나라가 일본보고 자위대 훈련하는거 보고 참견하면 일본이 가많이 있겠냐?벌레들아! 비판을 하려면 국내 내정에 간섭하는 아베를 비판해야지 아베를 두둔하냐? 이 스레기들아... 8 5 user 17 아베가 옳은말했네 지금이라고 김정은 참수 한미연합훈련을 시작하라 빨갱이한테 이 나라를 줄 수 없다 6 4 user 18 대한민국은 다시 한번 망해봐야 정신차리지..안된다. 6 4 user 19 일본이 우방이란애들 멍청한거 아니냐 일본애들도 그렇게 생각안하는데 왜 니혼자 망상해 찐따새끼인가ㅋㅋㅋㅋㅋㅋ 6 4 user 20 문재인씨 당신의 국적은 어디입니까? 다스 실소유주를 밝히는 것보다 훨씬 더 중요한 문제입니다. 6 4 공감 비율을 조금 높여주었을 때 기사 내용에 찬성하는 댓글과 반대하는 댓글이 top 10에 골고루 섞이게 되었다. 또 wilson score로 변환한 상태에서 비율을 조정해주게되어 vote수가 높은 경우에 up과 down의 차이에 덜 민감해질 수 있었다. Best-Anti 꼭 공감수가 많은 것만 괜찮은 의견이라고 볼 수 있을까? 비공감수가 많은 의견 또한 반대 진영의 입장을 대변하는 좋은 의견이라고도 볼 수 있지 않을까? 네이버 뉴스 댓글은 대부분 당시의 여론에 따라 분위기가 흘러간다. 순공감순이든 공감비율순이든 한가지 주장을 다른 방식으로 표현하고 있는 댓글들이 top 10이 된다. 이를 보는 대중은 한쪽의 영향만 받게 되어 생각이 더욱 치우쳐진다. 정치적 다양성을 수용하는 것은 의견의 객관성을 유지하는데에 도움이 된다. 그런 의미에서 당시 여론과 반대되는 내용의 댓글 또한 보여주는 것은 댓글에 영향을 받을 다른 사용자를 위해서도, 플랫폼의 중립성을 담보하기 위해서도 중요하다고 생각한다. Best-Anti는 negative vote에 대한 Wilson score를 구한 것이다. \\[ w_{neg}^- = max(0, \\frac{2n(1-\\hat{p}) + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)}) \\] python 구현식은 다음과 같다. import numpy as npdef best_anti(up, down): try: z = 1.96 # 95% confidence level n = up + down p_up = up / n p_down = 1 - p_up denominator = 2 * (n + z**2) numerator = 2 * n * p_down + z**2 - z * np.sqrt(z**2 + 4 * n * p_up * p_down) lower = numerator / denominator except ZeroDivisionError as e: lower = 0 return max(0, lower) 아베 \"한미군사훈련 예정대로\"…文대통령 \"내정문제 거론 곤란\"(종합)[11] userId comments 공감수 비공감수 user 21 평화협정후 미군철수 바랍니다 0 5 user 22 홍발정씨..트럼프도 좌파 빨갱이죠?? 0 4 user 23 늙다리 미치광이는 빠져 줄래!자주통일좀 하자! 0 4 user 24 자국당은 사형감 많던데... 미국철수 애기했다고 파면? 자국당 5월에는 문정인으로 놀고먹겠군~! 0 4 user 25 봐라. 지도자 하나가 이렇게나 세상을 바꿀 수 있다. 물론 촛불 들고, 직접민주주의를 구현한 국민 또한 위대하지. 지방선거 때 투표 잘 하자. 0 4 user 26 아직도. 미국이 인계철선이라믿고 50년대 사고방식이 존재하는구나 군사력 세계10위안에들고 1-1붙어도 안지니 너무 미군철수로 여론전말고 참신한거없어요 ? 자한당분들? 1 6 user 27 극우 자한당은 미국도 빨갱이란다 제비가 왔다고 봄은 아니람서 ㅋㅋㅋ 0 3 user 28 원샷-빅딜! 0 3 user 29 자한당분들께서 트럼프도 좌파래요.. 0 3 user 30 잊지마세요 지금도 북한은 세계 최악의 인권유린 국가입니다 이시간에도 북한 주민들은 김정은한테 총살당하거나 아오지탄광으로 끌려가고 있습니다 북한 여성들은 김정은의 성노예가 되고 있구요 대한한공 조현민의 갑질 화가나죠 미투운동으로 드러난 권력자들의 성폭력 정말 싫습니다 그런데 이것보다 수백배는 더심한 갑질과 성폭력을 일삼는게 북한 김정은입니다 0 3 Conclusions 현재의 네이버 뉴스 댓글 정렬방식은 공감수가 높은 댓글을 위주로 보여주고 있고, 기준 또한 쉽다. 조작에 들어가는 비용 대비 얻을 수 있는 효과가 큰 상황에서 조작으로 인해 이익을 볼 집단은 당연히 어뷰징을 할 수 밖에 없다. 그리고 이미 조직적인 세력이 되어버린 어뷰저들은 완벽히 차단할 수 없다. 때문에 어뷰징을 해결할 수 있는 가장 좋은 방법은 현재의 정렬 방식의 단점을 극복하면서 자연스럽게 기준이 복잡해지게 만드는 것과 사람들이 조작된 의견에 크게 흔들리지 않을 수 있도록 다양한 의견을 보여주는 것이다. 현재의 네이버 정렬 방식 중 순공감순과 공감비율순이 가지는 한계는 reddit에서 사용하고 있는 best 정렬방식으로 해결된다. 공감수에 가중치를 둔 정렬방식 외에 공감수와 비공감수가 비슷한 댓글에 가중치를 두는 방식, 비공감수에 가중치를 두는 방식을 제안하였다. 한 쪽의 의견만 듣는 것은 언제나 편향된 결과를 야기한다고 생각한다. 한 쪽이 명백히 잘못한 것처럼 보도될 때, 그 반대의 의견에도 귀를 기울일 수 있는 플랫폼이 되길 바란다. Future works 지금까지는 댓글의 contents보다는 댓글에 부과된 공감, 비공감의 interaction 데이터로 문제점과 해결방식을 제안했다. controversial로 의견의 다양성을 추구했지만 text를 보지 않았기 때문에 의견의 다양성을 간접적으로 보장하기엔 불안정할 수 있다. 쇼핑 리뷰에서 가격, 내구성, 디자인 등 다양한 측면을 보여주듯이 정치적 의견도 기사에서 언급된 중요한 단어들에 대한 사람들의 반응을 보는 방식도 생각해보면 좋을 것 같다. Appendix A: Wilson score 사실 본문에서 기술한 내용은 일반적인 Normal approximation interval이다. \\[ p = \\hat{p} \\pm z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] 여기서 \\(\\)은 Bernoulli process의 성공확률을 의미한다. Wilson score는 confidence interval을 \\(\\)가 아닌 \\(p\\)로 추정한 score interval의 최소값이다. \\[ p = \\hat{p} \\pm z\\sqrt{\\frac{p(1-p)}{n}} \\] \\(p\\)에 대해 정리하여 \\(p\\)에 대한 2차방정식을 만든다. \\[ (1 + \\frac{z^2}{n}) p^2 - (2\\hat{p} + \\frac{z^2}{n})p + \\hat{p}^2 = 0 \\] 근의 공식을 사용해 \\(p\\)를 구한다. \\[ p = \\frac{2n\\hat{p} + z^2 \\pm z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)} \\] Wilson score는 \\(p\\)의 lower bound이므로 - 에 대해 정리하면 다음과 같다. \\[ w^- = max(0, \\frac{2n\\hat{p} + z^2 - z\\sqrt{z^2 + 4n\\hat{p}(1-\\hat{p})}}{2(n+z^2)}) = \\text{wilson score} \\] 95%의 신뢰도로 고정하는 경우 z에 1.96을 대입할 수 있다. 그리고 이 경우 본문의 python 함수에서 구현한 best가 된다. References 1.2017년 11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의 호감순처럼 호감도를 “공감-비공감”으로 계산하게 되었다. ↩︎ 2.홍준표 “나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버 뉴스 ↩︎ 3.재판에 넘겨진 조국 부인 정경심 교수…검찰 '소환 임박', 네이버 뉴스 ↩︎ 4.대학교수 이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버 뉴스 ↩︎ 5.https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system ↩︎ 6.http://www.evanmiller.org/how-not-to-sort-by-average-rating.html ↩︎ 7.https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval ↩︎ 8.https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is ↩︎ 9.MB '정치보복' 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버 뉴스 ↩︎ 10.https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence ↩︎ 11.아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스 ↩︎","link":"/2019/09/23/naver-news-Comments-Analysis-3/"},{"title":"Pandas Dataframe의 다양한 iteration 방법 비교","text":"pandas는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에 없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만 오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어 있지 않은 편이다. 이번 글에서는 pandas의 여러 기능 중에서 iteration하는 여러 방법을 속도와 사용성 측면에서 비교해본 내용을 아주 간단하게 정리해 보았다. Summary rank method time iterrows 대비 속도 1 itertuples 7.7ms x8.1 2 at / iat 15.8ms x4 3 loc / iloc 24.6ms x2.5 4 iterrows 62.7ms x1 번외 values 7.1ms x8.8 번외 apply + to_dict 9.91 ms x6.3 Introduction 실험에 사용한 데이터는 아래와 같이 id, text, title 정보로 이루어진 위키피디아를 처리한 table 형식의 데이터이다. text는 위키피디아 문서를 일정 길이 단위로 잘라서 가공한 문장들이고, title은 해당 문장이 속한 위키피디아 문서의 제목을 의미한다. id는 각 문장들의 고유 번호이다. 데이터의 row 별로 iteration을 하면서 처리할 내용은 1) 아래의 cut_text를 통해 text의 길이를 줄이고, 2) table 의 내용을 list_of_dict 형식으로 변환하는 것이다. def cut_text(text, max_len: int = 100): return ' '.join(text.split()[:max_len]) 실험할 함수는 크게 iterrows, loc/iloc, at/iat, itertuples, 그리고 속도 면에서는 장점이 있으나 약간의 단점이 있는 values, 그리고 이번 task 에 overfitting 된 apply + to_dict 가 있다. 하나하나 살펴보도록 하자! iterrows 많이 사용되는 함수이지만 가장 성능이 좋지 않다. %%timeitresult = []for i, row in data.iterrows(): short_text = cut_text(row['text']) instance = { 'id': row['id'], 'text': short_text, 'title': row['title'] } result.append(instance) 62.7 ms ± 729 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) loc / iloc iterrows 다음으로 많이 사용되는 방식이다. iterrows에 비해 2.5배 정도 빠른 속도를 보인다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.loc[idx, 'text']) instance = { 'id': data.loc[idx, 'id'], 'text': short_text, 'title': data.loc[idx, 'title'] } result.append(instance) 24.6 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) :warning: 다만, loc을 잘못 쓰게 되면 iterrows를 썼을 때보다도 더 오랜 시간이 소요된다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.loc[idx]['text']) # diff instance = { 'id': data.loc[idx]['id'], 'text': short_text, 'title': data.loc[idx]['title'] } result.append(instance) 261 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) :warning: 미리 row를 받으면 조금 더 빨라지지만, 그럼에도 iterrows대비 느리다. %%timeitresult = []for idx in data.index: row = data.loc[idx] short_text = cut_text(row['text']) # diff instance = { 'id': row['id'], 'text': short_text, 'title': row['title'] } result.append(instance) 99.4 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) at / iat loc / iloc 과 유사하지만, 특정 column과 row에 해당하는 값을 받고 싶을 때 사용한다. at 함수에 대한 상세한 설명은 pandas 공식 문서에서 확인할 수 있다. iterrows에 비해 4배 정도 빠른 속도를 보인다. %%timeitresult = []for idx in data.index: short_text = cut_text(data.at[idx, 'text']) instance = { 'id': data.at[idx, 'id'], 'text': short_text, 'title': data.at[idx, 'title'] } result.append(instance) 15.8 ms ± 49.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) itertuples iterrows와 유사하지만, Series가 return되는 iterrows와는 다르게 NamedTuple이 return 된다. column에 대응되는 값에 접근하기도 쉽고, 속도도 8배 이상 빠르다. %%timeitresult = []for row in data.itertuples(): short_text = cut_text(row.text) instance = { 'id': row.id, 'text': short_text, 'title': row.title } result.append(instance) 7.7 ms ± 21.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) values 여기서부터는 번외 느낌인데, values는 속도가 가장 빠르다는 장점이 있지만 column에 대응되는 값을 불러올 때 불편한 점이 있다. 이 점을 감안해서 써도 무관하다면 가장 좋은 선택이 될 것 같다. %%timeitresult = []for value in data.values: short_text = cut_text(value[1]) instance = { 'id': value[0], 'text': short_text, 'title': value[2] } result.append(instance) 7.1 ms ± 43.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) apply + to_dict for 문 안에서 처리할 내용이 복잡하지 않은 이번 태스크같은 경우에 쓰기 적합한 방식이다. 새로운 dataframe 혹은 새로운 column을 생성해야 해서 메모리 측면에서 오는 단점은 있지만, 코드가 짧고 깔끔하다는 장점이 있다. %%timeitresult = data.copy()result['text'] = result['text'].apply(lambda x: cut_text(x))result = result.to_dict(orient='records') 9.91 ms ± 19.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) References https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html","link":"/2021/02/04/pandas-dataframe-iteration-methods/"},{"title":"PyTorch의 IterableDataset을 사용해서 데이터 불러오기","text":"PyTorch 1.2 이상부터 torch.utils.data 에서는 크게 map-style dataset (torch.utils.data.Dataset) 과 iterable dataset (torch.utils.data.IterableDataset) 의 두 종류의 데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는 IterableDataset 을 사용하는 것이 좋은데, Dataset 과는 딜리 아직 개발되어야 할 기능이 더 필요한 클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다. Map-style Dataset 1.2 이하 버전에서 사용되던 map-style dataset은 memory에 모든 데이터를 업로드할 수 있을 때 사용하는 가장 일반적인 dataset type 이다. custom dataset class를 생성하고자 할 때 torch.utils.data.Dataset 을 상속받아 __len__ , __getitem__ 을 구현하면 된다. from torch.utils.data import Datasetclass MyMapDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): return self.data['text'][index] Iterable Dataset 하지만 학습 데이터가 메모리에 다 올라가지 않는 경우가 발생할 수 있다. 이 문제를 해결할 수 있는 다양한 방법 중에 하나로, torch.utils.data.IterableDataset 을 사용하는 방법이 있다. Map-style Dataset과 비슷하게 torch.utils.data.IterableDataset 을 상속받아서 custom dataset class를 생성하고, __iter__ 를 선언하면 된다. from torch.utils.data import IterableDatasetclass MyIterableDataset(IterableDataset): def __init__(self, data_path): self.data_path = data_path def __iter__(self): iter_csv = pd.read_csv(self.data_path, sep='\\t', iterator=True, chunksize=1) for line in iter_csv: line = line['text'].item() yield line Dataset이 batch data를 생성할 때 map_dataset[index]를 사용한다면, IterableDataset은 next(iterable_dataset) 을 사용한다. 이 때문에 DataLoader를 통해 IterableDataset을 불러와서 사용하게 되면 sampler 옵션의 사용이 어렵다. 그래서 random suffling 을 하고 싶다면 미리 데이터셋을 shuffling 한 이후에 불러오는 것이 좋다. Going Parallel PyTorch 공식문서에 따르면 IterableDataset을 num_workers &gt; 0의 조건에서 사용할 때 특별히 다음을 유념할 것을 제안하고 있다. When num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset’s __iter__() method or the DataLoader ‘s worker_init_fn option to modify each copy’s behavior. 위의 문장을 이해하려면 num_workers 에 대한 이해와, num_workers &gt; 0 일 때 IterDataset 에서 어떤 현상이 일어나는지 알아야한다. num_workers는 데이터셋을 불러올 때 사용할 subprocess의 개수이다. num_workers == 0 은 main process에서 데이터를 불러오고 모델에 pass하는 작업을 모두 수행한다는 뜻이며, num_workers == 2는 subprocess 2개에서 데이터를 불러오고 main process에서는 subprocess에서 불러온 데이터를 model에 pass하는 역할만 담당한다. 따라서 num_workers &gt; 0 일 때 data loading에서의 병목이 줄어들며 gpu utilization 을 100% 가까이 끌어올릴 수 있다. 그럼, num_workers &gt; 0 일 때 어떤 현상이 발생하는지 살펴보자. Map-Style Dataset from torch.utils.data import DataLoader, Dataset, IterableDatasetimport timeclass MyMapDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): worker = torch.utils.data.get_worker_info() worker_id = worker.id if worker is not None else -1 start = time.time() time.sleep(0.1) end = time.time() return self.data[index], worker_id, start, enddata = range(16)map_dataset = MyMapDataset(data) num_workers == 0 인 경우 loader = DataLoader(map_dataset, batch_size=4, num_workers=0)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])tensor([ 8, 9, 10, 11]) tensor([-1, -1, -1, -1])tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1]) num_workers == 2 인 경우 loader = DataLoader(map_dataset, batch_size=4, num_workers=2)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])tensor([ 8, 9, 10, 11]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) 의도한대로, subprocess 별로 서로 다른 데이터를 불러오는 것을 알 수 있다. Iterable Dataset from torch.utils.data import DataLoader, Dataset, IterableDatasetimport timeclass MyIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for x in self.data: worker = torch.utils.data.get_worker_info() worker_id = worker.id if worker is not None else -1 start = time.time() time.sleep(0.1) end = time.time() yield x, worker_id, start, enddata = range(16)iterable_dataset = MyIterableDataset(data) num_workers == 0 loader = DataLoader(iterable_dataset, batch_size=4, num_workers=0)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])tensor([ 8, 9, 10, 11]) tensor([-1, -1, -1, -1])tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1]) num_workers == 2 loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([0, 1, 2, 3]) tensor([1, 1, 1, 1])tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])tensor([ 8, 9, 10, 11]) tensor([0, 0, 0, 0])tensor([ 8, 9, 10, 11]) tensor([1, 1, 1, 1])tensor([12, 13, 14, 15]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) ⚠️ worker 0과 worker 1에서 같은 데이터를 로딩하고 있다. 이 점이 공식문서에서 지적하고 있는 내용이다. 각 워커별로 같은 __iter__()를 사용하기 때문에 같은 데이터를 로딩하게 된다. 이를 방지하기 위해서는 worker_init_fn 에서 직접 워커 별 데이터를 재분배 시켜줘야 한다. def worker_init_fn(_): worker_info = torch.utils.data.get_worker_info() dataset = worker_info.dataset worker_id = worker_info.id split_size = len(dataset.data) // worker_info.num_workers dataset.data = dataset.data[worker_id * split_size: (worker_id + 1) * split_size] loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2, worker_init_fn=worker_init_fn)for d in loader: batch, worker_ids, starts, ends = d print(batch, worker_ids)-----tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])tensor([ 8, 9, 10, 11]) tensor([1, 1, 1, 1])tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1]) worker_init_fn 을 통해 분배시켜준 결과 worker 0과 worker 1 에서 다른 데이터를 순차적으로 불러오는 것을 알 수 있다 🙂 Conclusions IterableDataset 은 데이터가 메모리에 올라가지 않을만큼 클 때 사용하면 좋다. Dataset과 다르게 __iter__()를 선언해서 데이터를 부른다. 하지만 이 특징 때문에 Sampler 와 함께 사용할 수 없다. 또한 num_workers &gt; 0 인 세팅에서는 각 워커에서 다른 데이터를 불러올 수 있도록 worker_init_fn을 선언해야 한다. References How to Build a Streaming DataLoader with PyTorch | by David MacLeod | Speechmatics | Medium https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html torch.utils.data — PyTorch 1.7.1 documentation","link":"/2021/02/21/pytorch-dataset/"},{"title":"PyTorch의 view, transpose, reshape 함수의 차이점 이해하기","text":"최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래 구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서 의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는 transpose 와 view 의 기능을 헷갈려했기 때문이었다. 두 함수의 차이는 contiguous 를 이해해야 알 수 있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를 바탕으로 정리해보았다. contiguous 란 matrix 의 눈에 보이는 (advertised) 순차적인 shape information 과 실제로 matrix 의 각 데이터가 저장된 위치가 같은지의 여부이다. 아래의 예시에서 t 는 contiguous 하다. 왜냐하면 t[0][0][0] → t[0][0][1] → t[0][1][0] ... 의 데이터 포인터 위치 (0 → 1 → 2 ... ) 또한 연속적이기 때문이다. 아직 이해가 되지 않아도 괜찮다. 예시를 좀 더 보자! t = torch.tensor([[[0, 1], [2, 3], [4, 5]], \\ [[6, 7], [8, 9], [10, 11]], \\ [[12, 13], [14, 15], [16, 17]], \\ [[18, 19], [20, 21], [22, 23]]]) # (4, 3, 2) print(t) >foldedtensor([[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]], [[18, 19], [20, 21], [22, 23]]]) view view 를 통해 t 라는 tensor의 shape를 변경시켜보았다. tv = t.view(4, 2, 3) print(tv) >foldedtensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) shape이 (4, 2, 3) 으로 잘 바뀐 것을 확인할 수 있다. 그리고 tv[0][0][0] → tv[0][0][1] → tv[0][0][2] ... 의 데이터 포인터 위치 (0 → 1 → 2 ... ) 또한 연속적이기 때문에 tv 는 contiguous 하다. tv.is_contiguous()---True 데이터의 tensor index 순서대로 tensor를 flatten 시켜주는 함수를 통해 t 와 tv 를 비교하면 동일하게 나오는 것을 확인할 수 있다. t.flatten() == tv.flatten()---tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]) 또한 t 와 tv 의 데이터는 pointer 값이 동일하여 한 쪽의 tensor data 를 수정하면 다른 쪽의 값도 동시에 변경된다. t.storage().data_ptr() == tv.storage().data_ptr() # data pointer 값이 일치함---True # Modifying view tensor changes base tensor as well.t[0][0][0] = 99tv[0][0][0]---tensor(99) transpose transpose 를 통해 t 라는 텐서의 shape을 변경시켜보았다. shape은 tv와 동일하나, 구성이 조금 다른 것을 확인할 수 있다. 참고로, 보통 (batch_size, hidden_dim, input_dim) 을 (batch_size, input_dim, hidden_dim) 으로 바꿔주는 작업을 할 때에 transpose 를 사용한다. tt = t.transpose(2, 1) # (4, 2, 3) print(tt) >foldedtensor([[[ 0, 2, 4], [ 1, 3, 5]], [[ 6, 8, 10], [ 7, 9, 11]], [[12, 14, 16], [13, 15, 17]], [[18, 20, 22], [19, 21, 23]]]) 앞선 예시에서 t 의 데이터 포인터는 0 → 1 → 2 ... 순서대로 저장된 것을 알 수 있었다. t와 tv 모두 데이터 포인터의 물리적 순서와 shape 상에서의 데이터 순서가 같았기 때문에 contiguous 했다. 하지만 tt 의 경우 0 → 1 → 2 ... ≠ tt[0][0][0] → tt[0][0][1] → tt[0][0][2] ... 이기 때문에 contiguous 하지 않다. tt.is_contiguous()---False tt 를 flatten 시키면 물리적 순서 (0 → 1 → 2 ... ) 와 shape 상의 순서가 다른 것을 확인할 수 있다. tt.flatten()---tensor([ 0, 2, 4, 1, 3, 5, 6, 8, 10, 7, 9, 11, 12, 14, 16, 13, 15, 17, 18, 20, 22, 19, 21, 23]) contiguous 그렇다면 강제로 물리적 위치를 연속적으로 만들어버리면 어떻게 될까? 겉보기에는 tt 와 별 차이가 없어보인다. tt.contiguous() == tt---tensor([[[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]], [[True, True, True], [True, True, True]]]) 하지만 각 데이터 포인터를 비교하면 tt.contiguous() 는 0 → 2 → 4 ... 이고 tt 는 0 → 1 → 2 이기 때문에 아래의 값이 False가 나오는 것을 예상해볼 수 있다. tt.contiguous().storage().data_ptr() == tt.storage().data_ptr()---False reshape contiguous 개념을 이해했다면, reshape 과 view 함수의 차이도 알 수 있다. 쉽게 얘기하면 reshape() == contiguous().view() 와 같다. view 는 contiguous 하지 않은 tensor 에 대해서는 적용할 수 없다. tt.view(4, 3, 2) # tt.shape() == (4, 2, 3)---------------------------------------------------------------------------RuntimeError Traceback (most recent call last)&lt;ipython-input-89-785954c0ff12&gt; in &lt;module&gt;----&gt; 1 tt.view(4, 3, 2) # tt.shape() == (4, 2, 3)RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. tt.contiguous().view(4, 3, 2)---tensor([[[ 0, 2], [ 4, 1], [ 3, 5]], [[ 6, 8], [10, 7], [ 9, 11]], [[12, 14], [16, 13], [15, 17]], [[18, 20], [22, 19], [21, 23]]]) 하지만 reshape 은 강제로 tensor를 contiguous 하게 만들면서 shape을 변경하기 때문에 가능하다. tt.reshape(4, 3, 2)---tensor([[[ 0, 2], [ 4, 1], [ 3, 5]], [[ 6, 8], [10, 7], [ 9, 11]], [[12, 14], [16, 13], [15, 17]], [[18, 20], [22, 19], [21, 23]]]) tt.reshape(4, 3, 2).is_contiguous()---True Summary view : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치할 때 (contiguous) shape을 재구성한다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. transpose : tensor 에 저장된 데이터의 물리적 위치 순서와 상관없이 수학적 의미의 transpose를 수행한다. 즉, 물리적 위치와 transpose가 수행된 tensor 의 index 순서는 같다는 보장이 없으므로 항상 contiguous 하지 않다. reshape : tensor 에 저장된 데이터의 물리적 위치 순서와 index 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다. 이 때문에 항상 contiguous 하다는 성질이 보유된다. References https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2","link":"/2021/03/03/pytorch-view-reshape-transpose/"},{"title":"수학으로 이해하는 양자컴퓨터의 기초","text":"최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은 전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이 때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을 넘나들며 관련 지식을 습득해보았다. 아마 나와 같이 관련 기사나 여러 블로그 글, 유튜브 등을 찾아본 사람들이라면 어렵지 않게 아래의 정보는 얻었을 것이다. n개의 qbit은 bit와 달리 \\(2^n\\)의 state를 표현할 수 있다. superposition이란 동시에 0과 1의 상태를 띠는 성질로, 병렬연산이 가능해져서 고전컴퓨터에 비해 계산 속도의 이점이 생긴다. 텍스트만 보면 \"아 그렇구나.\" 싶은 내용들이다. 이해가 된 것일까 싶었지만 스스로에게 세 질문을 던졌을 때 답하지 못하는 것을 보며 제대로 이해하지 못했음을 인지했다. Q1. n개의 bit로도 \\(2^n\\)을 표현할 수 있는거 아닌가? 3개의 bit가 000, 001, 010, 011, 100, 101, 110, 111 이렇게 8개의 상태를 표현할 수 있으니까. Q2. 양자 세계는 불확정성 원리에 지배받는다고 하는데, 대체 양자컴퓨터로 어떻게 연산하고 있는 것이며, 이 성질이 어떻게 계산 비용을 감소시킬 수 있는걸까? Q3. Entanglement는 qbit들이 어떻게 된 상태인거지? 이 질문들에 제대로 답하기 위해서는 수학이 필요하다는 생각이 들었다. 4차원 이상의 공간을 제대로 시각화하지 못하듯이 양자 세계를 자연어로 표현한다는 것 자체가 말이 되지 않는 것 같았기 때문이다. 그래서 수학으로 설명된 자료를 찾으려고 부던히 애를 썼고, 끝내 \"내 수준에 맞는\" (= 이 글을 읽을 모두가 다 이해할 수 있는) 수학으로 설명된 자료를 찾았다. [slide] 이 영상을 보는 것을 추천하지만 무려 한시간이 넘는지라 글로도 정리를 해보았다. 아래에 기술된 내용은 내 방식대로 위의 영상과 자료를 재구성한 것이다. 사실 이 자료를 다 보더라도 양자컴퓨터에 대해 많은 것을 알았다고 보긴 어렵다. python을 처음 접한 사람이 print(\"Hello World!\")를 성공했다고 해서 python을 잘 알았다고 하기 어려운 것처럼. 그리고 딥러닝에 관심있는 사람이 tutorial을 따라해보며 CNN을 돌려봤다고 해서 딥러닝을 잘 알았다고 하기 어려운 것처럼. 그렇지만 양자 세계에 한 번은 Hello World!를 날려봐야 하지 않을까? Introduction The Deutsch-Jozsa problem 이라는 아주 간단한 문제를 통해 양자컴퓨터가 고전컴퓨터에 비해 어떻게 연산 속도에서 이점을 보이는지 알아보려고 한다. 이 과정을 이해하기 위해 양자컴퓨터가 연산하는 방법에 대해 소개할 것이며 matrix 연산과 기초적인 논리회로에 대한 내용을 짚고 넘어갈 것이다. 추가로, entanglement에 대한 간단한 설명이 있다. Basics Qubit / Qbit Qubit 혹은 Qbit은 양자컴퓨터 계산의 기본적인 단위이다. 조금이라도 양자컴퓨터에 대해 알아본 사람들이라면 qbit은 지겹도록 보고 들었을 것이다. Qbit은 언제나 다음의 조건을 만족시킨다. A qbit, represented by \\(\\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}\\) where \\(\\alpha\\) and \\(\\beta\\) are complex numbers must be constrained by the equation \\(||\\alpha||^2 + ||\\beta||^2 = 1\\) 따라서 아래의 예시들은 qbit에 해당된다. \\(\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\) \\(\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}\\) \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\) \\(\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\\) 그리고 이 모든 벡터들의 basis가 되는 \\(\\begin{pmatrix} 1 \\\\0 \\end{pmatrix}\\)과 \\(\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\)은 각각 \\(\\mid 0\\rangle\\)과 \\(\\mid 1\\rangle\\)이라는 특별한 기호로 정의한다. Superposition 양자컴퓨터의 qbit을 설명할 때 빠지지 않는 성질이다. \"동시에 0과 1을 가진다.\"는 문장으로 자주 설명되지만 이보다는 슈뢰딩거의 고양이 느낌이 물씬 나는 \"When we measure a qbit, it collapses to an actual value of 0 or 1.\" 이라는 문장이 더 좋은 설명인 것 같다. 위에서 qbit이라고 언급했던 벡터 하나를 예시로 들어보자. \\[\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\] 이 qbit은 \\(0\\) 혹은 \\(1\\)로 collapse될 확률이 \\(\\frac{1}{2}\\) ( \\(= || \\frac{1}{\\sqrt{2}} || ^2\\)) 이다. 감사하게도 IBM은 자사의 양자컴퓨터를 사용할 수 있는 API를 만들어 놓았다. 여기서 이 qbit을 만들고 1024번 관측해보면 0과 1이 50%씩 나오는 것을 확인할 수 있다. \\(\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}\\) 은 \\(0\\)으로 collapse 될 확률이 \\(\\frac{1}{4}\\), \\(1\\)로 collapse 될 확률이 \\(\\frac{3}{4}\\)인 qbit이다. \\(|0\\rangle\\)은 0으로만 collapse 한다. Tensor product 여러 개의 qbit을 나타내기 위해 Tensor product 개념이 필요하다. 수학적으로 엄밀한 표현은 아니지만, n개의 qbit 연산을 표현하기 위해서는 아래의 표기 방식을 따르는 것이 좋다. \\[ \\binom{x_0}{x_1} \\otimes \\binom{y_0}{y_1} = \\begin{pmatrix} x_0 \\binom{y_0}{y_1} \\\\ x_1 \\binom{y_0}{y_1} \\end{pmatrix} = \\begin{pmatrix} x_0 y_0 \\\\ x_0 y_1 \\\\ x_1 y_0 \\\\ x_1 y_1 \\end{pmatrix} \\] 이를 응용하면 2개, 3개의 qbit도 벡터처럼 표현할 수 있다. \\[ |01\\rangle = \\binom{1}{0} \\otimes \\binom{0}{1} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\hspace{10pt} |100\\rangle = \\binom{0}{1} \\otimes \\binom{1}{0} \\otimes \\binom{1}{0} = \\begin{pmatrix} 0\\\\ 0\\\\0\\\\0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] 이와 같이 tensor product의 결과로 표현된 벡터는 product state라고 한다. 여기서 우리는 \\(n\\)개 qbit의 product state 크기가 \\(2^n\\) 이라는 것을 알 수 있다. 만약 \\( \\binom{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}} \\otimes \\binom{\\frac{1}{\\sqrt{2}}}{\\frac{1}{\\sqrt{2}}} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\) 의 multiple qbits 이 있다면 \\(\\mid 00\\rangle\\), \\(\\mid 01\\rangle\\), \\(\\mid 10\\rangle\\), \\(\\mid 11\\rangle\\)으로 collapse될 확률이 모두 \\(\\frac{1}{4}\\)이므로 동시에 4개의 state를 표현할 수 있게 된다. 즉, qbit이 bit와는 다르게 \\(2^n\\)개의 state를 표현할 수 있다고 한 것은 동시에 가질 수 있는 최대 state 관점에서 비교한 것이다. bit는 절대로 동시에 2개 이상의 state를 가질 수 없으므로 한 번에 계산할 수 있는 정보는 1개 뿐이다. 또한 product state는, 뒤의 entanglement와 구분되는 중요한 성질로, 독립적인 state들로 factorize가 가능하다는 점이 있다. Multiple qbits의 product state 또한 single qbit과 같은 성질을 만족시킨다. \\[ \\binom{a}{b} \\otimes \\binom{c}{d} = \\begin{pmatrix} ac \\\\ ad \\\\ bc \\\\ bd \\end{pmatrix} \\] \\[ \\text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1 \\] 1-bit operations 1-bit에서 가능한 연산은 Identity, Negation, Constant-0, Constant-1의 총 4가지가 있다. 각각의 연산은 matrix로 표현할 수 있다. \\[ \\text{Identity} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\] \\[ \\text{Negation} = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] \\[ \\text{Contant-0} = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 0 \\end{pmatrix} \\] \\[ \\text{Contant-1} = \\begin{pmatrix} 0 &amp; 0 \\\\ 1 &amp; 1 \\end{pmatrix} \\] CNOT (one of the 2-bit operations) CNOT 연산은 control bit와 target bit로 구성된 2-bit가 있을 때 control bit가 0이면 target bit를 바꾸지 않고, control bit가 1일 때 target bit를 바꾸는 연산이다. 마찬가지로 이 연산도 matrix로 표현할 수 있다. \\[ C = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\] 2.4와 2.5에서 operation들을 matrix화 한 것에 주목하자. 확률이 지배하는 양자 세계에서 deterministic한 연산을 하기 위해서는 matrix를 관측하지 않은 qbit에 곱하는 것이 유일한 방법이기 때문이다. 아래의 예시에서 우리가 확신할 수 있는 정보는 qbit이 0과 1로 관측될 확률이 반대가 되었다는 것이다. \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\] 항상 0 혹은 1로 관측되는 \\(\\mid0\\rangle\\)이나 \\(\\mid1\\rangle\\)을 쓰면 matrix 연산을 고집하지 않아도 되지만 이런 qbit만 사용할거라면 고전컴퓨터를 쓰면 그만이다. 굳이 0K 가까이 되는 험악한 조건을 유지해가며 계산할 필요가 없다. 그래서 matrix 연산은 양자 컴퓨팅에서 굉장히 중요하다. 여기에는 한가지 추가조건이 있는데, 반드시 연산에 사용되는 matrix는 reversible해야한다는 것이다. 따라서 앞서 본 1-bit 연산 중 Constant-1과 Constant-0를 계산하기 위해서는 단순 matrix를 곱하는 것 외의 다른 방법이 필요하다. The Deutsch-Jozsa problem 이 문제[1]는 양자컴퓨터가 고전컴퓨터에 비해 계산적인 이점을 가지는 아주 간단한 (동시에 쓸데없는) 문제다. 1-bit를 입력받아서 1-bit를 내뱉는 어떤 함수가 있다고 하자. 만약 이 함수가 Constant(Contant-0, Constant-1)인지, 아니면 Variable(Identity, Negation)인지 알기 위해서는 최소 몇 번의 query를 날려야 할까? Classical computer 고전컴퓨터에서는 0과 1을 입력해야하므로 총 두 번의 연산이 필요하다. Quantum computer 예상했듯이 정답은 한 번이다. 왜인지 알기위해서는 추가적인 개념이 필요하다. Hadamard gate 앞서 언급된 적 있는 H gate이다. \\[ H = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] Hadamard gate는 0- 혹은 1-qbit을 받아서 0과 1을 같은 확률로 가지는 qbit으로 바꿔준다. \\[ H\\mid0\\rangle = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\] \\[ H\\mid1\\rangle = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] Hadamard gate는 또 다른 중요한 성질이 있다. 0과 1을 같은 확률로 가지는 qbit을 다시 0- 과 1-qbit으로 돌려보낸다는 것이다. \\[ \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] X gate X gate는 qbit의 위 아래를 바꿔준다. \\[ X = \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\] \\[ \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\frac{-1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] H gate와 X gate 연산을 이해하기 쉽게 표현하면 아래의 그림이 된다. 붉은색이 X gate, 노란색이 H gate 연산의 방향이다. \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\)에 X - H - X - H - X gate를 씌운 결과는 그림으로 보면 더 쉽게 이해된다. \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\)에서 출발해 각 gate가 연산되는 방향으로 화살표를 움직이면 마지막에 도달하는 곳이 결과값이 된다. non-reversible matrix 앞서 양자컴퓨터는 non-reversible한 matrix 를 곱하는 연산은 불가능하다고 했다. 1-bit 연산 중에서 Constant-0과 Constant-1은 non-reversible하다. 그래서 양자 컴퓨팅에서는 2개의 qbit을 사용한다. 이 그림의 input과 output notation을 보면 \"응?\" 이라는 생각이 절로 들 것이다. 영상에서도 사람들이 대체 왜 \"Output\"이 Input 쪽에 가 있는지에 대해 끊임없이 묻는다. 아쉽게도 강연자는 속시원하게 답변을 해주지 않아서 그런가보다 하고 넘어갔는데 다시 보니 이해가 된 부분이 있어 글로 설명해보려고 한다. Input'과 Output'이 실제 1-bit 연산의 input과 output을 나타낸다. 그리고 Input과 Output은 Input'과 Output'이 BB 이후에 있기 때문에 BB 이전에 Input'과 Output'이 1-bit 연산의 input과 output이 되도록 넣어주는, 양자컴퓨터 연산 방식 때문에 필요한 input들이다. 이 약속에 따라서 양자컴퓨터가 1-bit 연산을 어떻게 수행하는지 아래의 예시를 통해 좀 더 이해해보자. Constant-0은 Input'이 \\(\\mid 0 \\rangle\\)일 때와 \\(\\mid 1 \\rangle\\)일 때 모두 Output'이 \\(\\mid 0 \\rangle\\)이어야 한다. 어떤 gate도 없는 왼쪽 위 그림의 회로에서 Input에 \\(\\mid 0 \\rangle\\) 혹은 \\(\\mid 1 \\rangle\\)을 대입해보면 Input'과 Output'이 Constant-0의 관계를 가지는 것을 확인할 수 있다. Indentity는 Input'이 \\(\\mid 0 \\rangle\\)일 때는 Output'이 \\(\\mid 0 \\rangle\\)이고 Input'이 \\(\\mid 1 \\rangle\\)일 때는 Output'이 \\(\\mid 1 \\rangle\\)인 함수다. 왼쪽 아래 그림의 회로는 CNOT gate를 표현하고 있다. 색이 채워진 원이 control bit 쪽을 나타내고 그렇지 않은 쪽 원은 target bit를 나타낸다. Input이 \\(\\mid 0 \\rangle\\) 이면 control bit가 0이므로 target bit도 그대로 유지한다. 그래서 Input'도 \\(\\mid 0 \\rangle\\), Output'도 \\(\\mid 0 \\rangle\\)이 된다. Input이 \\(\\mid 1 \\rangle\\) 이면 control bit가 1이므로 target bit가 바뀐다. 그래서 Input'도 \\(\\mid 1 \\rangle\\), Output'도 \\(\\mid 1 \\rangle\\)이 된다. 그럼 다시 The Deutsch-Jozsa problem로 돌아가서, 양자컴퓨터에서는 어떻게 한 번에 구할 수 있을까? 정답은 아래의 그림이 설명해준다. 이 연산대로라면 BB가 Constant(Contant-0, Constant-1)이었을 경우, 측정 결과가 \\(\\mid11\\rangle\\)이고, Variable(Identity, Negation)이었을 경우에는 \\(\\mid01\\rangle\\)이 된다. BB의 경우의 수를 따져가며 이해해보자. preprocessing (BB 입력 직전까지의 연산) BB에 들어가기 전 input (\\(\\mid 0 \\rangle\\)) 과 output qbit (\\(\\mid 0 \\rangle\\)) 모두 X와 H gate를 거쳐서 \\(\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix}\\) 가 된다. case 1) BB가 Constant-0 이었을 경우 Constant-0은 input과 output에 어떤 gate도 씌우지 않는다. 따라서 BB가 Constant-0이었을 때 Input과 Output은 H gate만 통과한 이후 관측된다. case 2) BB가 Contstant-1 이었을 경우 Constant-1은 output에만 X gate를 적용한다. 따라서 BB가 Constant-1이었을 때는 Output에 X gate가 추가되고, 이후 Input과 Output 모두에 H gate가 적용된다. case 3) BB가 Identity 이었을 경우 Identity는 CNOT gate를 통해 연산된다. 앞에서 CNOT 연산은 \\( \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\) 을 곱하는 것과 같다고 설명했다. Preprocessing을 거친 Input과 Output은 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 이므로 CNOT연산은 아래와 같이 표현할 수 있다. \\[ C \\begin{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\end{pmatrix} = C \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{-1}{2} \\\\ \\frac{-1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\] 즉 Input은 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 에서 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\\) 로 바뀌고 Output은 그대로 \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{pmatrix} \\) 이 된다. 이 상태에서 H gate가 각각 적용되어 최종 결과는 \\(\\mid 01 \\rangle\\)이 된다. case 4) BB가 Negation 이었을 경우 Negation은 Indentity의 결과 중 Output에만 X gate가 추가되는 연산이다. 따라서 아래 그림처럼 연산이 이루어지고 Identity와 마찬가지로 최종 결과는 \\(\\mid 01 \\rangle\\)이 된다. 정리하면, 양자컴퓨터에서는 특정 설계 상황에서 고정된 BB input에 대한 BB output을 \"한 번\"만 관측하면 BB가 Constant인지 Variable인지 확인할 수 있다는 것이다! Entanglement Entanglement는 지금까지의 흐름에서는 동떨어진 이야기지만 양자컴퓨터에서 항상 소개되는 내용이기 때문에 추가하였다. 앞서 qbit과 product state의 성질을 수학적으로 나타낸 것처럼 entanglement도 수학적인 성질로 표현할 수 있다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\) 는 entangle된 qbit인데, 그 모양새가 product state와 닮아있다. 하지만 product state와는 중요한 성질에서 차이를 보인다. 위에서 설명했듯이 product state는 개별적인 qbit으로 factorize된다. 하지만 entanlged qbit은 개별적인 qbit으로 factorize 되지 않는다. (If the product state of two qbits cannot be factored, they are said to be entanlged.) 이 때문에 entangled qbit은 차원이 늘어난 하나의 qbit으로 볼 수 있으며 일부를 관측했을 때 나머지 일부의 상태가 유추된다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\) 이 entangle 되었음을 증명하는 것은 간단하다. \\( \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\otimes \\begin{pmatrix} c \\\\ d \\end{pmatrix} \\) 를 만족하는 \\(a\\), \\(b\\), \\(c\\), \\(d\\)는 존재하지 않기 때문에 이는 entanlge되어 있는 qbit이다. Entanlged qbit은 CNOT과 H gate를 통해 쉽게 생성할 수 있다. \\[ CH_1 \\begin{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\end{pmatrix} = C \\begin{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\otimes \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\] 만약 이후에 이런 게이트의 조합을 본다면 곧바로 'entanlge 되었군!' 이라고 생각하면 된다 :) Conclusion 개인적으로 이 영상을 본 이후, 속이 뻥 뚫리는 기분이 들었다. 아직 matrix로 표현되는 qbit이 물리적으로 어떤 모습인지, gate들이 물리적으로 어떻게 qbit에 적용되는지는 모르지만 (이건 실제 양자컴퓨터를 눈으로 보면 이해가 되지 않을까) 이 정도라도 양자컴퓨터와 고전컴퓨터의 연산과정에서의 차이를 구체적으로 알 수 있었기 때문에 만족할 수 있었다. 양자컴퓨터의 연산 과정을 이해하고나니 양자 우월성은 그냥 달성되는 것은 아니었으며, 잘 설계된 gate가 뒷받침되었을 때 가능한 것임을 깨닫게 되기도 했다. 이 정도면 양자 세계에 Hello World!를 했다고 볼 수 있지 않을까? References 1.https://en.wikipedia.org/wiki/Deutsch–Jozsa_algorithm#Problem_statement ↩︎","link":"/2019/11/07/quantum-computings/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"data analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"startup","slug":"startup","link":"/tags/startup/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"quantum computing","slug":"quantum-computing","link":"/tags/quantum-computing/"}],"categories":[{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"Startup","slug":"Startup","link":"/categories/Startup/"},{"name":"ML","slug":"Tech/ML","link":"/categories/Tech/ML/"},{"name":"Engineering","slug":"Tech/Engineering","link":"/categories/Tech/Engineering/"},{"name":"Quantum Computing","slug":"Tech/Quantum-Computing","link":"/categories/Tech/Quantum-Computing/"}]}