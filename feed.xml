<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Space Moon</title>
  <icon>https://www.gravatar.com/avatar/cbfa1112a0cbccdd92da7e3c728d4a63</icon>
  
  <link href="/feed.xml" rel="self"/>
  
  <link href="https://inmoonlight.github.io/"/>
  <updated>2023-11-02T02:50:49.145Z</updated>
  <id>https://inmoonlight.github.io/</id>
  
  <author>
    <name>Jihyung Moon</name>
    <email>mjihyung@gmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</title>
    <link href="https://inmoonlight.github.io/2021/07/11/Git-merge-strategy/"/>
    <id>https://inmoonlight.github.io/2021/07/11/Git-merge-strategy/</id>
    <published>2021-07-10T16:03:00.000Z</published>
    <updated>2023-11-02T02:50:49.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는다양한 방법들을 제공한다. 이러한 방법들을 <strong>Merge전략</strong>이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase andMerge에 대해 소개하려고 한다.</p><a id="more"></a><p><img src="/assets/images/git-merge-strategy-base.png?style=centerme" alt="현재 브랜치와 commit 상태" width="53%"></p><p>위의 그림과 같은 상태의 commit이 생성되었다고 가정하자.<code>feat/multiply</code>라는 브랜치가 있고,<code>feat/sum</code>이라는 브랜치가 있다. 각 commit 내의 숫자는commit의 global 순서를 나타낸다.</p><h2 id="merge-commit">Merge Commit</h2><p><img src="/assets/images/git-merge.png?style=centerme" alt="(좌) Merge Commit의 개념도 (우) Merge Commit 이후 Github에서의 commit log" width="92%"></p><p>브랜치의 commit log와 merge log가 동시에 기록된다. Commit log는commit을 행한 순서대로 기록되고, merge log는 merge가 된 순서대로기록된다. 동시에 기록되기 때문에 commit log가 verbose해지며 commit log의순서가 merge 순서와 다르기 때문에 history 관리 및 이해가 어렵다.</p><h2 id="squash-and-merge">Squash and Merge</h2><p><img src="/assets/images/git-squash-merge.png?style=centerme" alt="(좌) Squash and Merge의 개념도 (우) Squash and Merge 이후 Github에서의 commit log" width="82%"></p><p>Merge된 순서대로 master/main 브랜치에 기록된다. 그리고 작업 완료된브랜치의 commit은 새로운 commit 으로 모두 squash되며, 새로운 commit의제목은 PR 제목이 되고, 합쳐진 commit의 제목은 새로운 commit의 상세내용이 된다.</p><p>이러한 특징 때문에 master/main 브랜치의 히스토리 관리가 쉬우나,atomic commit level로 rollback 하는 것은 불가능하다.</p><h2 id="rebase-and-merge">Rebase and Merge</h2><p><img src="/assets/images/git-rebase-merge.png?style=centerme" alt="(좌) Rebase and Merge 이후 main 브랜치의 변화 (우) Rebase and Merge 이후 Github에서의 commit log" width="60%"></p><p>Commit 순서가 아닌 merge 순서대로 기록된다. 그래서 하나의 PR에 담긴commit message가 다른 PR의 commit message와 섞이지 않는다. 그리고 rebase덕분에 merge된 이후의 로그를 보았을 때 하나의 브랜치에서 연속적으로작업한 것과 같은 로그를 확인할 수 있다. 이 때문에 얼마든지 항상 원하는수준으로 rollback 이 가능하다.</p><p>하지만 잘 적용하기 위해서는 commit을 생성할 때부터 올바른 commit단위로 분리해야 하며, commit message 또한 설명력을 가지고 있어야 한다.그리고 다른 PR이 먼저 merge되는 경우, rebase 작업이 필요할 수 있고 이 때발생할 수 있는 conflict를 잘 해결할 수 있어야 한다.</p><h2 id="summary">Summary</h2><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>Merge Strategy</th><th>Pros</th><th>Cons</th></tr></thead><tbody><tr class="odd"><td><strong>Merge Commit</strong></td><td>아직 찾지 못함</td><td>불필요한 commit message가 생기고 merge 순서와 commit 순서가 별도로기록되어 history 관리가 어려움</td></tr><tr class="even"><td><strong>Squash and Merge</strong></td><td>Commit 단위 별로 꼼꼼하게 관리하지 않아도 PR title 만 제대로관리하면 history가 깔끔하게 정리됨</td><td>Atomic level의 rollback이 어려움</td></tr><tr class="odd"><td><strong>Rebase and Merge</strong></td><td>Atomic level의 rollback이 용이하며 commit 단위의 history 기록이됨</td><td>Commit을 잘 다루지 못하는 경우, rebase에 익숙하지 않은 경우 어려움이발생</td></tr></tbody></table><p>앞서 소개한 전략들의 장/단점을 정리하면 위와 같다. 개인적으로는 아직<strong>Merge Commit</strong>의 장점을 발견하지 못했다. 결론적으로 팀원전체가 git을 다루는데에 굉장히 익숙해서 commit 단위, commit message,rebase 등에 어려움이 없는 경우, 혹은 atomic level의 rollback이 필요한개발 상황에서 <strong>Rebase and Merge</strong>가 선호된다. 하지만atomic level 까지의 rollback은 필요하지 않고 팀이 이제 막 git으로버전관리하는 법을 배우기 시작했다면 <strong>Squash and Merge</strong>가좋을 것이다.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는
다양한 방법들을 제공한다. 이러한 방법들을 &lt;strong&gt;Merge
전략&lt;/strong&gt;이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장
많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase and
Merge에 대해 소개하려고 한다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ops" scheme="https://inmoonlight.github.io/categories/Ops/"/>
    
      <category term="Git" scheme="https://inmoonlight.github.io/categories/Ops/Git/"/>
    
    
      <category term="git" scheme="https://inmoonlight.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</title>
    <link href="https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/"/>
    <id>https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/</id>
    <published>2021-03-02T19:22:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는<code>transpose</code> 와 <code>view</code> 의 기능을 헷갈려했기때문이었다. 두 함수의 차이는 <code>contiguous</code> 를 이해해야 알 수있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를바탕으로 정리해보았다.</p><a id="more"></a><p><code>contiguous</code> 란 matrix 의 눈에 보이는 (advertised)순차적인 shape information 과 실제로 matrix 의 각 데이터가 저장된 위치가같은지의 여부이다. 아래의 예시에서 <code>t</code> 는<code>contiguous</code> 하다. 왜냐하면 <code>t[0][0][0]</code> →<code>t[0][0][1]</code> → <code>t[0][1][0]</code> ... 의 데이터 포인터위치 (<code>0</code> → <code>1</code> → <code>2</code> ... ) 또한연속적이기 때문이다. 아직 이해가 되지 않아도 괜찮다. 예시를 좀 더보자!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.tensor([[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>]], \</span><br><span class="line">                 [[<span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>]], \</span><br><span class="line">                 [[<span class="number">12</span>, <span class="number">13</span>], [<span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>]], \</span><br><span class="line">                 [[<span class="number">18</span>, <span class="number">19</span>], [<span class="number">20</span>, <span class="number">21</span>], [<span class="number">22</span>, <span class="number">23</span>]]])  <span class="comment"># (4, 3, 2)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(t) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  1],</span><br><span class="line">         [ 2,  3],</span><br><span class="line">         [ 4,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7],</span><br><span class="line">         [ 8,  9],</span><br><span class="line">         [10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13],</span><br><span class="line">         [14, 15],</span><br><span class="line">         [16, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 19],</span><br><span class="line">         [20, 21],</span><br><span class="line">         [22, 23]]])</span><br></pre></td></tr></table></figure><h2 id="view">view</h2><p><code>view</code> 를 통해 <code>t</code> 라는 tensor의 shape를변경시켜보았다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tv = t.view(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(tv) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7,  8],</span><br><span class="line">         [ 9, 10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13, 14],</span><br><span class="line">         [15, 16, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 19, 20],</span><br><span class="line">         [21, 22, 23]]])</span><br></pre></td></tr></table></figure><p>shape이 <code>(4, 2, 3)</code> 으로 잘 바뀐 것을 확인할 수 있다.그리고 <code>tv[0][0][0]</code> → <code>tv[0][0][1]</code> →<code>tv[0][0][2]</code> ... 의 데이터 포인터 위치 (<code>0</code> →<code>1</code> → <code>2</code> ... ) 또한 연속적이기 때문에<code>tv</code> 는 <code>contiguous</code> 하다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tv.is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>데이터의 tensor index 순서대로 tensor를 flatten 시켜주는 함수를 통해<code>t</code> 와 <code>tv</code> 를 비교하면 동일하게 나오는 것을확인할 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.flatten() == tv.flatten()</span><br><span class="line">---</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,</span><br><span class="line">        <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><p>또한 <code>t</code> 와 <code>tv</code> 의 데이터는 pointer 값이동일하여 한 쪽의 tensor data 를 수정하면 다른 쪽의 값도 동시에변경된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.storage().data_ptr() == tv.storage().data_ptr()  <span class="comment"># data pointer 값이 일치함</span></span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Modifying view tensor changes base tensor as well.</span></span><br><span class="line">t[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">99</span></span><br><span class="line">tv[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">---</span><br><span class="line">tensor(<span class="number">99</span>)</span><br></pre></td></tr></table></figure><h2 id="transpose">transpose</h2><p><code>transpose</code> 를 통해 <code>t</code> 라는 텐서의 shape을변경시켜보았다. shape은 <code>tv</code>와 동일하나, 구성이 조금 다른것을 확인할 수 있다. 참고로, 보통<code>(batch_size, hidden_dim, input_dim)</code> 을<code>(batch_size, input_dim, hidden_dim)</code> 으로 바꿔주는 작업을 할때에 <code>transpose</code> 를 사용한다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt = t.transpose(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># (4, 2, 3)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(tt) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  2,  4],</span><br><span class="line">         [ 1,  3,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  8, 10],</span><br><span class="line">         [ 7,  9, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 14, 16],</span><br><span class="line">         [13, 15, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 20, 22],</span><br><span class="line">         [19, 21, 23]]])</span><br></pre></td></tr></table></figure><p>앞선 예시에서 <code>t</code> 의 데이터 포인터는 <code>0</code> →<code>1</code> → <code>2</code> ... 순서대로 저장된 것을 알 수 있었다.<code>t</code>와 <code>tv</code> 모두 데이터 포인터의 물리적 순서와shape 상에서의 데이터 순서가 같았기 때문에 <code>contiguous</code> 했다.하지만 <code>tt</code> 의 경우 <code>0</code> → <code>1</code> →<code>2</code> ... ≠ <code>tt[0][0][0]</code> → <code>tt[0][0][1]</code>→ <code>tt[0][0][2]</code> ... 이기 때문에 <code>contiguous</code> 하지않다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p><code>tt</code> 를 flatten 시키면 물리적 순서 (<code>0</code> →<code>1</code> → <code>2</code> ... ) 와 shape 상의 순서가 다른 것을확인할 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.flatten()</span><br><span class="line">---</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">7</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">13</span>, <span class="number">15</span>, <span class="number">17</span>,</span><br><span class="line">        <span class="number">18</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">19</span>, <span class="number">21</span>, <span class="number">23</span>])</span><br></pre></td></tr></table></figure><h2 id="contiguous">contiguous</h2><p>그렇다면 강제로 물리적 위치를 연속적으로 만들어버리면 어떻게 될까?겉보기에는 <code>tt</code> 와 별 차이가 없어보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous() == tt</span><br><span class="line">---</span><br><span class="line">tensor([[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure><p>하지만 각 데이터 포인터를 비교하면 <code>tt.contiguous()</code> 는<code>0</code> → <code>2</code> → <code>4</code> ... 이고<code>tt</code> 는 <code>0</code> → <code>1</code> → <code>2</code> 이기때문에 아래의 값이 False가 나오는 것을 예상해볼 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous().storage().data_ptr() == tt.storage().data_ptr()</span><br><span class="line">---</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="reshape">reshape</h2><p><code>contiguous</code> 개념을 이해했다면, <code>reshape</code> 과<code>view</code> 함수의 차이도 알 수 있다. 쉽게 얘기하면<code>reshape() == contiguous().view()</code> 와 같다.</p><p><code>view</code> 는 <code>contiguous</code> 하지 않은<code>tensor</code> 에 대해서는 적용할 수 없다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.view(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># tt.shape() == (4, 2, 3)</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">RuntimeError                              Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-89</span><span class="number">-785954</span>c0ff12&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 tt.view(4, 3, 2)  # tt.shape() == (4, 2, 3)</span><br><span class="line"></span><br><span class="line">RuntimeError: view size <span class="keyword">is</span> <span class="keyword">not</span> compatible <span class="keyword">with</span> input tenso<span class="string">r's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous().view(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">---</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6</span>,  <span class="number">8</span>],</span><br><span class="line">         [<span class="number">10</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">17</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><p>하지만 <code>reshape</code> 은 강제로 tensor를<code>contiguous</code> 하게 만들면서 shape을 변경하기 때문에가능하다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.reshape(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">---</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6</span>,  <span class="number">8</span>],</span><br><span class="line">         [<span class="number">10</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">17</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.reshape(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>).is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="summary">Summary</h2><ul><li><code>view</code> : tensor 에 저장된 데이터의 물리적 위치 순서와index 순서가 일치할 때 (<code>contiguous</code>) shape을 재구성한다. 이때문에 항상 <code>contiguous</code> 하다는 성질이 보유된다.</li><li><code>transpose</code> : tensor 에 저장된 데이터의 물리적 위치순서와 <strong>상관없이</strong> 수학적 의미의 transpose를 수행한다. 즉,물리적 위치와 transpose가 수행된 tensor 의 index 순서는 같다는 보장이없으므로 항상 <code>contiguous</code> 하지 않다.</li><li><code>reshape</code> : tensor 에 저장된 데이터의 물리적 위치 순서와index 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다.이 때문에 항상 <code>contiguous</code> 하다는 성질이 보유된다.</li></ul><h2 id="references">References</h2><ul><li><a href="https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html">https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html</a></li><li><a href="https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2" rel="external nofollow noopener noreferrer" target="_blank">https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래
구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서
의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는
&lt;code&gt;transpose&lt;/code&gt; 와 &lt;code&gt;view&lt;/code&gt; 의 기능을 헷갈려했기
때문이었다. 두 함수의 차이는 &lt;code&gt;contiguous&lt;/code&gt; 를 이해해야 알 수
있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를
바탕으로 정리해보았다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="PyTorch" scheme="https://inmoonlight.github.io/categories/ML/PyTorch/"/>
    
    
      <category term="pytorch" scheme="https://inmoonlight.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch의 IterableDataset을 사용해서 데이터 불러오기</title>
    <link href="https://inmoonlight.github.io/2021/02/21/PyTorch-IterableDataset/"/>
    <id>https://inmoonlight.github.io/2021/02/21/PyTorch-IterableDataset/</id>
    <published>2021-02-21T14:22:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>PyTorch 1.2 이상부터 <code>torch.utils.data</code> 에서는 크게map-style dataset (<code>torch.utils.data.Dataset</code>) 과 iterabledataset (<code>torch.utils.data.IterableDataset</code>) 의 두 종류의데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는<code>IterableDataset</code> 을 사용하는 것이 좋은데,<code>Dataset</code> 과는 딜리 아직 개발되어야 할 기능이 더 필요한클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다.</p><a id="more"></a><h2 id="map-style-dataset">Map-style Dataset</h2><p>1.2 이하 버전에서 사용되던 map-style dataset은 memory에 모든 데이터를업로드할 수 있을 때 사용하는 가장 일반적인 dataset type 이다. customdataset class를 생성하고자 할 때 <code>torch.utils.data.Dataset</code>을 상속받아 <code>__len__</code> , <code>__getitem__</code> 을 구현하면된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[<span class="string">'text'</span>][index]</span><br></pre></td></tr></table></figure><h2 id="iterable-dataset">Iterable Dataset</h2><p>하지만 학습 데이터가 메모리에 다 올라가지 않는 경우가 발생할 수 있다.이 문제를 해결할 수 있는 다양한 방법 중에 하나로,<code>torch.utils.data.IterableDataset</code> 을 사용하는 방법이 있다.Map-style Dataset과 비슷하게<code>torch.utils.data.IterableDataset</code> 을 상속받아서 customdataset class를 생성하고, <code>__iter__</code> 를 선언하면 된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> IterableDataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span><span class="params">(IterableDataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_path)</span>:</span></span><br><span class="line">        self.data_path = data_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        iter_csv = pd.read_csv(self.data_path, sep=<span class="string">'\t'</span>, iterator=<span class="literal">True</span>, chunksize=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> iter_csv:</span><br><span class="line">            line = line[<span class="string">'text'</span>].item()</span><br><span class="line">            <span class="keyword">yield</span> line</span><br></pre></td></tr></table></figure><p><code>Dataset</code>이 batch data를 생성할 때<code>map_dataset[index]</code>를 사용한다면,<code>IterableDataset</code>은 <code>next(iterable_dataset)</code> 을사용한다. 이 때문에 <code>DataLoader</code>를 통해<code>IterableDataset</code>을 불러와서 사용하게 되면<code>sampler</code> 옵션의 사용이 어렵다. 그래서 random suffling 을하고 싶다면 미리 데이터셋을 shuffling 한 이후에 불러오는 것이 좋다.</p><h2 id="going-parallel">Going Parallel</h2><p><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset" rel="external nofollow noopener noreferrer" target="_blank">PyTorch공식문서</a>에 따르면 <code>IterableDataset</code>을<code>num_workers &gt; 0</code>의 조건에서 사용할 때 특별히 다음을유념할 것을 제안하고 있다.</p><blockquote><p>When <code>num_workers &gt; 0</code>, each worker process will have adifferent copy of the dataset object, so it is often desired toconfigure each copy independently to avoid having duplicate datareturned from the workers. <code>get_worker_info()</code>, when calledin a worker process, returns information about the worker. It can beused in either the dataset’s <code>__iter__()</code> method or the<code>DataLoader</code> ‘s <code>worker_init_fn</code> option to modifyeach copy’s behavior.</p></blockquote><p>위의 문장을 이해하려면 <code>num_workers</code> 에 대한 이해와,<code>num_workers &gt; 0</code> 일 때 <code>IterDataset</code> 에서 어떤현상이 일어나는지 알아야한다.</p><p><img src="/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme" alt="num_workers == 2 인 경우 발생하는 모습이다. 위의 두 라인은 subprocess이며, 맨 아래 라인은 main process이다. 파란색 박스는 single datapoint를 로딩하는 것을 의미하며 붉은색 박스는 로딩된 데이터를 모델에 전달하는 프로세스를 의미한다. (image credit: https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd)" width="90%"></p><p><code>num_workers</code>는 데이터셋을 불러올 때 사용할 subprocess의개수이다. <code>num_workers == 0</code> 은 main process에서 데이터를불러오고 모델에 pass하는 작업을 모두 수행한다는 뜻이며,<code>num_workers == 2</code>는 subprocess 2개에서 데이터를 불러오고main process에서는 subprocess에서 불러온 데이터를 model에 pass하는역할만 담당한다. 따라서 <code>num_workers &gt; 0</code> 일 때 dataloading에서의 병목이 줄어들며 gpu utilization 을 100% 가까이 끌어올릴 수있다.</p><p>그럼, <code>num_workers &gt; 0</code> 일 때 어떤 현상이 발생하는지살펴보자.</p><h3 id="map-style-dataset-1">Map-Style Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        worker = torch.utils.data.get_worker_info()</span><br><span class="line">        worker_id = worker.id <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        </span><br><span class="line">        start = time.time()</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        end = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.data[index], worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = range(<span class="number">16</span>)</span><br><span class="line">map_dataset = MyMapDataset(data)</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 0</code> 인 경우</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 2</code> 인 경우</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>의도한대로, subprocess 별로 서로 다른 데이터를 불러오는 것을 알 수있다.</p><h3 id="iterable-dataset-1">Iterable Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterableDataset</span><span class="params">(IterableDataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> self.data:</span><br><span class="line">            worker = torch.utils.data.get_worker_info()</span><br><span class="line">            worker_id = worker.id <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        </span><br><span class="line">            start = time.time()</span><br><span class="line">            time.sleep(<span class="number">0.1</span>)</span><br><span class="line">            end = time.time()</span><br><span class="line">        </span><br><span class="line">            <span class="keyword">yield</span> x, worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = range(<span class="number">16</span>)</span><br><span class="line">iterable_dataset = MyIterableDataset(data)</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 0</code></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 2</code></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>⚠️ worker 0과 worker 1에서 같은 데이터를 로딩하고 있다. 이 점이공식문서에서 지적하고 있는 내용이다. 각 워커별로 같은<code>__iter__()</code>를 사용하기 때문에 같은 데이터를 로딩하게 된다.<strong>이를 방지하기 위해서는 <code>worker_init_fn</code> 에서 직접워커 별 데이터를 재분배 시켜줘야 한다.</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker_init_fn</span><span class="params">(_)</span>:</span></span><br><span class="line">    worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">    </span><br><span class="line">    dataset = worker_info.dataset</span><br><span class="line">    worker_id = worker_info.id</span><br><span class="line">    split_size = len(dataset.data) // worker_info.num_workers</span><br><span class="line">    </span><br><span class="line">    dataset.data = dataset.data[worker_id * split_size: (worker_id + <span class="number">1</span>) * split_size]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>, worker_init_fn=worker_init_fn)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    print(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><code>worker_init_fn</code> 을 통해 분배시켜준 결과 worker 0과 worker1 에서 다른 데이터를 순차적으로 불러오는 것을 알 수 있다 🙂</p><h2 id="conclusions">Conclusions</h2><ul><li><code>IterableDataset</code> 은 데이터가 메모리에 올라가지 않을만큼클 때 사용하면 좋다.</li><li><code>Dataset</code>과 다르게 <code>__iter__()</code>를 선언해서데이터를 부른다.<ul><li>하지만 이 특징 때문에 <code>Sampler</code> 와 함께 사용할 수없다.</li><li>또한 <code>num_workers &gt; 0</code> 인 세팅에서는 각 워커에서 다른데이터를 불러올 수 있도록 <code>worker_init_fn</code>을 선언해야한다.</li></ul></li></ul><h2 id="references">References</h2><ul><li><a href="https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd" rel="external nofollow noopener noreferrer" target="_blank">Howto Build a Streaming DataLoader with PyTorch | by David MacLeod |Speechmatics | Medium</a></li><li><a href="https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html">https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html</a></li><li><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset" rel="external nofollow noopener noreferrer" target="_blank">torch.utils.data— PyTorch 1.7.1 documentation</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch 1.2 이상부터 &lt;code&gt;torch.utils.data&lt;/code&gt; 에서는 크게
map-style dataset (&lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;) 과 iterable
dataset (&lt;code&gt;torch.utils.data.IterableDataset&lt;/code&gt;) 의 두 종류의
데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는
&lt;code&gt;IterableDataset&lt;/code&gt; 을 사용하는 것이 좋은데,
&lt;code&gt;Dataset&lt;/code&gt; 과는 딜리 아직 개발되어야 할 기능이 더 필요한
클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="PyTorch" scheme="https://inmoonlight.github.io/categories/ML/PyTorch/"/>
    
    
      <category term="pytorch" scheme="https://inmoonlight.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pandas Dataframe의 다양한 iteration 방법 비교</title>
    <link href="https://inmoonlight.github.io/2021/02/04/Pandas-Dataframe-iterations/"/>
    <id>https://inmoonlight.github.io/2021/02/04/Pandas-Dataframe-iterations/</id>
    <published>2021-02-04T05:22:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><code>pandas</code>는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어있지 않은 편이다. 이번 글에서는 <code>pandas</code>의 여러 기능 중에서<code>iteration</code>하는 여러 방법을 속도와 사용성 측면에서 비교해본내용을 아주 간단하게 정리해 보았다.</p><a id="more"></a><h2 id="summary">Summary</h2><table><thead><tr class="header"><th>rank</th><th>method</th><th>time</th><th><code>iterrows</code> 대비 속도</th></tr></thead><tbody><tr class="odd"><td>1</td><td><strong><code>itertuples</code></strong></td><td><strong>7.7ms</strong></td><td><strong>x8.1</strong></td></tr><tr class="even"><td>2</td><td><code>at</code> / <code>iat</code></td><td>15.8ms</td><td>x4</td></tr><tr class="odd"><td>3</td><td><code>loc</code> / <code>iloc</code></td><td>24.6ms</td><td>x2.5</td></tr><tr class="even"><td>4</td><td><code>iterrows</code></td><td>62.7ms</td><td>x1</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>번외</td><td><code>values</code></td><td>7.1ms</td><td>x8.8</td></tr><tr class="odd"><td>번외</td><td><code>apply</code> + <code>to_dict</code></td><td>9.91 ms</td><td>x6.3</td></tr></tbody></table><h2 id="introduction">Introduction</h2><p>실험에 사용한 데이터는 아래와 같이 <code>id</code>,<code>text</code>, <code>title</code> 정보로 이루어진 위키피디아를처리한 table 형식의 데이터이다. <code>text</code>는 위키피디아 문서를일정 길이 단위로 잘라서 가공한 문장들이고, <code>title</code>은 해당문장이 속한 위키피디아 문서의 제목을 의미한다. <code>id</code>는 각문장들의 고유 번호이다.</p><p><img src="/assets/images/pandas-data-example.png?style=centerme" width="70%"><br></p><p>데이터의 row 별로 iteration을 하면서 처리할 내용은 1) 아래의<code>cut_text</code>를 통해 <code>text</code>의 길이를 줄이고, 2) table의 내용을 <code>list_of_dict</code> 형식으로 변환하는 것이다.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_text</span><span class="params">(text, max_len: int = <span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(text.split()[:max_len])</span><br></pre></td></tr></table></figure></p><p>실험할 함수는 크게 <code>iterrows</code>,<code>loc</code>/<code>iloc</code>, <code>at</code>/<code>iat</code>,<code>itertuples</code>, 그리고 속도 면에서는 장점이 있으나 약간의단점이 있는 <code>values</code>, 그리고 이번 task 에 overfitting 된<code>apply</code> + <code>to_dict</code> 가 있다. 하나하나 살펴보도록하자!</p><h2 id="iterrows">iterrows</h2><p>많이 사용되는 함수이지만 가장 성능이 좋지 않다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> data.iterrows():</span><br><span class="line">    short_text = cut_text(row[<span class="string">'text'</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: row[<span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: row[<span class="string">'title'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure></p><p><strong>62.7 ms</strong> ± 729 µs per loop (mean ± std. dev. of 7runs, 10 loops each)</p><h2 id="loc-iloc">loc / iloc</h2><p><code>iterrows</code> 다음으로 많이 사용되는 방식이다.<code>iterrows</code>에 비해 2.5배 정도 빠른 속도를 보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.loc[idx, <span class="string">'text'</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: data.loc[idx, <span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: data.loc[idx, <span class="string">'title'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>24.6 ms</strong> ± 235 µs per loop (mean ± std. dev. of 7runs, 10 loops each)</p><p>:warning: 다만, <code>loc</code>을 잘못 쓰게 되면<code>iterrows</code>를 썼을 때보다도 더 오랜 시간이 소요된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.loc[idx][<span class="string">'text'</span>])  <span class="comment"># diff</span></span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: data.loc[idx][<span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: data.loc[idx][<span class="string">'title'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>261 ms</strong> ± 1.12 ms per loop (mean ± std. dev. of 7runs, 1 loop each)</p><p>:warning: 미리 row를 받으면 조금 더 빨라지지만, 그럼에도<code>iterrows</code>대비 느리다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    row = data.loc[idx]</span><br><span class="line">    short_text = cut_text(row[<span class="string">'text'</span>])  <span class="comment"># diff</span></span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: row[<span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: row[<span class="string">'title'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure> <strong>99.4 ms</strong> ±904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</p><h2 id="at-iat">at / iat</h2><p><code>loc</code> / <code>iloc</code> 과 유사하지만, 특정 column과row에 해당하는 값을 받고 싶을 때 사용한다. <code>at</code> 함수에 대한상세한 설명은 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html" rel="external nofollow noopener noreferrer" target="_blank">pandas공식 문서</a>에서 확인할 수 있다. <code>iterrows</code>에 비해 4배 정도빠른 속도를 보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.at[idx, <span class="string">'text'</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: data.at[idx, <span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: data.at[idx, <span class="string">'title'</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>15.8 ms</strong> ± 49.6 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="itertuples">itertuples</h2><p><code>iterrows</code>와 유사하지만, Series가 return되는<code>iterrows</code>와는 다르게 NamedTuple이 return 된다. column에대응되는 값에 접근하기도 쉽고, 속도도 8배 이상 빠르다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data.itertuples():</span><br><span class="line">    short_text = cut_text(row.text)</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: row.id,</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: row.title</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>7.7 ms</strong> ± 21.9 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="values">values</h2><p>여기서부터는 번외 느낌인데, values는 속도가 가장 빠르다는 장점이있지만 column에 대응되는 값을 불러올 때 불편한 점이 있다. 이 점을감안해서 써도 무관하다면 가장 좋은 선택이 될 것 같다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> data.values:</span><br><span class="line">    short_text = cut_text(value[<span class="number">1</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">'id'</span>: value[<span class="number">0</span>],</span><br><span class="line">        <span class="string">'text'</span>: short_text,</span><br><span class="line">        <span class="string">'title'</span>: value[<span class="number">2</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure></p><p><strong>7.1 ms</strong> ± 43.8 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="apply-to_dict">apply + to_dict</h2><p><code>for</code> 문 안에서 처리할 내용이 복잡하지 않은 이번태스크같은 경우에 쓰기 적합한 방식이다. 새로운 dataframe 혹은 새로운column을 생성해야 해서 메모리 측면에서 오는 단점은 있지만, 코드가 짧고깔끔하다는 장점이 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = data.copy()</span><br><span class="line">result[<span class="string">'text'</span>] = result[<span class="string">'text'</span>].apply(<span class="keyword">lambda</span> x: cut_text(x))</span><br><span class="line">result = result.to_dict(orient=<span class="string">'records'</span>)</span><br></pre></td></tr></table></figure><p><strong>9.91 ms</strong> ± 19.6 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="references">References</h2><ul><li><a href="https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html">https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html</a></li><li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html" rel="external nofollow noopener noreferrer" target="_blank">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html</a></li><li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html" rel="external nofollow noopener noreferrer" target="_blank">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;pandas&lt;/code&gt;는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에
없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만
오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어
있지 않은 편이다. 이번 글에서는 &lt;code&gt;pandas&lt;/code&gt;의 여러 기능 중에서
&lt;code&gt;iteration&lt;/code&gt;하는 여러 방법을 속도와 사용성 측면에서 비교해본
내용을 아주 간단하게 정리해 보았다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
    
      <category term="pandas" scheme="https://inmoonlight.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>2020년을 정리하고, 2021년을 맞이하는 글</title>
    <link href="https://inmoonlight.github.io/2021/01/10/Adieu-2020-and-happy-new-year/"/>
    <id>https://inmoonlight.github.io/2021/01/10/Adieu-2020-and-happy-new-year/</id>
    <published>2021-01-10T12:13:00.000Z</published>
    <updated>2023-11-02T02:50:49.143Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>2020년의 연말은 그 어느 때보다도 연말같지 않았다. 일 년을 주기로움직였던 학생 때나 회사의 근로자일 때와는 처한 상황이 다르기도 했고,코로나로 인한 고요함도 한 몫했던 것 같다. 새롭게 회사를 만들고 멤버들과align되어 달려나갈 준비를 마친 시점이 11월 정도였고, 한창 KLUE프로젝트가 달리고 있던 시기가 12월이었으니 12월 말이 연말보다는 월말의느낌이었다. 게다가 코로나로 사회적 거리두기 단계가 격상되면서 연말파티를하며 사람들과 강제로라도 일년을 회고하고 마무리하는 것조차 어려웠다.따로 시간을 내서 마무리하기에는 체력적으로도, 정신적으로도 휴식이 더필요했기 때문에 "나의 연말은 이 프로젝트가 마무리되는 시점으로유예한다!"고 자기 합리화를 하며 연말과 연초를 보냈다. <a id="more"></a></p><p>그런데 떡하니 회고의 다짐의 주제를 담을 법한 이 글은 대체 무엇이냐.작년 상반기의 프로젝트가 완료되고 한 템포 쉬어가던 4월을 제외하고하반기를 자의반 타의반으로 쉴틈없이 달려왔던지라 번아웃으로 인한슬럼프가 오기 시작했다. 나의 번아웃 극복 노하우 중의 하나로 <em>당장해야할 것 같은 일을 머리에서 지우고, 그 일을 해야하는 이유를 정리하는것</em>이 있다. 왜 나는 이 시간에 이 일을 하고 있는지 스스로를이해시키고 정말 해야하는 일과 하면 좋을 일을 구분하면 현재의 내가 짊어진무게를 덜어낼 수 있고, 꼭 짊어져야 할 무게를 감당할 의지를 다잡을 수있어서 자연스럽게 슬럼프가 극복된다. 지금 정리하는 글은, 그래서 새해부터찾아온 번아웃 극복의 일환이다.</p><p>최근에 사람들을 만날 때마다 가장 자주 들었던 질문은, "그래서 어쩌다가업스테이지에 합류하게 된거야?" 이다. 물어보는 사람에 따라 여러 버전으로대답을 했던 것 같다. 사실이 아닌 답변은 없었지만, 진짜 나의 생각을 꾹꾹담아 전달해본 적도 없던 것 같다.</p><p>나는 사람들의 삶에 비록 미약할지라도, 긍정적인 변화를 불어넣고 싶다.그리고 그 변화는 내가 살아오면서 느꼈던 불만족스러움의 영역을만족스러움의 영역으로 만드는 것을 의미한다. <strong>나는 사람들 개개인의독창성이 존중받는 사회를 원한다.</strong> 이 때문에 똑똑함을 판단하는기준을 시험을 잘 보는 것 정도로 규정하는 우리나라의 교육제도와 개인에대한 제대로 된 이해없이 일부의 특성만으로 개인을 규정하고 판단하는사회적 편견과 혐오를 개선시키고 싶었다.</p><p>우리나라의 교육제도를 개선시키기 위해서는 무엇을 해야할까? 내가교육에 있어 많은 사람들에게 전달하고 싶은 메시지는, "똑똑하고 지혜로운사람은 자리에 앉아서 책에 적힌 개념을 외우거나 이해해서 시험문제를실수없이 잘 풀고 맞추는 사람"이 아니라는 것이다. 그렇기 때문에 하루의대부분을 학원이나 학교에서 보내고 정작 인생에서 중요한 "나라는 사람을다각도에서 이해하기 위해 세상을 경험하는 시간"을 가지지 못하는 것이안타까웠다. 왜 다들 학원과 학교에서 오랜 시간을 보낼까? 그 것은 입시에서뒤쳐지면 망할 것이라는 어른들의 과도한 두려움과 양극화로 인한 불공정한사회라는 사실 혹은 인식 때문이다. 개인의 노력으로는 더 이상 좋은 조건의환경을 극복할 수 없는 사회라는 생각이 강해질수록, 공정해보이는 수능을통한 학벌 사다리를 타기 위해 사교육시장은 더욱 치열하게 불타오를 것이다.다행인지 불행인지, 나는 그렇게 좋은 수저를 물고 태어난 편은 아니다.그렇기 때문에 이 사회가 얼만큼 불공정한지, 부모님들이 느끼는 두려움은정당한 것인지 직접 경험하고 나의 인생이 저물어갈 때쯤에야 내가 한국교육에서 문제라고 느끼는 가설에 대한 답을 찾을 수 있을 것이라고생각했다. 답을 찾는다면, 주저없이 학교를 세워서 나의 경험 혹은 유사한경험을 했던 분들의 이야기를 전달하고 싶다.</p><p>사회적 편견과 혐오는 왜 문제이고 어떻게 해결할 수 있을까? 편견과혐오의 근거가 되는 속성이 "나"를 구성하는 특징이고 이는 나의 노력으로어찌할 수 없는 영역이거나 존중받아야 마땅할 선택이기 때문이다. 나의성별은 여자다. 이 속성은 나의 의지로 그렇게 된 것이 아니다 (놀랍게도500여년 전에는 어찌할 수 있는 영역이라고 생각했었다). 성적지향성, 장애도마찬가지라고 생각한다. 이민 혹은 정치적 성향은 나의 선택과 생각의영역이지 남에게 비난받아야할 속성은 아니다. 어떻게 한 사람을 겉으로드러날 뿐인 얕은 특성만으로 많은 부분을 규정짓고 호와 불호를 가볍게판단해버릴 수 있는걸까.</p><p>교육과는 다르게 사회적 편견과 혐오는 내가 가진 능력으로 해결하기 위해더 빠른 시점에 도전해볼 가치가 있는 문제라고 여겨졌다. 표현을 하는 행위자체는 어찌할 수 없더라도 (일정부분 표현의 자유와도 맞물려 있는영역이기도 하고) 적어도 그 피해가 피해자에게까지는 닿지 않았으면 하는마음이 있었다. 그래서 조그맣게나마 내가 할 수 있는 일부터 시작했고, 그일이 계기가 되어 다양한 사람들과 기회를 마주하게 되었다. 처음에는 아무것도 없었기 때문에 데이터셋을 만들고 연구로서의 가치를 만드는 것으로시작했지만 더 큰 변화를 만들기 위해서는 더 많은 사람들과 함께 해야한다고생각했다. 사람을 모으는 것이 내가 아니더라도 상관없었고, 그저 나와방향이 같은 사람들과 무에서 유를 창조하고 싶었다. 그 마음이 원익님,준범님, 성준님을 거쳐 루시, 활석님, 그리고 성킴까지 만나게 해주었고업스테이지의 합류를 결정하게 해주었다. 결과적으로 Making AI Beneficial이라는 넓은 비전 아래에서 하고 싶은 방향, 해야한다고 생각하는 방향의일을 하고 있다고 생각한다. 8월 어느 날, 업스테이지(그 때는너울이었던...)의 합류를 결정짓고 내가 이 곳에서 배우고 싶고, 이 곳을가야만 한다고 생각했던 이유를 적어둔 일기가 있었는데 대부분이 실현되고있다.</p><p>2021년에는 이전과는 조금 다른 방향으로의 성장을 꿈꾸고 싶다. 우선함께 일하고 싶은 사람이 되고 싶다. soft skill 측면에서 내가 생각하는같이 일하고 싶은 사람이란 겸손하면서 도전하기를 멈추지 않고, 사람을이해하는 사람이다. hard skill 측면에서는 자신만의 엣지를 가진 사람이다.그 영역이 기술일 수도 있겠지만 매니징이나 멘토링도 포함된다고 생각한다.필요하다고 생각하는 내용을, 설사 그 내용이 아플지라도 부드럽고 주저없이전달하는 사람이 되고 싶다. 보다 스스로에게 떳떳한 사람이 되고도 싶다.하지만 가장 되고 싶은 모습은, 많은 것들에 익숙해져서 여유가 있는사람이다. 나의 인생에 slack을 두고 싶고, 그 시간에 나를 포함해서 나의도움을 필요로 하는 사람을 살피고 돌보고 싶다.</p><p>그 밖에도 2021년에는 보다 규칙적인 생활을 하길 바라고 마음편히운동하고 여행다니며 친구들과 수다떨 수 있는 날이 오길 소망한다.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2020년의 연말은 그 어느 때보다도 연말같지 않았다. 일 년을 주기로
움직였던 학생 때나 회사의 근로자일 때와는 처한 상황이 다르기도 했고,
코로나로 인한 고요함도 한 몫했던 것 같다. 새롭게 회사를 만들고 멤버들과
align되어 달려나갈 준비를 마친 시점이 11월 정도였고, 한창 KLUE
프로젝트가 달리고 있던 시기가 12월이었으니 12월 말이 연말보다는 월말의
느낌이었다. 게다가 코로나로 사회적 거리두기 단계가 격상되면서 연말파티를
하며 사람들과 강제로라도 일년을 회고하고 마무리하는 것조차 어려웠다.
따로 시간을 내서 마무리하기에는 체력적으로도, 정신적으로도 휴식이 더
필요했기 때문에 &quot;나의 연말은 이 프로젝트가 마무리되는 시점으로
유예한다!&quot;고 자기 합리화를 하며 연말과 연초를 보냈다.
    
    </summary>
    
    
      <category term="Essay" scheme="https://inmoonlight.github.io/categories/Essay/"/>
    
    
      <category term="essay" scheme="https://inmoonlight.github.io/tags/essay/"/>
    
  </entry>
  
  <entry>
    <title>With Little Power Comes Great Responsibility</title>
    <link href="https://inmoonlight.github.io/2020/12/13/With-little-power-comes-great-responsibiltiy/"/>
    <id>https://inmoonlight.github.io/2020/12/13/With-little-power-comes-great-responsibiltiy/</id>
    <published>2020-12-13T09:33:00.000Z</published>
    <updated>2023-11-02T02:50:49.148Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>요즘 등장하는 NLP model 페이퍼들은 주로 GLUE 벤치마크에 성능을 report하면서 아주 미세한 성능 개선을 근거로 "우리 방법론은 효과적이었다!"를주장하고 있다. 과연 이 결과가 실제로 그 모델이 더 나은 모델임을 주장할수 있을만큼 근거가 탄탄할까?</p><p>이번에 소개하는 논문에서는 NLP research에서 모델의 성능 개선을주장하는 실험 결과에 대해 그 결과가 "정말 유의미한 모델의 성능 개선을보장할 수 있는가?"에 대해 분석한다. 더불어, 분석 결과를 통해 발견된문제점을 개선할 수 있는 간단한 overview 까지 제안하고 있다.<a id="more"></a></p><h2 id="tmi">TMI</h2><p>참고로 이 논문의 제목은 Spiderman의 유명한 대사에서 유래했다.제목에서부터 느껴지는 현 시대의 NLP model evaluation에 대한 강한부정적인 뉘양스는 논문의 교신저자가 Dan Jurafsky 이기에 가능하지않았을까 싶다.</p><p><img src="/assets/images/with-great-power-spiderman.png?style=centerme" width="55%" alt="source: Spiderman(2002)"></p><h2 id="introduction">Introduction</h2><p>본격적인 분석 방법론 소개에 앞서 앞으로 등장하게 될 용어에 대해간단하게 짚고 넘어가려고 한다.</p><blockquote><p><strong>Power</strong></p><p>Power 란, 샘플 데이터에서 관측한 결과가 true distribution 데이터에대해서도 적용될 수 있는지에 대한 확률로, "통계적 유의미함"과는 관련은있으나, 다른 metric이다. 뒤에서 언급하겠지만, Power는 r 번의 반복수행동안 통계적으로 유의미한 결과가 몇번 등장했는가를 바탕으로 측정된다.</p></blockquote><blockquote><p><strong>Type-S error</strong></p><p>sign 에 대한 에러. 쉬운 예를 들면, 실제로 모델 A가 모델 B에 비해성능이 좋은데 실험 결과는 반대로 나오는 경우에 해당한다.</p></blockquote><blockquote><p><strong>Type-M error</strong></p><p>Magnitude에 대한 에러. 예를 들어, 실제 모델의 예측값의 차이가 적은데관측된 결과에 따르면 예측값의 차이가 큰 경우에 해당한다.</p></blockquote><blockquote><p><strong>MDE (minimum detectable effect) size</strong></p><p>유의미한 성능 차이(effect)를 보장할 수 있는 최소 데이터 사이즈. 이논문에서 유의미한 effect는 80% Power를 의미한다. 만약 테스트셋의사이즈가 작으면 MDE는 커진다.</p></blockquote><p>이 용어들을 활용해서 다시 본 논문의 contribution을 정리하면 다음과같다.</p><p>NLP 커뮤니티에서의 실험 결과에 대한 Power를 고려하지 않는 실험 세팅과결과 report 때문에 statistical noise와 유의미한 모델의 성능 향상을구분하기 어렵다. Underpowered result는 실험 결과를 과장하거나, 실제효과를 반대로 해석할 수 있는 여지가 있다. 너무 적은 샘플에 대해서는유의미한 차이를 판단하기 어렵고, 유의미한 차이를 보이는 경우이더라도샘플 수가 적을수록 그 효과를 과장해서 평가할 수 있다.</p><p>따라서 모델을 평가할 때에 주어진 조건들 - 테스트셋의 크기, 평가자인원수 등 - 에 대해 결과가 유의미함을 보장하는지 생각해 볼 필요가 있다.이를 위해 기존의 평가 framework에 대한 Power Analysis 가 필요하고,동시에 유의미한 성능 개선을 보여줄 수 있는 조건에 대해서도 논의될 필요가있다.</p><p>먼저, NLP 에서의 Power Analysis에 대해 알아보자.</p><h2 id="power-analysis-for-nlp">Power Analysis for NLP</h2><p>NLP의 task의 평가 format은 그동안 다른 과학적 방법론에서 적용되는Power Analysis를 수행하기에 적합하지 않다. NLP 평가의 모든 시나리오를커버할 수 없기 때문에 저자들은 최대한 일반화 가능한 simluation 기반의power analysis를 제안한다.</p><p>저자들이 제안하는 Power Analysis 알고리즘은 다음과 같다.</p><p><img src="/assets/images/with-little-power-figure-2.png?style=centerme" width="50%"></p><p>Simulation을 하기 위해서는 특정 parameter들이 요구된다. 저자들은 총6개의 parameter를 제안하고 각각은 다음과 같다.</p><ul><li>\(n\): 평가에 사용되는 데이터의 개수</li><li>\(e*\): 관심있는 평가 수치 (e.g., \(\Delta_{acc}\) 등)</li><li>\(h\): 관련있는 다른 수치 (e.g., variance 등)</li><li>\(T\): 유의미성을 판단할 수 있는 statistical test</li><li>\(\alpha\): 유의미성을 판단할 수 있는 threshold</li><li>\(r\): 반복 수행 횟수</li></ul><p>알고리즘을 간단하게 설명하면, 총 r번의 실험에 대해 n 개의 데이터에대한 평가 결과를 생성하고 이 결과를 바탕으로 significance test를수행해서 p-value 를 얻는다. 최종적으로 유의미한 결과 (p-value가 특정threshold 이하인 경우)의 횟수가 r번 중 몇번인지를 바탕으로 power 를계산한다. 아래는 위 알고리즘을 바탕으로 power analysis를 수행한예시이다.</p><p><img src="/assets/images/with-little-power-figure-1.png?style=centerme" width="50%"></p><p>제일 왼쪽의 이미지는 실제 정답으로, B모델이 A모델에 비해 65% 정도선호되는 것을 나타낸다. 두 모델의 선호도가 같음(50%)을 기준으로 약 15%정도 더 선호된다고 볼 수 있다. 가운데 이미지는 \(r\)=10, \(n\)=100인관측 결과이다. 샘플 수가 많기 때문에 10번의 trial 중, 유의미하게 B모델이A모델보다 선호되는 경우가 8번으로 power는 80%라고 할 수 있다. 반면 샘플수가 적은 마지막 이미지는 유의미한 trial은 3번으로 power는 30%라고판단된다.</p><p>또한 샘플 수가 많은 경우에 유의미한 결과의 평균(B 모델이 선호되는정도)과 null hypothesis(50%)의 차이가 실제 분포의 차이와 유사하다(파란색 라인과 검은색 라인의 차이). 샘플 수가 적은 경우, 그 차이가과장된다 (Type-M error).</p><p>이제 위의 Power Analysis가 여러 NLP 평가 시나리오에서 어떻게수행되는지에 대한 예시를 보여준다. 크게 classfication task와 generationtask, 그리고 human evaluation 시나리오를 보여주는데, 이 글에서는 그 중classification task와 human evaluation에 대해서 중점적으로 다뤄보고자한다.</p><h3 id="classification-task">Classification Task</h3><p>Classification Task를 수행하는 모델들의 Accuracy 차이가 유의미한지를판단한다. 위의 Power Analysis 시나리오를 만들기 위해서 필요한parameter를 소개한다.</p><ul><li><strong>\(n\): 100, 500, ...</strong></li><li><strong>\(T\) (Significance test)</strong></li></ul><p>classification 결과의 유의미성을 확인하는 statistical test로 가장많이 쓰이고 있는 것은 McNemar's test로 모델 간의 불일치하는 정도를바탕으로 두 모델의 차이에 대한 유의성을 검증한다.</p><p>\[\chi^2 = \frac{(p_{10} - p_{01})^2}{p_{10} + p_{01}}\]</p><p><img src="/assets/images/with-little-power-table-1.png?style=centerme" width="50%"></p><ul><li><strong>\(e*\) (관심있는 평가 수치)</strong></li></ul><p>모델의 accuracy 차이 (\(\Delta_{acc}\))와 두 모델이 같은 결과를예측할 확률 (\(P_a\)) 을 제안한다. 예를 들어, 모델 A가 baseline과비교했을 때 0.02 만큼의 accuracy 차이를 보이고, 90%의 예측 결과가일치한다고 하자. 이는 다시 말해 10%의 결과는 다르다는 것이며, 2%라는성능 차이에 따라서 6%는 모델 A가 맞고, 4%는 baseline의 결과가 맞을것이다.</p><p>with-little-power-table-4.png</p><ul><li><strong>\(\alpha\): 0.05</strong></li></ul><p>위의 파라미터에서 \(n\)을 100부터 5000까지 늘리면서 실험한 결과는다음과 같다.</p><p><img src="/assets/images/with-little-power-figure-3.png?style=centerme" width="45%"></p><p>두 모델이 같은 결과를 예측할 확률 (\(P_a\)) 이 낮을수록 두 모델의성능차이(\(\Delta_{acc}\))가 적게 나는 경우는 상식적으로 발생하기어렵다. \(n\)이 5000으로 많은 경우에도 80% Power를 얻는 시점의(\(\Delta_{acc}\))는 \(P_a\)가 높을 때가 더 낮다. \(n\)이 작은 경우에는더더욱 유의미한 성능차이를 주장하기 위해 필요한 성능 폭이 증가한다.</p><h4 id="assessing-power-in-the-literature">Assessing power in theliterature</h4><p>위의 결과를 바탕으로 다음의 질문에 대한 답을 할 수 있다. 주어진테스트셋에 대한 baseline 대비 유의미한 성능 차이는 어느 정도일까?</p><p>GLUE와 SQuAD 2.0 에 대해 결과를 report한 모델들의 baseline 대비 평균accuracy 차이 (\(|\Delta_{acc}|\)) 를 구한다. 이를 바탕으로 선형회귀방식을 통해 구해진 \(P_a\)를 도출하며, 도출된 두 값과 테스트셋의사이즈를 바탕으로 80% power를 만족(MDE; minimum detectable effect)하는\(\Delta_{acc}\)를 구한다.</p><p><img src="/assets/images/with-little-power-table-2.png?style=centerme" width="50%"></p><p>Est. MDE 보다 \(|\Delta_{acc}|\)이 낮은 task들(WNLI, MRPC, SST-2)은성능 개선을 주장하는 대부분의 모델이 실제로 그 task를 더 잘한다고 말할수 없다. 평균 값이므로, 만약 어떤 모델이 Est. MDE 보다 높은 성능 개선을보여주었다고 하더라도 데이터셋의 크기가 작기 때문에 Type-M error를의심하게 된다.</p><p>저자들은 이 분석 결과를 두고 다음과 같이 강하게 주장하고 있다.<em><del>(패기무엇)</del></em></p><blockquote><p>In extreme cases, such as MRPC and SST-2, it is worth consideringwhether it is time to retire these datasets as the basis for modelcomparison.</p></blockquote><h3 id="likert-scale-human-evaluations">Likert-Scale HumanEvaluations</h3><p>대화와 같은 NLG 태스크에서는 적절한 evaluation metric 이 부재하기때문에 주로 Human evaluation 결과를 report한다. Human evaluation 결과도다양하게 measure되는데 여기서는 그 중에서 Likert-scale 로 report하는경우에 대해서 power analysis를 수행하였다.</p><h4 id="meta-analysis">Meta-analysis</h4><p>Power Analysis에 앞서 논문에서 수행한 Human evaluation에 대해 간단한통계량을 정리하였다. 놀랍게도 69%의 논문에서 100개 미만의 테스트셋을사용했고, 18%만이 200개 이상의 테스트셋을 사용했다. 또한 34%의 논문에서테스트 데이터 건당 rating 수를 제공하지 않았고, 28%의 논문에서 전체어노테이터 수를 공개하지 않았다. 57%의 실험에서 아이템 당 3명의어노테이터를 두었다.</p><p>정리하면, typical 한 성능 평가 시나리오는 3명의 작업자가 100개의결과를 평가하는 것이다.</p><h4 id="power-analysis-for-human-likert-ratings">Power analysis forhuman Likert ratings</h4><p>Power analysis를 위해서는 앞서 언급한 simulation을 수행해야 한다.여기에는 작업자의 편차, 작업자 수, 평가한 문장 수 등의 정보가 필요하다.이 parameters를 도출하기 위해 저자들은 기존 논문에서 report한 성능 평가데이터를 기반으로 hierarchical mixed effects model을 사용한다. 그리고도출된 parameter를 바탕으로 power analysis를 수행해서 아래의 그래프를얻는다.</p><p><img src="/assets/images/with-little-power-figure-6.png?style=centerme" width="45%"></p><p>이 그래프를 통해 충분한 power를 가진 성능 평가를 위해 몇명의어노테이터와 몇개의 평가 인스턴스가 필요한가? 에 대한 답을 알 수 있다.분석 결과를 정리하면 다음과 같다.</p><ul><li>대부분의 human evaluation은 underpowered 일 가능성이 높다.<ul><li>가장 흔한 평가 방식은 3명의 어노테이터에게 100개의 데이터를평가시키는 것이다. 보수적으로 작업자들의 편차가 크다고 한다면 유의미한차이를 보이는 점수차이는 [0,1]로 normalize 시켰을 때 0.2 이상이어야한다.</li></ul></li><li>일반적인 시나리오에서 작업자들의 편차가 적더라도, 작은 성능 차이를유의미하다고 인지하기에는 부족하다<ul><li>일반적인 시나리오(3명의 어노테이터, 100개의 데이터)에서 작업자들의편차가 작다고 가정하더라도 0.1 이상의 점수차이여야 유의미하다고 볼 수있다.</li><li>10명의 작업자들이 500개의 데이터에 평가를 한다고 할 때에야 0.05정도의 차이만으로도 유의미하다고 이야기할 수 있다.</li></ul></li><li>대부분의 human evaluation은 제대로 결과를 report하지 않는다. 즉,평가의 디테일이 빠져있다.<ul><li>그러나 (안타깝게도) 대부분의 성능 결과에서 위의 정보를 제공하지 않아애초에 평가의 유의미함을 제대로 판단하지 못한다.</li></ul></li></ul><h2 id="overall-recommendations">Overall Recommendations</h2><ul><li>baseline 결과와 비교하기 전에 power analysis 가 선행되어야 한다.Underpowered 실험은 개선되었다고 주장하면 안된다.</li><li>새로운 데이터셋과 shared task에서 결정하는 데이터셋 사이즈는 MDE를고려해서 결정되어야 한다.</li><li>GLUE task에서 성능 개선의 유의미함을 판단할 수 없는 MRPC, SST-2의task는 평가 대상에서 빠지거나 테스트셋을 확장시켜야 한다.</li><li>Power analysis를 위해 fine-tuned model의 checkpoint가 공개되어야한다.</li><li>Anonymized human evaluation 결과가 공유되어야 하고, humanevalutation을 수행함에 앞서 적절한 sample size를 도출하기 위한 poweranalysis가 필요하다.</li></ul><h2 id="references">References</h2><p><a href="https://arxiv.org/abs/2010.06595" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/abs/2010.06595</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;요즘 등장하는 NLP model 페이퍼들은 주로 GLUE 벤치마크에 성능을 report
하면서 아주 미세한 성능 개선을 근거로 &quot;우리 방법론은 효과적이었다!&quot;를
주장하고 있다. 과연 이 결과가 실제로 그 모델이 더 나은 모델임을 주장할
수 있을만큼 근거가 탄탄할까?&lt;/p&gt;
&lt;p&gt;이번에 소개하는 논문에서는 NLP research에서 모델의 성능 개선을
주장하는 실험 결과에 대해 그 결과가 &quot;정말 유의미한 모델의 성능 개선을
보장할 수 있는가?&quot;에 대해 분석한다. 더불어, 분석 결과를 통해 발견된
문제점을 개선할 수 있는 간단한 overview 까지 제안하고 있다.
    
    </summary>
    
    
      <category term="Paper" scheme="https://inmoonlight.github.io/categories/Paper/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="paper" scheme="https://inmoonlight.github.io/tags/paper/"/>
    
      <category term="evaluation" scheme="https://inmoonlight.github.io/tags/evaluation/"/>
    
  </entry>
  
  <entry>
    <title>Don&#39;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
    <link href="https://inmoonlight.github.io/2020/11/30/Don-t-stop-pretraining/"/>
    <id>https://inmoonlight.github.io/2020/11/30/Don-t-stop-pretraining/</id>
    <published>2020-11-30T06:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.144Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>다양한 출처의 데이터로 학습한 pretrained model이 NLP task에서 좋은성능을 보여주고 있다. 하지만 아직 주어진 labeled data 의 크기나 targetdomain의 코퍼스와 사전학습 코퍼스의 유사도가 특정 task의 결과에 얼마나영향을 미치는지에 대해 알려진 바가 없다. 또한 RoBERTa와 같은 LM이 정말다양한 task에 generalize될만큼의 다양한 source로 학습되었는지도 확실하지않다. 이 논문에서는 pretrained model을 풀고자 하는 특정 task의 domain에tailor시켜서 추가로 학습시키면 더 좋은 성능을 보일 수 있을까? 라는질문에 대한 답을 하고 있다. <a id="more"></a></p><p>결과는 "그렇다"이다. 보다 구체적으로는 다음의 3가지 findings가있었다. 첫번째, pretrained model을 추가로 in-domain 데이터에 학습시키는것이 성능향상에 도움을 준다 (Domain-Adaptive PreTraining; DAPT). 두번째,DAPT 이후로 풀고자 하는 task의 unlabeled data에 대해 추가로 학습하는것(Task-Adaptive PreTraining; TAPT)도 성능향상에 도움을 준다. 세번째,DAPT를 할 corpus가 없을 때 간단한 data selection strategies으로 taskcorpus를 augment 한 후 TAPT를 수행해도 성능 향상을 보인다.</p><h2 id="experimental-settings">Experimental settings</h2><ul><li>Pretrained model: RoBERTa-base<ul><li>160GB heterogeneous data로 학습</li><li>bookcorpus, stories, wikipedia, and realnews</li></ul></li><li>Domains:<ul><li>biomedical papers (BIOMED)</li><li>cs papers (CS)</li><li>newstext from realnews (NEWS)</li><li>Amazon reviews (REVIEWS)</li></ul></li><li>Tasks:<ul><li>2 text classfication tasks per each domain<br><img src="/assets/images/dapt-dataset.png?style=centerme" width="80%"></li></ul></li></ul><h3 id="domain-의-기준">Domain 의 기준?</h3><p>내가 아는 한에서는, domain 이라는 용어에 대한 확실한 정의는 없다.어떤 논문은 domain에 대해 두루뭉술하게 언급하고 넘어가고, 어떤 논문은뭐라도 정의하고 넘어가는 경우가 있는데 이번 논문에서는 domain에 대한논의가 중요하다보니 선정한 4개의 도메인과 pretraining domain이 서로상이함을 밝힐 필요가 있었다.</p><p><img src="/assets/images/dapt-vocab-overlap.png?style=centerme" width="40%"><br></p><p>위의 이미지는 Vocab overlap 을 통한 domain의 similarity 를 구한heatmap이다. 빈번한 10K 의 vocab 중 얼마나 겹치는지를 나타내고 있으며결과는 직관과 일치하는 것으로 보인다. PT corpus는 news, reviews와 가장유사하며 cs와 가장 거리가 멀다. 또 reviews와 cs의 거리가 가장 멀고,news와 reviews는 cs에 비해 상대적으로 더 유사하다.</p><h2 id="dapt-results">DAPT Results</h2><h3 id="domain-별-lm-loss-변화">Domain 별 LM loss 변화</h3><p><img src="/assets/images/dapt-lm-loss.png?style=centerme" width="80%"><br></p><p>각 도메인에 RoBERTa를 12.5K steps 씩 학습시킨 후 LM loss의 전후를비교하였다. domain similarity 가 가장 높았던 news를 제외한 나머지도메인에서 marginal한 성능향상이 있었다.</p><h3 id="plm-vs.-dapt-vs.-dapt-모델의-classification-결과">PLM vs. DAPTvs. ~DAPT 모델의 classification 결과</h3><p><img src="/assets/images/dapt-table-3.png?style=centerme" width="40%"><br></p><p>LM loss 의 변화가 시사했던 것처럼 BM과 CS 도메인에서의 효과가 가장컸다. 하지만 이 변화가 단순히 더 많은 데이터에 노출되었기 때문인지아닌지를 판단하기 위해 out-of-domain corpus로 DAPT를 시킨 후의 결과와비교했다. news의 경우 CS로 DAPT한 LM을, reviews의 경우 BIOMED LM을, cs의경우 news LM을, biomed의 경우 reviews LM을 사용했다. DAPT가 ~DAPT모델보다 모든 경우에서 더 좋은 성능을 낸다. 심지어 RoBERTa와 비교해보면~DAPT 의 결과는 더 나빠지는 경향을 보인다. 이는 단순히 더 많은 데이터에노출되는 것이 항상 모든 도메인의 결과에서 효과적이지 않다는 사실을시사한다.</p><h3 id="fuzzy-한-domain-boundary">Fuzzy 한 domain boundary</h3><p>Vocab overlap 결과에서도 알 수 있지만 domain 이라는 것이 무자르듯나뉘는 것이 아니다. 지금까지의 실험은 news와 reviews 도메인을구분했지만, reviews corpus가 news corpus에 아예 도움을 주지 않는다고 볼수 없다.</p><h2 id="tapt">TAPT</h2><p>Domain 보다 더 협소한 범위의 Task에 대해서도 DAPT와 같은 효과가있는지를 검증하였다. DAPT와 비교해서 더 적은 corpus로 학습한다는 단점이있지만 더 task relevant한 corpus라는 장점이 있다. 만약 최종 성능이비슷하다면 TAPT가 더 값싼 학습방식이라고 볼 수 있다. 여기서는 labeledtraining data를 사용해서 second phase PLM을 진행했다.</p><h3 id="plm-vs.-dapt-vs.-tapt-vs.-dapttapt-모델의-classification-결과">PLMvs. DAPT vs. TAPT vs. DAPT+TAPT 모델의 classification 결과</h3><p><img src="/assets/images/dapt-table-5.png?style=centerme" width="80%"><br></p><p>TAPT의 경우, corpus 사이즈를 고려해서, second phase of pretraining은100 epoch만 진행하였다.</p><ul><li><strong>PLM vs TAPT</strong><ul><li>결과를 보면, TAPT를 진행한 LM으로 classficiation task를 수행한경우가 RoBERTa-base 보다 항상 결과가 더 좋다.</li></ul></li><li><strong>DAPT vs TAPT</strong><ul><li>하지만 DAPT와 비교해보면 언제나 더 좋은 결과를 보여주는 것은아니었다.</li></ul></li><li><strong>PLM vs DAPT + TAPT</strong><ul><li>DAPT 이후 TAPT 를 적용하는 것이 언제나 최고의 성능을 보여준다.</li><li><strong>PLM에 많이 활용된 데이터가 AGNews와 IMDB와 비슷했다면 그둘의 성능 폭이 작은 것이 이해가 되지만, 아니라면 HyperPartisan이랑Helpfulness의 성능향상이 BIOMED와 비슷하다는 점에서 꼭 PLM의 학습코퍼스의 도메인과 성능향상이 관련있다고 보긴 어렵다.</strong></li><li>그 task 를 잘하기 위해서는 마지막에 weight 를 옮겨주는 것이 필요하지않을까?</li></ul></li></ul><h3 id="cross-task-transfer">Cross-Task Transfer</h3><p><img src="/assets/images/dapt-table-6.png?style=centerme" width="80%"><br></p><p>같은 도메인 내의 2 task 간 transfer 효과가 있는지에 대해서살펴보았다. BIOMED 내의 RCT, ChemProt task를 예로 들면, Transfer-TAPT는RCT의 unlabeled data로 pretraining한 이후, ChemProt의 결과를 본 것이다.모든 경우 Transfer-TAPT의 결과가 TAPT보다 낮았다.</p><h3 id="augmenting-training-data-for-tapt">Augmenting Training data forTAPT</h3><p>task: RCT, HyperPartisan, IMDB</p><p><strong>case 1) target task의 labeled data와 같은 distribution의unlabeled target task corpus (by human)</strong></p><p><img src="/assets/images/dapt-table-7.png?style=centerme" width="40%"><br></p><p>Curated TAPT의 경우 TAPT보다 더 많은 task corpus (unlabeled)로 PLM을진행하였고, 좋은 성능을 보였다. DAPT와 함께 진행하게 되면 그 효과는 더욱확실하다.</p><p><strong>case 2) Automated Data selection for TAPT</strong></p><p>task setup 당시에 large unlabeled corpus 조차 풀리지 않는 경우가있다. 이 때, 자동으로 관련있는 데이터를 찾아서 이를 기반으로 학습하게되면 어떨지를 실험하였다.</p><p>하지만 large in-domain corpus 여야 하며, 이 중에서 task와의 접점이있는 task-relevant data를 찾는 것이다. 그리고 embedding space 내에서접점을 찾는 것이므로 경량화모델이 필요하다. 여기서는 vampire model을사용하였다.</p><p><img src="/assets/images/dapt-table-8.png?style=centerme" width="40%"><br></p><ul><li><strong>rand-TAPT vs kNN-TAPT</strong><ul><li>kNN &gt; rand-TAPT</li></ul></li><li><strong>TAPT vs automated data selection</strong><ul><li>방법이 무엇이든 추가 데이터를 활용하는 것이 나쁘지는 않음</li><li>아직 RoBERTa에게는 더 학습할 수 있는 여지가 남아있다고도 해석 가능(데이터는 많을수록 좋다)</li></ul></li><li><strong>DAPT vs 500NN-TAPT</strong><ul><li>약 500개의 데이터만 사용해도 DAPT 효과를 어느 정도 낼 수 있음</li></ul></li></ul><h2 id="conclusions">Conclusions</h2><p><img src="/assets/images/dapt-table-9.png?style=centerme" width="40%"><br></p><ul><li>Task 문제를 더 잘 풀기 위해서 관련된 distribution 의 데이터로 추가학습을 하는 것이 효과적이다</li><li>Task-specific data 일 필요는 없다. Domain이 비슷하면 효과를 볼 수있다.</li><li>다만 아쉬운 건, PLM 자체를 학습시킬 때 DAPT+TAPT에 사용한 데이터를활용하면 점수가 어떻게 변하는지 알 수 없었다는 점이나, 이번 논문의scope에 들어갈 필요는 없었다고도 생각한다.</li></ul><h2 id="references">References</h2><p>https://www.aclweb.org/anthology/2020.acl-main.740.pdf</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;다양한 출처의 데이터로 학습한 pretrained model이 NLP task에서 좋은
성능을 보여주고 있다. 하지만 아직 주어진 labeled data 의 크기나 target
domain의 코퍼스와 사전학습 코퍼스의 유사도가 특정 task의 결과에 얼마나
영향을 미치는지에 대해 알려진 바가 없다. 또한 RoBERTa와 같은 LM이 정말
다양한 task에 generalize될만큼의 다양한 source로 학습되었는지도 확실하지
않다. 이 논문에서는 pretrained model을 풀고자 하는 특정 task의 domain에
tailor시켜서 추가로 학습시키면 더 좋은 성능을 보일 수 있을까? 라는
질문에 대한 답을 하고 있다.
    
    </summary>
    
    
      <category term="Paper" scheme="https://inmoonlight.github.io/categories/Paper/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="paper" scheme="https://inmoonlight.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Social Bias in NLP Models</title>
    <link href="https://inmoonlight.github.io/2020/11/14/Social-bias-in-NLP-models/"/>
    <id>https://inmoonlight.github.io/2020/11/14/Social-bias-in-NLP-models/</id>
    <published>2020-11-14T02:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.148Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>한 스타트업에서 개발한 인공지능 채용솔루션(a.k.a. AI 면접관)을 벌써여러 기업에서 사용하고 있다는 뉴스기사를 접하게 되었다. 해당 기업은"성별이나 학력 등에 따른 차별 방지와 정확한 역량 추정"을 위해 5만2천여명의 데이터를 확보하여 학습했다고 말한다. 5만 2천여명의 데이터와차별 방지가 어떤 관련이 있는지는 모르겠지만, <em>많은</em> 양의 데이터를사용한다는 걸 내세우고 싶었다면 대량의 데이터가 편향성을 줄이는 것과는무관하다고 말하고 싶다. 5만 2천개보다 더 많은 데이터로 학습한 LanguageModel 도 편향성 문제가 있으며 이 이슈는 아직도 연구자들에 의해 활발히연구되고 있다. <a id="more"></a></p><h2 id="introduction">Introduction</h2><p>2020년 6월 말에 다음과 같은 트윗이 올라왔다. 오바마를 모자이크한이미지를 넣었는데 백인의 특징을 가진 이미지가 생성이 되었다는것이다.</p><p><img src="/assets/images/twitter_obama.png?style=centerme" width="45%"><br> 이 트윗은 흘러흘러 Yann LeCun의 귀에 들어간다.</p><p><img src="/assets/images/twitter_yann.png?style=centerme" width="65%" alt="그건 모델의 문제가 아니야! 데이터의 문제라구!"><br></p><p>그리고 이는 다시 조경현 교수님을 통해 Alice Oh 교수님의 AI &amp;Ethics 특강발표자료<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://kyunghyuncho.me/social-impacts-bias-of-ai/">[9]</span></a></sup>에서 아래의 문구와 함께 다시 한번 인용된다.</p><blockquote><p>Too much blame on data curation, too little blame on algorithms</p></blockquote><p><img src="/assets/images/yann_cho.png?style=centerme" width="65%"></p><p>조경현 교수님의 이야기를 좀 더 들어보자. "물론 데이터도 문제가 있지만알고리즘의 잘못이 전혀 없는 것은 아니다" 라고 주장한다.</p><p>아래의 장표에서는 ML 알고리즘의 solution space를 구획별로 나누어서 총4가지로 구분하고 있다.</p><ul><li>Training solutions: 학습 데이터셋에서 좋은 성능을 보이는 solutionspace</li><li>Shortcut solutions: 학습 및 주어진 테스트셋에서 좋은 성능을 보이는solution space</li><li>Intended solutions: 학습 및 주어진 테스트셋과 o.o.d 테스트셋모두에서 좋은 성능을 보이는 solution space. 여기야말로 우리가 원하는진짜 general knowledge가 있는 space</li><li>Learnable solutions: 알고리즘이 학습을 통해 도달할 수 있는 solutionspace</li></ul><p><img src="/assets/images/cho_ethics_solution_space.png?style=centerme" width="75%"></p><p>우리가 원하는 것은 intended solution space와 learnable space가 만나는점에 학습 모델이 수렴하게 만드는 것이다. 하지만 이는 쉽지 않다. databias 가 있기 때문에 model 이 학습과정에서 data 내에 존재하는 bias 를함유할 수 밖에 없는 것은 맞지만, 그럼에도 불구하고 이 모든 것이 databias 의 잘못때문만은 아니라는 것이 조경현 교수님이 이야기하고 싶었던내용이다.</p><p>아래의 장표는 random seed 를 바꿈에 따라 learnable 모델의 학습 결과가위치하는 solution space를 보여준다. seed 만 바꾸어도 solution space가달라지는 것으로 미루어보아, 충분히 모델의 학습 과정을 잘 tuning 하면원하는 solution space 쪽으로 학습할 여지가 있다고 보여진다.</p><p><img src="/assets/images/cho_ethics_seed_solution_space.png?style=centerme" width="75%"><br></p><h2 id="undesirable-solution-의-이유는-무엇일까">Undesirable solution 의이유는 무엇일까?</h2><p>여러 이유 중 하나로, shortcut solution 중 일부는 잘못된 correlation을 학습하기 때문이라는 점이 있다.</p><p>"Correlation does not imply Causality" 라는 유명한 명제가 있다. 눈에드러나는 X와 Y variable 의 관계에 모두 영향을 미치는 confounding factorZ 가 있고 hidden varirable 인 경우 우리는 실제 인과관계를 놓치고상관관계를 인과관계처럼 학습할 위험이 있다.</p><p><img src="/assets/images/casuality_graph.png?style=centerme" width="25%"></p><p>가장 유명한 예로, "초콜렛의 소비(X)가 많은 나라에서 노벨상(Y)을 많이받는다." 가 있다. 실제로는 "고등교육에 투자하는 물질적, 시간적 여유가많을수록(Z) 초콜렛 소비(X)가 많다." 와 "고등교육에 투자하는 물질적,시간적 여유가 많을수록(Z) 노벨상(Y)을 많이 받는다."의 인과관계에서 Zvariable 을 무시한채 해석하면 잘못된 정보를 학습할 수 있다는 것을보여준다.</p><p>모델도 마찬가지다. 학습에 노출되는 데이터 분포에서는 Z가 명시적으로보이지 않을 수 있다. 백인 남성이 상대적으로 경제적인 여유가 더 있어서인터넷에 사진을 업로드하는 경우가 많았고, 이 때문에 웹에서 수집한 데이터중의 대다수가 백인 남성일 수 밖에 없다고 했을 때, 이 사실은 데이터의분포만 파악하는 모델의 입장에서는 학습하기 어려운 정보일 수 있다.</p><h2 id="모든-bias-가-나쁠까">모든 bias 가 나쁠까?</h2><p>꼭 그렇지는 않다. 때론 우리는 모델이 inductive bias 를 학습하길바란다. 예를 들어, "나비가 코끼리를 포식하지 않는다" 는 정보는 모델입장에서는 필요한 bias 이다. 그러나 social bias 와 같은 종류의 정보는학습하지 않기를 기대한다.</p><h2 id="nlp에서는">NLP에서는?</h2><p>앞에서는 이미지를 예로 들었지만, 여러 논문에서도 NLP 모델이 socialbias 를 학습하고 있다는 사실을 직/간접적적으로 보여주고 있다.</p><p>모델이 학습한 word embedding 을 분석해서 bias를 학습했음을 통해보이거나<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) ">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Semantics derived automatically from language corpora contain human-like biases](https://arxiv.org/abs/1608.07187) ">[2]</span></a></sup>, 학습 dataset 자체에 social bias 가 있음을보이거나<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Social Bias in Elicited Natural Language Inferences](https://www.aclweb.org/anthology/W17-1609/) ">[3]</span></a></sup>, bias 를 측정하는 task와 metric을 제안하여 얼만큼bias를 학습했는지 를 수치적으로평가<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Semantics derived automatically from language corpora contain human-like biases](https://arxiv.org/abs/1608.07187) ">[2]</span></a></sup><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods](https://www.aclweb.org/anthology/N18-2003/) ">[4]</span></a></sup><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/abs/2004.09456)">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://arxiv.org/abs/2010.00133) ">[6]</span></a></sup>한다. 새로운 bias 측정 task를 제안할 때 수반되는dataset이 있는데, 여기에도 구축 방식에 따라 두가지 종류로 나눌 수 있다.하나는 templated-baseddataset<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods](https://www.aclweb.org/anthology/N18-2003/) ">[4]</span></a></sup>, 다른 하나는 crowdsourceddataset<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/abs/2004.09456)">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://arxiv.org/abs/2010.00133) ">[6]</span></a></sup> 이다.</p><h3 id="모델의-embedding-space-분석">모델의 embedding space 분석</h3><p>이 방법론은 PLM 이전, word2vec 이 많이 사용되었던 시기에 등장했다.word2vec을 통해 학습된 단어의 관계를 분석했을 때 social bias 가 얼마나반영되었는지를 판단한다. 예를 들어,<code>man - woman ~= computer programmer - homemaker</code> 의 관계가성립한다면 이 모델은 social bias 를 함유하고 있다고 볼 수 있다.</p><p><strong>Man is to computer programmer as woman is to homemaker?debiasing wordembeddings</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) ">[1]</span></a></sup>에서는 word2vec의 geometric bias에 대해분석하였다.</p><p><img src="/assets/images/bias_word2vec_figure.png?style=centerme" width="75%"></p><p>Figure 1의 왼쪽에는 she / he 와 가까이에 있는 직업 단어 목록이 있다.그리고 각 직업 단어들을 crowd worker 에게 female-stereotypic,male-steretotypic, neutral 한지를 물었을 때의 결과와 corrleation을구했고, 그 결과 0.51 정도의 moderate 한 결과가 나왔다.</p><p>Figure 1의 오른쪽에는 she-he 의 관계와 유사한 단어들의 관계 목록을보여주고 있다. 이 중에서 Gender stereotypical 한 analogy와 genderappropriate한 analogy의 수량을 비교하였고, 150 개의 관계 중 29개가gender stereotypical 하다고 판단되었다.</p><h3 id="데이터셋-자체에-함유된-bias">데이터셋 자체에 함유된 Bias</h3><p>학습한 모델을 분석하는 방법이 아닌, 학습하는 대상이 되는 데이터셋자체를 model-agnostic 하게 분석하는 방법도 제안되었다.</p><p><strong>Social Bias in Elicited Natural LanguageInferences</strong><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Social Bias in Elicited Natural Language Inferences](https://www.aclweb.org/anthology/W17-1609/) ">[3]</span></a></sup>에서는 SNLI 데이터셋에 존재하는 bias 를 PMI 와Likelihood ratio test of independence를 통해 통계적으로 분석하였다.</p><p><img src="/assets/images/pmi_equation.png?style=centerme" width="50%" alt="w1과 w2의 PMI. 값이 높을수록 함께 등장하는 확률이 높다고 볼 수 있음"><br><img src="/assets/images/likelihood_confidence_equation.png?style=centerme" width="50%" alt="X와 Y의 independence에 대한 기각 test로, 값이 낮을수록 dependency가 있는 것으로 해석할 수 있음. 자세한 수식은 논문 참고"></p><p>아래의 Table 1은 SNLI 데이터에서 gender, age,race/ethnicity/nationality 에 해당하는 단어들 각각에 대해 PMI가 높은단어들을 나열해 놓은 것이다. woman &amp; man, girls &amp; boys, whiteman &amp; african american 등 차이가 없어야할 개념들에 대해 PMI 상가까운 단어들을 비교해보면 차이가 있다.</p><p><img src="/assets/images/snli_bias_table.png?style=centerme" width="75%"></p><p>Table 2는 inference type 별로 gender words 들에 대해 PMI가 높은순서대로 정렬한 결과다. 당연히 비슷할 수 밖에 없는 결과 - women &amp;woman, females - 를 제외하고 gender stereotypical 한 단어들을 살펴보면gender bias가 사뭇 드러난다. women - chat, smile 은 entailed 관계인반면, women - dicussing, politics 는 contradiction 관계에서 자주등장한다.</p><p><img src="/assets/images/snli_bias_table_2.png?style=centerme" width="80%"></p><h3 id="bias-evaluation-task">Bias evaluation task</h3><p>앞서 소개한 두 방법은 모델 학습결과의 embedding space에서의 단어분포와 데이터 자체의 특성을 이용한 통계학적 관점을 활용하였다.</p><p>이와 다르게 모델의 bias를 확인할 수 있는 방법으로 task basedevaluation 이 있다. bias 를 판단할 수 있는 task 즉, dataset과 metric 을설계해서 점수를 내고 비교하는 방식이다. 이는 다시, template baseddataset 과 crowdsource-based dataset을 활용한 task로 나눠볼 수 있다.</p><p><strong>Gender Bias in Coreference Resolution: Evaluation andDebiasingMethods</strong><sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods](https://www.aclweb.org/anthology/N18-2003/) ">[4]</span></a></sup>에서는 winograd schema 를 바탕으로 bias를 측정할수 있는 task를 제안하였다. WinogradSchema<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge">[8]</span></a></sup>란, Terry Winograd 의 이름을 딴 schema 로, 아래의예시와 같은 문장 형식을 일컫는다.</p><blockquote><p>The city councilmen refused the demonstrators a permit because they[feared/advocated] violence.</p></blockquote><p>이 task의 정답을 맞추기 위해선, 대명사가 무엇을 가리키는지에 대한이해가 필요하고, 이 이해는 councilment과 demonstrator의 관계에 대한지식을 바탕으로 한다.</p><p>아래의 두가지 타입을 따르는 문장들이 bias를 테스트하기 위한dataset으로 제안된다. <code>[entity1]</code>, <code>[entity2]</code>는Male 혹은 Female entity 들이고, <code>[pronoun]</code>은 he/she 의대명사이다. 문장 내에서 <code>[pronoun]</code>과 <code>[entity]</code>의관계를 보고, pro-stereotypical 한 관계를 anti-stereotypical 한 관계보다선호한다면 bias가 있는 모델로 판단한다.</p><p><img src="/assets/images/bias_coref_type1.png?style=left" width="45%"><img src="/assets/images/bias_coref_type2.png?style=right" width="45%"><img src="/assets/images/winobias_figure.png?style=centerme" width="55%"></p><p><strong>StereoSet: Measuring stereotypical bias in pretrainedlanguagemodels</strong><sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/abs/2004.09456)">[5]</span></a></sup> 은 4개의 bias domain - gender, profession, race,religion - 에 대해서 crowdsource 방식으로 수집된 데이터와 bias 측정metric 을 제안하고 있다. crowdsource 방식은 template 기반 방식 대비real-world 에 더 가깝게 데이터셋을 수집할 수 있다는 장점이 있다.</p><p>StereoSet에서는 LM이 배워야 할 지식과 배우지 않길 기대하는 지식(social bias) 을 구분해서 학습했는지 여부를 테스트한다. ContextAssociation Test (CAT) 라고 명칭한 Test는 Intrasentence와 Intersentence두 종류의 테스트로 구성되어 있다.</p><p><img src="/assets/images/stereoset_example.png?style=centerme" width="45%"></p><p>그리고 모델이 각 test에 대해 답변한 결과를 ranking problem (option1을 선택한 비율이 option 2를 선택한 비율보다 많았는가, 적었는가) 으로pose 시켜서 최종 bias score를 낸다.</p><p>앞서 언급했듯이, LM이 배워야 할 지식과 배우지 않길 기대하는 지식(social bias) 을 구분해서 학습했는지 여부를 테스트하기 때문에 lms 와 ss를 구분하였고, 이 두 점수를 종합해서 최종 CAT score 를 도출한다.</p><ul><li><strong>Language Modeling Score (lms)</strong><ul><li>model has to rank the meaningful association higher than meaninglessassociaton</li><li>score: percentage of instances in which a language modelprefers</li><li>ideal: 100</li></ul></li><li><strong>Stereotype Score (ss)</strong><ul><li>score: percentage examples in which a model prefers a stereotypicalassociation over an anti-stereotypical association</li><li>ideal: 50</li></ul></li><li><strong>Idealized CAT Score (icat)</strong><ul><li><p>combine both lms and ss</p><p>\[icat = lms *\frac{min(ss, 100-ss)}{50}\]</p></li><li><p>ideal: 100 (when lms: 100 and ss: 50)</p></li><li><p>fully biased: 0 (when lms: 0, ss: 100 or 0)</p></li><li><p>random model: 50 (when lms: 50, ss: 50)</p></li></ul></li></ul><p><strong>CrowS-Pairs: A Challenge Dataset for Measuring Social Biasesin Masked LanguageModels</strong><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://arxiv.org/abs/2010.00133) ">[6]</span></a></sup>은 EMNLP 2020에 억셉된 논문으로, stereoset과마찬가지로 template 기반이 아닌 crowndsourced 데이터셋으로 구성되어있다. 크게 9가지의 bias에 대해 문장 하나는 stereotyping한 것, 다른하나는 덜 stereotyping 한 것으로 구성한다. 두 문장의 거리는 매우가까워야 한다.</p><p><img src="/assets/images/crowspair_table.png?style=centerme" width="80%"></p><p>평가의 경우, 차이가 있는 token을 제외한 나머지 token (=unmodifiedtoken)을 순차적으로 masking 해서 각각의 log-likelihood를 구한 뒤 그 합을최종 점수로 가진다.</p><p><img src="/assets/images/crowspair_figure.png?style=centerme" width="85%"></p><p><img src="/assets/images/crowspair_score.png?style=centerme" width="35%"></p><h3 id="limitations-on-bias-in-nlp-researches">Limitations on "Bias inNLP" researches</h3><p><strong>Language (Technology) is Power: A Critical Survey of “Bias”in NLP</strong><sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Language (Technology) is Power: A Critical Survey of “Bias” in NLP](https://arxiv.org/abs/2005.14050)">[7]</span></a></sup>에서는 기존의 146개 "bias in NLP" paper에 대한survey를 진행하면서 이전 연구들에 대한 비판과 이를 극복할 수 있는방향으로의 연구를 제안하였다.</p><p>비판의 포인트는 크게 3가지이다.</p><ol type="1"><li>"bias"의 정의에 대한 논의 부재</li><li>"bias"로 인해 발생하는 문제에 대한 고민 부족 (movitations are oftenvague, inconsistent, and lacking in normative reasoning)</li><li>기존 학계에서 논의되고 있는 "bias"와의 약한 연결성</li></ol><p>예를 들어, "racial bias"에 대한 다음 기존 연구들을 살펴보자. 다루고있는 주제는 racial bias 지만 실제로는 보다 협소한 African-AmericanEnglish (AAE) 에 대해서 다루고 있다. (그래놓고 제목에 racial bias를적어둔 것은 뭘까? Asian 차별에 대해서는?) 그리고 같은 AAE에 대해 다음과같이 다양한 방식으로 bias 가 존재한다고 주장하고 있다.</p><ul><li>African American과 연관된 이름은 pleasant words보다 unpleasantwords에 더 가까움</li><li>POS tagger, Language Identification, dependency parser 에서 AAE에연관된 term이 포함되는 경우 덜 정확함</li><li>toxicity detection system 이 AAE 와 연관된 feature 가 있는 트윗을 더offensive 하다고 판단내리는 경향이 있음</li></ul><p>기존 연구들 중 어떤 것도 AAE 혹은 racial hierarchies in the US에서의racial hierarchies, raciolinguistic 분석을 언급하지 않았다. 또, AAE를누가 이야기하는가에 따라서도 bias 여부를 다르게 판단할 수 있으나 이에대한 고려는 없이 오직 text 만 놓고 판단하였다. 어떤 맥락에서 AAE 가문제가 될 수 있고, bias 되었다고 판단할 수 있는지에 대한 고려는없었다.</p><p>개인적으로도 단순 데이터/모델 결과 분석의 방법론이 애매하다는 생각이든다. bias가 있는 문장/단어에 대해 다른 문장/단어와 다른 결과를도출한다고 이야기하고 있지만, 임의의 특성들에 대해서도 심층적으로분석해보면 똑같은 결과가 나올 것 같다.</p><p>그리고 Crowdsourcing 으로 만들어진 dataset이 bias 가 없음을 보장하는내용도 부족했다. US 에 살고 있는 사람들이 annotator 로 참여했지만보수적인 주의 사람들이 더 많았다거나, bias에 대한 지식이 부족한 사람들이더 많았다면 문제가 될 여지가 있다.</p><h2 id="so">So?</h2><p>올바른 방식으로 bias in NLP 주제를 tackle 하기 위해서는 언제 Biased모델이 문제를 일으키는지를 우선 고민해봐야할 것 같다.</p><p>Biased model은 언제 문제가 될까? (Open Question) Bias 의 범위가넓으므로 gender bias 에 국한해서 생각해보자. 만약 gender bias detectionmodel 을 간단하게 KcBERT를 korean-hatespeech-dataset (gender bias)task에finetuning<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://github.com/inmoonlight/detox/tree/master">[10]</span></a></sup> 한 것으로 사용한다고 했을 때, 모델이 틀리는gender-biased 예문이 o.o.d test set 에서 많이 등장한다면 문제가 될까?혹은 정답은 제대로 맞추었더라도 잘못된 단어를 queue로 받아서 맞추는것이라면 문제이지 않을까?</p><p>문제가 된다면, 이 문제는 무엇으로부터 기인할까?</p><ul><li>KcBERT 가 학습한 데이터로부터 오는 문제</li><li>KcBERT를 finetuning 하는 방법으로부터 오는 문제</li><li>finetuning dataset 의 문제<ul><li>제작 시 "gender bias"를 annotator 의 직관에 맡기기보다는 guideline을바탕으로 tagging 되었다고 볼 수 있다. 따라서 annotator의 bias를최소화했다고 보여진다.</li></ul></li><li>dataset 이 작기 때문에 real world 를 충분히 반영하지 못해서 나타나는문제<ul><li>Human-in-the-loop 으로 풀어볼 수 있지 않을까?</li><li>static benchmark 가 쉽게 stale 해지는 것을 막기 위해 제안된DynaBench<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://dynabench.org/">[11]</span></a></sup>라는 dynamic benchmark framework을 참고하면어떨까? DynaBench는 user들이 특정 task를 푸는 목적으로 학습된 모델이틀리는 예문을 생성하고, 이를 바탕으로 다시 모델이 학습해서 지속적으로발전할 수 있도록 만들어졌다.</li></ul></li></ul><p>간단하게 학습시킨 KcBERT finetuning model 을 통해 예측한 결과를보았을 때 주어진 testset 과 생각해볼 수 있는 간단한 예문에 대해서는나쁘지 않은 결과를 보임을 확인하였다. DynaBench를 benchmarking 한웹사이트를 만들어보면 재밌는 결과를 수집할 수 있지 않을까?</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" rel="external nofollow noopener noreferrer" target="_blank">Manis to computer programmer as woman is to homemaker? debiasing wordembeddings</a><a href="#fnref:1" rev="footnote"> ↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1608.07187" rel="external nofollow noopener noreferrer" target="_blank">Semanticsderived automatically from language corpora contain human-likebiases</a><a href="#fnref:2" rev="footnote"> ↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/W17-1609/" rel="external nofollow noopener noreferrer" target="_blank">SocialBias in Elicited Natural LanguageInferences</a><a href="#fnref:3" rev="footnote"> ↩︎</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/N18-2003/" rel="external nofollow noopener noreferrer" target="_blank">GenderBias in Coreference Resolution: Evaluation and DebiasingMethods</a><a href="#fnref:4" rev="footnote"> ↩︎</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/2004.09456" rel="external nofollow noopener noreferrer" target="_blank">StereoSet:Measuring stereotypical bias in pretrained languagemodels</a><a href="#fnref:5" rev="footnote"> ↩︎</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/2010.00133" rel="external nofollow noopener noreferrer" target="_blank">CrowS-Pairs:A Challenge Dataset for Measuring Social Biases in Masked LanguageModels</a><a href="#fnref:6" rev="footnote"> ↩︎</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/2005.14050" rel="external nofollow noopener noreferrer" target="_blank">Language(Technology) is Power: A Critical Survey of “Bias” inNLP</a><a href="#fnref:7" rev="footnote"> ↩︎</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://en.wikipedia.org/wiki/Winograd_Schema_Challenge<a href="#fnref:8" rev="footnote">↩︎</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://kyunghyuncho.me/social-impacts-bias-of-ai/<a href="#fnref:9" rev="footnote">↩︎</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://github.com/inmoonlight/detox/tree/master<a href="#fnref:10" rev="footnote">↩︎</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://dynabench.org/<a href="#fnref:11" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;한 스타트업에서 개발한 인공지능 채용솔루션(a.k.a. AI 면접관)을 벌써
여러 기업에서 사용하고 있다는 뉴스기사를 접하게 되었다. 해당 기업은
&quot;성별이나 학력 등에 따른 차별 방지와 정확한 역량 추정&quot;을 위해 5만
2천여명의 데이터를 확보하여 학습했다고 말한다. 5만 2천여명의 데이터와
차별 방지가 어떤 관련이 있는지는 모르겠지만, &lt;em&gt;많은&lt;/em&gt; 양의 데이터를
사용한다는 걸 내세우고 싶었다면 대량의 데이터가 편향성을 줄이는 것과는
무관하다고 말하고 싶다. 5만 2천개보다 더 많은 데이터로 학습한 Language
Model 도 편향성 문제가 있으며 이 이슈는 아직도 연구자들에 의해 활발히
연구되고 있다.
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/categories/ML/NLP/"/>
    
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="social good" scheme="https://inmoonlight.github.io/tags/social-good/"/>
    
      <category term="bias" scheme="https://inmoonlight.github.io/tags/bias/"/>
    
  </entry>
  
  <entry>
    <title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (a.k.a. T5)</title>
    <link href="https://inmoonlight.github.io/2020/08/29/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/"/>
    <id>https://inmoonlight.github.io/2020/08/29/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer/</id>
    <published>2020-08-29T08:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.144Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>최근 NLP task에서 좋은 성능을 보이는 모델은 대량의 monolingualcorpus를 통해 unsupervised pre-training을 한 LM을 task에 맞게 supervisedfine-tuning을 하는 transfer learning에 기반하고 있다. 같은 transferlearning framework 안에서도 다양한 모델이 존재한다. 우리가 아는 모델만하더라도 BERT, GPT, ELMO 등이 있고, GLUE benchmark에 대해서 테스트한점수가 있다.</p><p>하지만 과연 점수가 더 높다고 더 좋은 모델이라고 할 수 있을까? 우리가모델이라고 부르는 것 안에는 학습 방식 외에도 학습에 사용한 데이터셋,optimizer, 모델의 크기 등 많은 내용이 함축되어 있다. 그래서 각 모델의아이디어 중 과연 <strong>"어떤 특징이 좋은 모델 성능을 내는데에 도움이되었을까?"</strong>라고 묻는다면 쉽게 대답하기 어렵다.</p><p>이 논문에서 소개하는 Text-to-Text Transfer Transformer (T5) 는 그답을 찾기 위해 고안한 framework이다. <a id="more"></a></p><h2 id="transfer-learning-framework-text-to-text-transfer-transformer-t5">Transferlearning framework: Text-to-Text Transfer Transformer (T5)</h2><p>T5는 모든 task를 transformer의 building block으로 하는 seq2seqframework 이다 (주의! encoder-decoder 와는 다르다. sequence X가 입력되면sequence Y가 출력되는 것일 뿐). 다양한 downstream tasks (questionanswering, document summarization, semtiment classification, machinetranslation, etc) 를 하나의 모델 안에서 해결해야 서로 다른 transferlearning 방식의 효과를 공정하게 비교할 수 있기 때문에 이와 같은 unifiedframework 가 제안되었다. 분석하고 싶은 내용과 무관한 특징들 -- 사용한데이터, tokenizer, vocab size, learning rate 등 -- 은 task에 상관없이고정된다.</p><p><img src="https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s1600/image3.gif?style=centerme" alt="T5의 학습 framework. output이 text sentence이든, class이든, score이든 모두 text-to-text framework로 학습하는 것을 볼 수 있다. <br> source: https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" width="80%"></p><p><img src="https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s1600/image2.png?style=centerme" alt="T5의 transfer learning schema. Pre-training과 Fine-tuning step으로 구분되어 있으며 기본적인 text-to-text framework는 그대로 고수한다. <br> source: https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" width="80%"></p><h2 id="pre-training-dataset-colossal-clean-crawled-corpus-c4">Pre-trainingdataset: Colossal Clean Crawled Corpus (C4)</h2><p>Transfer learning framework에 사용되는 pre-trained model은 어떤종류의 corpus를 사용했는지, 얼만큼의 corpus를 사용했는지에 따라서도특징이 달라진다. 이에 대한 실험을 위해 논문에서는 common crawl로 수집한아주 많은 양의 데이터를 소개한다.</p><p>양에 따른 모델 성능을 비교하기 위해 우선 기존의 Wikipedia corpus 보다2배 이상 큰 사이즈의 데이터를 crawling 한다. 인터넷에서 crawling한문서는 보통 매우 지저분하다. 이런 지저분한 데이터를 학습에 바로 사용하게되면 side effect 가 있을 수 있기 때문에 중복 제거, 미완성 문장 제거,공격적이거나 bias가 있는 문장 제거 등의 cleansing process를 거치고깔끔한 데이터를 남긴다. 이 데이터셋의 이름이 Colossal Clean CrawledCorpus (C4) 이다. 여러 필터링을 거쳤음에도 Wikipedia corpus 크기의 2배이상이라고 한다.</p><p><a href="https://www.tensorflow.org/datasets/catalog/c4" rel="external nofollow noopener noreferrer" target="_blank">TFdastasets</a>에서 다운로드 가능하다.</p><h2 id="main-contributions-of-the-paper">Main Contributions of thepaper</h2><p>앞서 소개했듯이, 이 논문에서 풀고 싶은 질문은 "다양한 NLP transferlearning framework 중에서 어떤 feature가 좋은 성능을 내는데에 도움이될까?"이다. 따라서 조금씩 training schema를 달리해가며 여러 downstreamtask의 성능을 비교해야 한다.</p><p>이 논문에서는 크게 1) <a href="#pre-training-architecture">Pre-training architecture</a> 2) <a href="#pre-training-objective">Pre-training objective</a> 3) <a href="#pre-training-dataset">Pre-training dataset</a> 4) <a href="#pre-training-datasize">Pre-training datasize</a> 5) <a href="#training-strategy">Training strategy</a> 6) <a href="#scaling-strategy">Scaling strategy</a>를 주목하고 있다. 그리고중요한 Disclaimer로, <strong><em>여기서 소개하는 architecture와objective는 GPT, ELMO 등의 구현을 아주 정확하게 replicate 하고 있지않다</em></strong>고 언급한다. 어느 정도 이 모델들의 구조에 motivate된내용이 보이지만 정확하게 같지는 않다.</p><h3 id="pre-training-architecture">Pre-training architecture</h3><h4 id="encoder-decoder-baseline-vs.-language-model-vs.-prefix-lm">Encoder-Decoder(Baseline) vs. Language Model vs. Prefix LM</h4><p>크게 세가지 모델 구조를 실험하였다. Encoder-Decoder에서 Encoder는fully-visible attention을 적용하였고 decoder만 causal attention을적용하였다. LM은 전부 causal attention이며, 이는 uni-directional하게정보를 습득하는 것을 나타낸다. PrefixLM의 경우 encoder-decoder 와 유사한것으로 보인다. input인 x에 대해서는 bi-directional하게 정보를 습득하고y만 causal attention이 적용된다.</p><p><img src="/assets/images/t5-architecture-variants.png?style=centerme" width="50%" alt="모든 모델의 입력과 출력은 text와 text이다. building block은 transformer이며, block과 block을 연결하는 attention의 방식이 <br> 아래 그림의 Fully-visible, Causal, Causal with prefix 중에 해당하는 것을 볼 수 있다."><img src="/assets/images/t5-attention-variants.png?style=centerme" width="60%"></p><p><strong>Q. 어떤 Pre-training model architecture 가 가장효과적일까?</strong></p><p><img src="/assets/images/t5-architecture-results.png?style=centerme" width="90%"></p><p>Encoder-Decoder architecture가 가장 효과적이었다. LM의 성능이 가장좋지 않았는데, 이를 통해 bi-directional context를 input으로 넣어주는것이 효과적이라는 사실을 알 수 있다.</p><h3 id="pre-training-objective">Pre-training objective</h3><p>아래 이미지에서 확인할 수 있듯, 크게 네단계로 나누어서 생각해볼 수있다: 1) High-level approaches 2) Corruption strategies 3) Corruptionrate 4) Corrupted span length</p><p><img src="/assets/images/t5-flowchart-unsupervised-objectives.png?style=centerme" width="60%"></p><h4 id="high-level-approaches">High-level approaches</h4><p>세가지 방식을 비교한다. Prefix language modeling 은 문장의 앞부분을context로 제공하는 방식, BERT-style 은 Masked-LM (MLM) 방식, 그리고Deshuffling 은 문장의 token을 뒤섞고 원 문장을 맞추는 방식이다.</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>Objective</th><th>Inputs</th><th>Targets</th></tr></thead><tbody><tr class="odd"><td>Prefix language modeling</td><td>Thank you for inviting</td><td>me to your party last week .</td></tr><tr class="even"><td>BERT-style</td><td>Thank you &lt;M&gt; &lt;M&gt; me to your party apple week.</td><td>(original text)</td></tr><tr class="odd"><td>Deshuffling</td><td>party me for your to . last fun you inviting week Thank</td><td>(original text)</td></tr></tbody></table><p><strong>Q. 어떤 High-level approach 가 가장 효과적일까?</strong></p><p><img src="/assets/images/t5-high-level-result.png?style=centerme" width="80%"></p><p>위의 표의 결과에 따르면 BERT-style (MLM) objective 가 가장 좋은성능을 보인다.</p><h4 id="corruption-strategies">Corruption strategies</h4><p>이번에는 High-level approaches 중 가장 좋은 성능을 보인 BERT-styleapproach 에 대해서 적용해 볼 수 있는 다양한 corruption strategies 에따라 실험한다. 총 세가지 전략이 있다. 모두 i.i.d. 로 masking을 하는 것은동일하지만, token 단위로 masking 하는 방식이 있고 (i.i.d. noise, masktokens) span 단위로 masking 해서 예측하는 문장은 입력에서 masking된부분을 예측하고 아닌 부분을 masked token으로 예측하는 방식 (i.i.d.noise, replace spans (a.k.a. Denoising, Baseline)), 그리고 원문에서token을 제외하고 제외된 부분을 예측하는 방식 (i.i.d. noise, drop tokens)이 있다.</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>Objective</th><th>Inputs</th><th>Targets</th></tr></thead><tbody><tr class="odd"><td>i.i.d. noise, mask tokens</td><td>Thank you &lt;M&gt; &lt;M&gt; me to your party &lt;M&gt; week.</td><td>(original text)</td></tr><tr class="even"><td>i.i.d. noise, replace spans (a.k.a, Denoising, Baseline)</td><td>Thank you &lt;X&gt; me to your party &lt;Y&gt; week.</td><td>&lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;</td></tr><tr class="odd"><td>i.i.d. noise, drop tokens</td><td>Thank you me to your party week.</td><td>for inviting last</td></tr></tbody></table><p><strong>Q. BERT-style approach의 다양한 corruption strategy 중 무엇이가장 효과적일까?</strong></p><p><img src="/assets/images/t5-strategy-result.png?style=centerme" width="80%" alt="별표시가 baseline을 의미한다"></p><p>위의 표에 따르면 Denoising 방식이 가장 좋은 성능을 낸다.</p><h4 id="corruption-rate">Corruption rate</h4><p>마찬가지로 위에서 가장 성능이 좋았던 Denoising 방식에서 corruptionrate을 10%, 15%, 25%, 50%로 다르게하며 살펴본다.</p><p><strong>Q. 원 문장의 얼만큼을 corrupt 하는 것이 가장좋을까?</strong></p><p><img src="/assets/images/t5-corruption-result.png?style=centerme" width="80%"></p><p>표에 따르면 15%가 적당하다는 결론이 나온다.</p><h4 id="random-spans">Random spans</h4><p>15% 정도 denoising 방식으로 corrupt 할 때 평균적인 span의 길이를다르게 조정해볼 수 있다. Baseline으로 사용한 i.i.d.와 2, 3, 5, 10 의길이를 비교할 수 있다.</p><p><strong>Q. corrupt할 때 가장 적정한 평균 span 길이는몇일까?</strong></p><p><img src="/assets/images/t5-span-length-result.png?style=centerme" width="80%"></p><p>표에 따르면 i.i.d 에 따라 corrupt하는 것이 가장 효과적이다.</p><h3 id="pre-training-dataset">Pre-training Dataset</h3><p>Pre-training 모델의 성능은 데이터셋에 따라 달라질 수 있다. 이를실험하기 위해 논문에서 수집한 C4와 C4, unfiltered 그리고 다른 특징을가진 데이터셋으로 pre-training 한 후 task 마다의 성능을 비교하였다.결과는 C4를 쓰는 것이 가장 좋았다.</p><p><img src="/assets/images/t5-dataset-result.png?style=centerme" width="80%" alt="TBC는 Toronto Book Corpus의 약자"></p><h3 id="pre-training-datasize">Pre-training datasize</h3><p>GLUE benchmark의 상위권 모델은 보통 pre-training에 사용한 datasize가크며 모델의 사이즈도 굉장히 크다. T5는 다행히도 모델의 capacity가 큰편이라 데이터 사이즈를 조절해가며 실험을 할 수 있었고, C4 전체를사용했을 때 가장 성능이 좋았다. 다시 말해, 아직 모델의 capacity 가 더크다고 이해할 수 있다.</p><p><img src="/assets/images/t5-datasize-result.png?style=centerme" width="80%"></p><h3 id="training-strategy">Training strategy</h3><p>Transfer learning 은 Training strategy에 따라서도 성능이 달라질 수있다. Training strategy는 Fine-tuning, Multi-task learning 방식으로나뉘며 이를 어떻게 조합하는지 또한 전략에 포함되었다.</p><h4 id="fine-tuning-methods">Fine-tuning methods</h4><p>Fine-tuning 은 모든 파라미터를 사용했을 때 가장 성능이 좋다.<img src="/assets/images/t5-finetuning-result.png?style=centerme" width="80%"></p><h4 id="multi-task-learning-pre-training">Multi-task learning(pre-training)</h4><p>T5 의 task는 다양한데, 과연 각 task를 얼만큼 학습시키는 것이 좋을지에대한 실험이다. Equal 의 경우 task dataset의 사이즈에 상관없이 같은 수의문장을 학습시키는 방식이다. K threshold 를 사용한 경우, 기본적으로task의 문장 사이즈만큼 샘플링하되, K 이상의 데이터는 K 만큼만 학습에활용한다.</p><p>결과는 sampling 하지 않는 것이 가장 좋다.<img src="/assets/images/t5-multitask-result.png?style=centerme" width="80%"></p><h4 id="combining-fine-tuning-and-multi-task-learning">Combiningfine-tuning and multi-task learning</h4><p>위에서 소개한 다양한 방식을 조합하여 실험하였고, multi-taskpre-training과 fine-tuning 조합이 가장 좋은 성능을 보였다.<img src="/assets/images/t5-combination-result.png?style=centerme" width="80%"></p><h3 id="scaling-strategy">Scaling strategy</h3><p>scale-up 할 수 있는 hyperparameter의 값을 조절해가며 실험하였다.training steps, batch size, model size 를 조절한 결과, model size 를키우고 training steps 을 늘린 경우에 가장 좋은 성능을 보였다.</p><p><img src="/assets/images/t5-scaling-result.png?style=centerme" width="80%"></p><h2 id="conclusions">Conclusions</h2><p>위의 여러 실험들의 결과를 종합하면, <strong>1) Encoder-Decoderarchitecture 2) Span prediction objective 3) C4 dataset 4) Multi-taskpre-training 5) Bigger models trained longer</strong> 의 구조로 학습할때 가장 좋은 transfer learning 효과를 얻을 수 있다.</p><h2 id="references">References</h2><p>[1] https://youtu.be/eKqWC577WlI <br> [2]https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;최근 NLP task에서 좋은 성능을 보이는 모델은 대량의 monolingual
corpus를 통해 unsupervised pre-training을 한 LM을 task에 맞게 supervised
fine-tuning을 하는 transfer learning에 기반하고 있다. 같은 transfer
learning framework 안에서도 다양한 모델이 존재한다. 우리가 아는 모델만
하더라도 BERT, GPT, ELMO 등이 있고, GLUE benchmark에 대해서 테스트한
점수가 있다.&lt;/p&gt;
&lt;p&gt;하지만 과연 점수가 더 높다고 더 좋은 모델이라고 할 수 있을까? 우리가
모델이라고 부르는 것 안에는 학습 방식 외에도 학습에 사용한 데이터셋,
optimizer, 모델의 크기 등 많은 내용이 함축되어 있다. 그래서 각 모델의
아이디어 중 과연 &lt;strong&gt;&quot;어떤 특징이 좋은 모델 성능을 내는데에 도움이
되었을까?&quot;&lt;/strong&gt;라고 묻는다면 쉽게 대답하기 어렵다.&lt;/p&gt;
&lt;p&gt;이 논문에서 소개하는 Text-to-Text Transfer Transformer (T5) 는 그
답을 찾기 위해 고안한 framework이다.
    
    </summary>
    
    
      <category term="Paper" scheme="https://inmoonlight.github.io/categories/Paper/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="paper" scheme="https://inmoonlight.github.io/tags/paper/"/>
    
      <category term="LM" scheme="https://inmoonlight.github.io/tags/LM/"/>
    
      <category term="Google" scheme="https://inmoonlight.github.io/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>Zero to One (제로투원)</title>
    <link href="https://inmoonlight.github.io/2020/07/20/Zero-to-one-by-peter/"/>
    <id>https://inmoonlight.github.io/2020/07/20/Zero-to-one-by-peter/</id>
    <published>2020-07-20T14:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.149Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>제로투원. 해석하면 0에서 1. 즉, 무에서 유를 창조한다는 의미다. 책의제목에 걸맞게 주변에 스타트업에 다니는 분들에게 자주 추천받았던 책이다.<a id="more"></a></p><h2 id="내용-소개">내용 소개</h2><p>스타트업이 성공하기 위한 조건에 대해 이야기하고 있다. 흔히들"시장우위를 점해서 경쟁에서 승리한 스타트업"을 성공한 스타트업이라고생각한다. 하지만 저자는 할 수 있다면 경쟁은 피할수록 좋다고 말한다. 경쟁때문에 라이벌 회사를 사용자보다 더 신경쓰게 되면 더 이상 서비스가사용자를 향하지 못하기 때문이다. 저자에게 경쟁이란, 아무도 이윤을 얻지못하고 의미 있게 차별화 되는 부분도 없이 생존을 위해 싸우는 것이다.</p><p>그렇다면 저자는 성공하는 스타트업을 어떤 기업으로 생각하고 있을까?아래의 글에 그 답이 있다.</p><blockquote><p>모든 기업은 남들이 할 수 없는 것을 해내는 만큼, 딱 그만큼만 성공할 수있다. <br> 창조적 독점이란, 새로운 제품을 만들어서 모든 사람에게 혜택을주는 동시에 그 제품을 만든 사람은 지속 가능한 이윤을 얻는 것이다. <br>행복한 기업들은 다들 서로 다르다. 다들 독특한 문제를 해결해 독점을구축했기 때문이다. 반면에 실패한 기업들은 한결같다. 경쟁을 벗어나지 못한것이다.</p></blockquote><h2 id="마음에-닿았던-구절">마음에 닿았던 구절</h2><blockquote><p>대학생들은 몇몇 전공 분야에서는 고도의 전문적 기술을 습득하기도하지만, 정작 그 능력으로 더 넓은 세상에서 무엇을 할 수 있는지에 관해서는아무것도 배우지 못한다.</p></blockquote><p>책의 주제와는 상관없지만, 이 문장에 공감하지 않을 수 없었다. 교육이사회를 쫓아가지 못하는 것은 미국이나 한국이나 다를 바 없다는 사실이인상깊었다.</p><blockquote><p>지속적인 가치를 창출하고 또 보유하고 싶다면, 차별화되지 않는 제품으로회사를 차리지 마라.</p></blockquote><p>연울림을 런칭했을 때의 경험이 떠올랐다. 사용자에게 주고 싶은 가치가있었지만, 나와 비슷한 생각으로 이미 사람들에게 가치를 전파하고 있는기업이 어럿있었다. 아이디어가 비슷하더라도, 남들과 다르게 구현했다면차별화된 제품이라고 부를 수 있었겠지만 그러기가 쉽지 않았고 끝내 경쟁속에서 뒤쳐졌다. 사실 가벼운 동아리 같은 느낌이었기 때문에 나쁘지 않은경험이라고 생각되지만, 다음에 무엇인가 시작하게 된다면 (사업이든프로젝트이든) 차별성을 염두에 두어야겠다는 생각이 들었다.</p><blockquote><p>신생기업에게 완벽한 표적 시장은 경쟁자가 없거나 아주 적으면서도특정한 사람들이 적은 규모로 모여 있는 시장이다.<br> 틈새시장을 만들어내지배하게 되었다면, 관련 있는 좀 더 넓은 시장으로 서서히 사업을 확장해야한다.</p></blockquote><p>이 주장은 의견이 다를 수 있다고 생각한다. 컨설팅 회사에서 일해본경험이 있는 사람이라면, 신사업 제안을 할 때의 논리와 절대적으로 반대되는내용이라는 것을 알 것이다. "시장의 규모가 크고, 그 시장의 경쟁상대가누구이며 우리는 그 안에서 어떤 포지셔닝이기 때문에 시장의 n% 를 차지할수 있다." 라는 논리로 이 신사업의 가능성을 타진하는 것이 일반적이다.그런 면에서 이 문장은 신선했고, 제로투원이라는 제목에 걸맞는 주장이라는생각이 들었다.</p><blockquote><p>어려운 일은 성취할 수 있지만, 불가능한 일은 성취할 수 없다</p></blockquote><p>가능성에 대해서 생각해보게 만든 글귀다.</p><blockquote><p>아무도 하지 않고 있는 중요한 일을 왜 우리가 하고 있는지 설명할 수있어야 한다.</p></blockquote><p>새로운 프로젝트, 혹은 사업을 한다면 스스로에게 물어봐야 하는질문이라는 생각이 들었다. 왜 이 문제인지, 왜 우리의 해결책이어야하는지에 대해 나부터 납득시키지 못한다면 누구를 설득할 수 있을까?</p><blockquote><p>사회를 위해서 정말로 좋은 일은 뭔가 남들과 ‘다른’ 일을 하는 것이다.그리고 그렇게 하는 것이야말로 기업이 새로운 시장을 독점해 이윤을 만드는방법이기도 하다. 최고의 프로젝트는 다들 떠들어대는 것이 아니라 남들에게간과되고 있을 가능성이 크다. 가장 덤벼볼 만한 문제는 아무도 해결해보려고하지조차 않는 문제일 때가 많다.<br> 독자 기술은 가장 가까운 대체기술보다 중요한 부분에서 ‘10배’는 더 뛰어나야 진정한 독점적 우위를확보할 수 있다.</p></blockquote><p>아무도 해결해보려고 하지 않는 문제는 불가능한 문제이거나, 어려운 일일것이다. 그리고 어려운 일을 풀 수 있는 기업일수록 더 뛰어난 역량을 보유할것이고, 남들이 쉽게 접근하지 못하는 독점적 우위에 가까운 포지션에 있지않을까?</p><h2 id="개인적인-감상">개인적인 감상</h2><p>흔히 생각하는 성공하는 사업에 대한 조건의 틀에서 빠져나오게 만든책이었다. 누구나 생각할 수 있는 문제와, 누구나 풀 수 있는 해결책이라면그보다 위험한 스타트업은 없을 것 같다. "누구나 공감하는 크고 어려운문제"에 대해 "나만의 해결책"을 가질 수 있는 사람이고 싶다.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;제로투원. 해석하면 0에서 1. 즉, 무에서 유를 창조한다는 의미다. 책의
제목에 걸맞게 주변에 스타트업에 다니는 분들에게 자주 추천받았던 책이다.
    
    </summary>
    
    
      <category term="Book" scheme="https://inmoonlight.github.io/categories/Book/"/>
    
    
      <category term="essay" scheme="https://inmoonlight.github.io/tags/essay/"/>
    
      <category term="book" scheme="https://inmoonlight.github.io/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>How multilingual is Multilingual BERT?</title>
    <link href="https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/"/>
    <id>https://inmoonlight.github.io/2020/06/20/How-multilingual-is-multilingual-BERT/</id>
    <published>2020-06-20T08:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>"How multilingual is MultilingualBERT?"<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/P19-1493.pdf&gt;">[1]</span></a></sup> 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 AutumaticPost-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingualBERT에 대해 분석한 논문을 쓴 것이 아닐까? <a id="more"></a></p><h2 id="abstract">Abstract</h2><p>In this paper, we show that Multilingual BERT (<code>M-BERT</code>),released by Devlin et al. (2018) as a single language model pre-trainedfrom monolingual corpora in 104 languages, is <strong>surprisingly goodat zero-shot cross-lingual model transfer</strong>, in whichtask-specific annotations in one language are used to fine-tune themodel for evaluation in another language. To understand why, we presenta large number of probing experiments, showing that <strong>transfer ispossible even to languages in different scripts</strong>, that<strong>transfer works best between typologically similarlanguages</strong>, that monolingual corpora can train models forcode-switching, and that the model can find translation pairs. Fromthese results, we can conclude that <code>M-BERT</code> does createmultilingual representations, but that these representations exhibitsystematic deficiencies affecting certain language pairs.</p><h2 id="motivation">Motivation</h2><p>Pretrained LM 이 다양한 NLP downstream task 에서 좋은 성능을보여주었다. Pretrained LM의 probing 연구들은 모델이 학습한representation 이 syntactic and named entity 에서 특히 유용한 정보를가지고 있다는 사실을 보여주었지만, 이 연구들은 영어에 대해서만집중적으로 진행되어 왔다. (2019 년 6월 기준) (참고로, BERT 는 2018년11월에 첫 release)</p><h2 id="main-idea">Main Idea</h2><p>영어에 대해서 Pretrained LM이 가지고 있는 정보, 그 중에서도 syntacticand named entity information 이 언어에 상관없이 잘 generalize 되는지분석 - Main task: NER, POS - Main method: Zero-shot cross-lingualtransfer (Multilingual BERT 모델을 한 언어에 대해 finetuning 시키고,다른 언어에 대해 같은 task의 성능을 평가)</p><h2 id="main-findings">Main Findings</h2><ul><li>아래 내용으로 학습한 Multilingual BERT (<code>M-BERT</code>) 는NER과 POS task 에 대해 cross-lingual transfer ability 가 좋다.<ul><li>language identifier 없이</li><li>위키피디아의 문서로 학습 (140 개 언어)</li><li>w/ shared word piece vocab</li></ul></li><li>모든 언어쌍에 대해 zero-shot transfer 가 잘 된 것은 아니었는데그렇다면 왜 이런 차이가 발생할까?<ul><li>finetuning 언어와 evaluation 언어의 vocab overlap 때문은 아님</li><li>오히려 언어의 typological 특징 때문<ul><li>typological 특징도 여러 종류가 있는데 (여기서는 subject/object/verborder, adjective/noun order에 대해서만 결과를 보여줌), 그 중 <strong>SVOorder</strong> 에 가장 큰 영향을 받음</li></ul></li><li>transfer 하기 위한 언어에 대해 학습한 적이 있을 때 transfer가능</li><li><code>M-BERT</code> 의 중간 layer (8/12) 에서 cross-lingualinformation 이 높음</li></ul></li></ul><h2 id="detailed-experiments-and-results">Detailed Experiments andResults</h2><p>Main Question: 무엇이 <code>M-BERT</code>의 zero-shot cross-lingualtransferability를 만들어내는가?</p><h3 id="preliminaries">Preliminaries</h3><h4 id="ner-task">NER task</h4><ul><li>dataset: <a href="https://www.clips.uantwerpen.be/conll2002/ner/" rel="external nofollow noopener noreferrer" target="_blank">CoNLL-2002</a>, <a href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="external nofollow noopener noreferrer" target="_blank">2003 dataset</a>,Google in-house dataset<ul><li>There are four types of phrases: person names (<code>PER</code>),organizations (<code>ORG</code>), locations (<code>LOC</code>) andmiscellaneous names (<code>MISC</code>)</li></ul></li><li>lang: <code>nl</code>, <code>es</code> (2002) / <code>en</code>,<code>de</code> (2003) 총 4개 + in-house dataset with 16 languages(Arabic, Bengali, Czech, German, English, Spanish, French, Hindi,Indonesian, Italian, Japanese, Korean, Portuguese, Russian, Turkish, andChinese)</li><li>example (<code>en</code>)<ul><li>NER tagged plain text: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .</span><br></pre></td></tr></table></figure></li><li>NER data: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">     Wolff B-PER</span><br><span class="line">         , O</span><br><span class="line"> currently O</span><br><span class="line">         a O</span><br><span class="line">journalist O</span><br><span class="line">        in O</span><br><span class="line"> Argentina B-LOC</span><br><span class="line">         , O</span><br><span class="line">    played O</span><br><span class="line">      with O</span><br><span class="line">       Del B-PER</span><br><span class="line">    Bosque I-PER</span><br><span class="line">        in O</span><br><span class="line">       the O</span><br><span class="line">     final O</span><br><span class="line">     years O</span><br><span class="line">        of O</span><br><span class="line">       the O</span><br><span class="line"> seventies O</span><br><span class="line">        in O</span><br><span class="line">      Real B-ORG</span><br><span class="line">    Madrid I-ORG</span><br><span class="line">         . O</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="pos-task">POS task</h4><ul><li>dataset: Universal Dependencies (UD) data (Universal dependenciesv1: A multilingual treebank collection, Nivre, 2019) for 41 languages<ul><li>Arabic, Bulgarian, Catalan, Czech, Danish, German, Greek, English,Spanish, Estonian, Basque, Persian, Finnish, French, Galician, Hebrew,Hindi, Croatian, Hungarian, Indonesian, Italian, Japanese, Korean,Latvian, Marathi, Dutch, Norwegian (Bokmaal and Nynorsk), Polish,Portuguese (European and Brazilian), Romanian, Russian, Slovak,Slovenian, Swedish, Tamil, Telugu, Turkish, Urdu, and Chinese</li></ul></li><li>evaluation set: Multilingual Parsing from Raw Text to UniversalDependencies, Zemman et al. 2017 (CoNLL 2017 shared Task)<ul><li><a href="https://raw.githubusercontent.com/UniversalDependencies/UD_Korean-PUD" rel="external nofollow noopener noreferrer" target="_blank">example(<code>ko</code>)</a>: <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sent_id &#x3D; n01007012</span><br><span class="line"># text &#x3D; 이 부분에서 게임과 우리 일상 생활 사이의 유사점을 찾을 수 있습니다.</span><br><span class="line"># text_en &#x3D; There are parallels to draw here between games and our everyday lives.</span><br><span class="line"># translit &#x3D; .i .bu.bun.e.seo .ge.im.gwa .u.ri .il.sang .saeng.hwal .sa.i.yi .yu.sa.jeom.eul .chaj.eul .su .iss.seub.ni.da.</span><br><span class="line">1이_DETDT_2det_Translit&#x3D;.i|LTranslit&#x3D;_</span><br><span class="line">2부분에서부분NOUNNN+CMCase&#x3D;Advb|Polite&#x3D;Form9advmod_MSeg&#x3D;부분-에서|Translit&#x3D;.bu.bun.e.seo|LTranslit&#x3D;.bu.bun</span><br><span class="line">3게임과게임NOUNNN+CPPolite&#x3D;Form7compound_MSeg&#x3D;게임-과|Translit&#x3D;.ge.im.gwa|LTranslit&#x3D;.ge.im</span><br><span class="line">4우리_PRONPRPPerson&#x3D;16compound_Translit&#x3D;.u.ri|LTranslit&#x3D;_</span><br><span class="line">5일상_NOUNNN_6compound_Translit&#x3D;.il.sang|LTranslit&#x3D;_</span><br><span class="line">6생활_NOUNNN_3conj_Translit&#x3D;.saeng.hwal|LTranslit&#x3D;_</span><br><span class="line">7사이의사이NOUNNN+CMCase&#x3D;Gen|Polite&#x3D;Form8nmod:poss_MSeg&#x3D;사이-의|Translit&#x3D;.sa.i.yi|LTranslit&#x3D;.sa.i</span><br><span class="line">8유사점을유사점NOUNNN+CMCase&#x3D;Acc|Polite&#x3D;Form9obj_MSeg&#x3D;유사점-을|Translit&#x3D;.yu.sa.jeom.eul|LTranslit&#x3D;.yu.sa.jeom</span><br><span class="line">9찾을_VERBVVForm&#x3D;Adn10acl:relcl_Translit&#x3D;.chaj.eul|LTranslit&#x3D;_</span><br><span class="line">10수_NOUNNNB_11nsubj_Translit&#x3D;.su|LTranslit&#x3D;_</span><br><span class="line">11있습니다_ADJJJMood&#x3D;Ind|VerbForm&#x3D;Fin0root_SpaceAfter&#x3D;No|Translit&#x3D;.iss.seub.ni.da|LTranslit&#x3D;_</span><br><span class="line">12..PUNCT._11punct_Translit&#x3D;.|LTranslit&#x3D;_</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="code-switching-cs-and-transliterate-tlit-task">Code-switching(CS) and Transliterate (Tlit) task</h4><ul><li>Code-switching: 여러 언어가 한 문장에 등장하는 경우<ul><li>ex) <code>I thought मौसम different होगा बस fog है</code></li></ul></li><li>Tlit: 음차 표기<ul><li>ex) <code>I thought mosam different hoga bas fog hy</code></li></ul></li></ul><hr><h3 id="m-bert-의-cross-lingual-transferability-는-vocab-overlap-때문일까-no"><code>M-BERT</code>의 cross-lingual transferability 는 vocab overlap 때문일까? → NO</h3><ul><li>vocab overlap: fine-tuning dataset (train) 의 word piece 와evaluation dataset (test) 의 word piece 간의 overlap \[overlap = {|E_{train} ∩ E_{eval} |}{| E_{train} ∪ E_{eval}|}\]</li><li>검증 방식:<ul><li>NER task 중 16개의 언어에 대한 in-house 데이터셋으로 가능한 언어쌍(16 * 15 = 240 개) 에 대해 overlap을 구하고, trasfer score (F1) 를report</li><li><em><strong>(결과) <code>M-BERT</code>는 vocab overlap 과 무관하게generally 성능이 좋다. vocab overlap 이 0인 언어쌍에 대해서도 최소 40%의F1 score 를 보인다. 반면 EN-BERT 는 vocab overlap 에 굉장히 많이 영향을받는다.</strong></em><img src="https://user-images.githubusercontent.com/24843996/85154700-bfbc9d00-b292-11ea-8f7b-f25ca48b965b.png" width="90%" alt="credit) http://www.dhgarrette.com/papers/pires_multilingual_bert_acl2019_slides.pdf"></li></ul></li></ul><hr><h3 id="m-bert의-cross-lingual-transferability-는-언어의-typological-특징-때문일까-yes"><code>M-BERT</code>의cross-lingual transferability 는 언어의 typological 특징 때문일까? →YES</h3><ul><li><p>근거: POS accuracy of <code>ur</code> → <code>hi</code> (91%)while <code>en</code>→ <code>ja</code> (49.4%) (둘 다 다른 script 를사용하는 언어쌍, a.k.a vocab overlap ~= 0)</p></li><li><p>typologicalfeatures<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/P12-1066.pdf&gt;">[2]</span></a></sup><img src="https://user-images.githubusercontent.com/24843996/85156077-8553ff80-b294-11ea-8a8d-5478762d6b43.png?style=centerme" width="85%" alt="typological features"><br></p></li><li><p><em><strong>(결과) 공통된 typological feature 의 개수가 많을수록transferabiltiy 향상</strong></em><img src="https://user-images.githubusercontent.com/24843996/85156248-c3e9ba00-b294-11ea-8ca1-b3e74ea88860.png?style=centerme" width="45%"></p></li><li><p><em><strong>(결과) 여러 typological features 중에서 SOV order 와AN order의 영향을 비교했을 때 전자가 더 영향이 큼</strong></em><img src="https://user-images.githubusercontent.com/24843996/85156350-ea0f5a00-b294-11ea-862f-64ab5e3166f3.png?style=centerme" width="50%"></p></li></ul><h3 id="m-bert의-cross-lingual-transferability-는-cs-혹은-tlit-까지-적용될-수-있을까-cs-yes-tlit-no"><code>M-BERT</code>의cross-lingual transferability 는 CS 혹은 Tlit 까지 적용될 수 있을까? →CS (YES) / Tlit (NO)</h3><ul><li>CS 실험 목적: Generalizing to code-switching is similar to othercross-lingual transfer scenarios, but would beneﬁt to an even largerdegree from a shared multilingual representation.</li><li>Tlit 실험 목적: Generalizing to transliterated text is similar toother cross-script transfer experiments, but has the additional caveatthat <em><code>M-BERT</code> was not pre-trained on text that looks likethe target</em>.</li><li><strong><em>(결과) <code>M-BERT</code>는 CS text 에 좋은 성능을 보임(90.56% ⇒ 86.59%). 하지만 Tlit 은 이러한 종류의 데이터에 학습되지않고서는 trasferability를 기대하기 어려움 (85.64% ⇒50.41%)</em></strong><img src="https://user-images.githubusercontent.com/24843996/85159242-34dea100-b298-11ea-9922-3b1acc66ef7b.png?style=centerme" width="60%" alt="credit) https://www.youtube.com/watch?v=ZGZy_GrFkAY"></li></ul><h3 id="m-bert의-feature-space">4. <code>M-BERT</code>의 featurespace</h3><ul><li>WMT16 병렬 코퍼스를 사용해서 언어쌍 간의 NN accuracy 측정</li><li><strong><em>(결과) 중간 layer에서 linguistic information을 공유하고이는 언어에 관계없이 비슷하게 나타남</em></strong><img src="https://user-images.githubusercontent.com/24843996/85160445-7ae83480-b299-11ea-8ff2-3209922e78e7.png?style=centerme" width="45%"></li></ul><h2 id="my-thoughts-on-the-results">My Thoughts on the Results</h2><h3 id="vocab-overlap-실험에서-en-bert와의-비교는-정당한가">vocaboverlap 실험에서 <code>EN-BERT</code>와의 비교는 정당한가?</h3><ul><li>제 추측으로는, vocab overlap 실험에서 <code>M-BERT</code> 만을보았을 때, 이 경향성이 overlap에 영향을 받는 것인지 아닌지 판단하기어려웠기 때문에 상대 비교를 할만한 결과가 필요해서 EN-BERT의 실험 결과를넣은 것 같다.<ul><li>아래 이미지에서 corr 을 보면 어느 정도 유의미해 보이는 양의상관관계가 나올 것 같고, 그렇다면 영향을 받는다고 해석해야할 수도있지만,</li><li>vocab overlap이 0임에도 40%의 성능을 보이므로 영향을 받는다고 하기도애매한 상황이 아니었을까?<img src="https://user-images.githubusercontent.com/24843996/85162060-ac61ff80-b29b-11ea-90a2-7e497a5196a9.png?style=centerme" width="70%"></li></ul></li><li>그렇지만 EN-BERT와의 비교가 정당하다고 생각하긴 어려운 것 같다.<ul><li>NER 예측 task 는 sent -&gt; model (either <code>M-BERT</code> orEN-BERT) -&gt; last activation -&gt; add.layer -&gt; NER prediciton 로진행되는데, sent 가 영어가 아닌 경우 tok 단계에서부터<code>unk</code>으로 인식될 가능성이 높기 때문에 transfer는 고사하고fine-tuning도 어려울 수 있다. 이 때문에 논문에서 EN-BERT와 XLM을비교하는데, Indo-european 인 (<code>de</code>, <code>nl</code>,<code>es</code>) 에 대해서만 나와있다. (Table 3)</li><li>영어와 alphabet 이 비슷하면, unk 이 나오지 않을 가능성이 높다고생각해서 위의 결과가 EN-BERT로 얻어진 것에 대해 크게 거부감이 들지않았고, 오히려 CJK 에 대해서 보였어야 하는 것 아닌가 하는 의심이들었다.</li><li>그래서 🤗 로 간단하게 tokenize 결과를 비교해보았다.</li><li>sent 가 <code>es</code> 인 경우<ul><li>sent: Por su parte , el Abogado General de Victoria , Rob Hulls ,indicó que no hay nadie que controle que las informaciones contenidas enCrimeNet son veraces .</li><li><code>M-BERT</code> tok: Por su parte , el Ab ##oga ##do General deVictoria , Rob Hull ##s , ind ##ic ##ó que no hay nadie que controle quelas informa ##ciones conte ##nida ##s en Crime ##Net son vera ##ces.</li><li>EN-BERT tok: Po ##r su part ##e , el A ##bo ##gado General deVictoria , Rob Hull ##s , in ##dic ##ó que no hay na ##die que control##e que las inform ##ac ##ione ##s con ##ten ##idas en Crime ##Net sonve ##race ##s .</li></ul></li><li>sent 가 <code>ko</code> 인 경우<ul><li>sent: 언어(言語)에 대한 정의는 여러가지 시도가 있었다.</li><li><code>M-BERT</code> tok: 언 ##어 ( 言 語 ) 에 대한 정 ##의 ##는 여러##가지 시 ##도가 있었다 .</li><li>EN-BERT tok: [UNK] ( [UNK] [UNK] ) [UNK] [UNK] [UNK] [UNK] [UNK][UNK] .</li></ul></li><li><strong><em>위의 결과로 미루어보아, vocab overlap 이 낮으면서 성능도낮았던 점들의 언어쌍에는 EN-BERT에서 unk이 많이 나왔던 언어가 포함되어있지 않을까하는 생각이 들었고, 비교가 공정하지 않다는 생각이들었다.</em></strong></li></ul></li><li>또한 tlit. 을 <code>M-BERT</code> 가 못하는 이유로, pre-trainstep에서 tlit. corpus 가 없었기 때문이라고 언급하였는데 EN-BERT 또한같은 이유로 성능이 낮을 수 밖에 없었을 것이라고 생각.</li></ul><h3 id="sov-order-가-중요한-점이었을까">SOV order 가 중요한점이었을까?</h3><ul><li>논문에서는 아래 표에서 SVO -&gt; SVO (81.55) &gt; SVO -&gt; SOV(66.52) 라는 점 때문에 SOV order 가 가장 중요하다고 주장한다.<ul><li>하지만 반대로 SOV -&gt; SOV (64.22) &gt; SOV -&gt; SVO (63.98) 의차이가 적게 나는 점은 설명할 수 없다.<img src="https://user-images.githubusercontent.com/24843996/85165966-93f4e380-b2a1-11ea-9290-8451958499e3.png?style=centerme" width="50%"></li></ul></li><li>제 추측으로는, 오히려 각 그룹을 구성하는 언어와 그 언어들이Wikipedia 에서 차지하는 비율, 즉 <code>M-BERT</code> 학습에 영향을 많이끼친 언어가 중요한 역할을 했을 수도 있을 것 같다.<ul><li>SVO languages: Bulgarian, Catalan, Czech, Danish, English, Spanish,Estonian, Finnish, French, Galician, Hebrew, Croatian, Indonesian,Italian, Latvian, Norwegian (Bokmaal and Nynorsk), Polish, Portuguese(European and Brazilian), Romanian, Russian, Slovak, Slovenian, Swedish,and Chinese.</li><li>SOV Languages: Basque, Farsi, Hindi, Japanese, Korean, Marathi,Tamil, Telugu, Turkish, and Urdu.<ul><li>Urdu -&gt; Hindi 의 성능이 91% 였던 점을 고려하면 이 중 어떤언어쌍에서 굉장히 낮은 성능을 보였을 것으로 예상 (평균이 64.22이어야하므로)</li><li>그룹의 평균치를 보지 않았다면 다른 해석이 가능했을 수도..?</li></ul></li></ul></li><li>SVO order 가 비슷하면 -&gt; transfer 가 잘된다! 라는 주장을 하고싶었다면 비교하려는 대상 언어쌍들간에 SVO order 빼고는 조건을 동일하게만족시켰어야 하지 않을까하는 아쉬움이 남는다.</li></ul><h3 id="feature-space">Feature space</h3><ul><li><p>MLM은 보통 중간 layer 에서 semantic 한 성질이 가장 두드러지게나타나는 것 같다.<img src="https://user-images.githubusercontent.com/24843996/85170841-ed144580-b2a8-11ea-9db9-099c019568f3.png?style=centerme" alt="credit) The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives" width="60%"></p><figure><img src="https://user-images.githubusercontent.com/24843996/85170937-1208b880-b2a9-11ea-81e8-c1102286646a.png" alt="credit) BERTScore: Evaluating Text Generation with BERT"><figcaption aria-hidden="true">credit) BERTScore: Evaluating TextGeneration with BERT</figcaption></figure></li></ul><h3 id="논문의-분석-내용">논문의 분석 내용</h3><ul><li>논문에서 분석한 결론이 지나친 일반화가 아닐까하는 생각도 든다.</li><li>일단 <code>M-BERT</code>가 zero-shot transferability 가 높은 이유는어쩌면 같은 내용의 Wikipedia 문서로 학습했기 때문일 수 있다.<ul><li>만약에 한국어는 동일 내용에 대한 번역된 문서가 없는 (e.g., 네이버블로그) 로 학습하고, 영어도 영어 나름대로의 번역문이 없는 문서로 학습된BERT 였더라도 같은 결과가 도출되었을지는 모르겠다.</li></ul></li><li><code>M-BERT</code> 의 학습 데이터의 특징에서 기인한 특징이 얼마나되는지도 궁금하다. 오히려 여기에서 영향을 많이 받았을 수도 있을 것같다.</li></ul><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P19-1493.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/P19-1493.pdf</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/P12-1066.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/P12-1066.pdf</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&quot;How multilingual is Multilingual
BERT?&quot;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;&amp;lt;https://www.aclweb.org/anthology/P19-1493.pdf&amp;gt;
&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt; 는 ACL 2019 에 억셉된 논문으로, Telmo Pires 가
Google AI Residency 프로그램 중에 작성하였다. Unbabel 에서 Autumatic
Post-Editing (APE) 쪽 연구를 진행했었던 연구자였고, 그래서 multilingual
BERT에 대해 분석한 논문을 쓴 것이 아닐까?
    
    </summary>
    
    
      <category term="Paper" scheme="https://inmoonlight.github.io/categories/Paper/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="paper" scheme="https://inmoonlight.github.io/tags/paper/"/>
    
      <category term="BERT" scheme="https://inmoonlight.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기</title>
    <link href="https://inmoonlight.github.io/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/"/>
    <id>https://inmoonlight.github.io/2020/05/28/Retrospect-of-Constructing-Korean-HateSpeech-Dataset/</id>
    <published>2020-05-28T12:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>약 4-5개월동안 사이드로 진행했던 혐오 댓글프로젝트<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://github.com/kocohub/korean-hate-speech&gt;">[1]</span></a></sup>가 성공적으로 마무리되었다. 같은 문제의식을 가진사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의상호보완적인 역량 덕분이 아니었을까 싶다.</p><p>사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는사람들에게도 좋은 팁이 되지 않을까? <a id="more"></a></p><hr><h2 id="어노테이션">어노테이션</h2><h3 id="왜-편견과-혐오인가">왜 <code>편견</code>과<code>혐오</code>인가?</h3><p><a href="https://www.notion.so/c1ecb7cc52d446cc93d928d172ef8442" rel="external nofollow noopener noreferrer" target="_blank">어노테이션가이드라인</a>에도 나와있듯이 우리는 크게 편견과 혐오라는 두 가지aspect에 대해서 label을 수집했다. 처음에는<code>성에 관련된 편견 및 혐오</code>와<code>그 외의 편견 및 혐오</code>로 나누었는데, 이보다는<code>편견</code>과 <code>혐오</code>로 구분하는 것이 낫다는 판단을했다.</p><p>가이드라인 작성을 위해 댓글을 직접 태깅하다 보니 편견만 존재하는댓글과 혐오만 존재하는 댓글이 존재했다. 항상 혐오가 편견으로부터시작되지는 않았고, 편견이 있음을 부끄러워하지 않고 세상의 진리인 것처럼이야기하는 댓글이 보였다. 혐오가 편견으로 시작된 경우는, 무리하게 개인의특성을 집단의 특성으로 확장해서 그 집단에 대한 혐오를 개인에게 표출할때였다. 그래서 이 둘의 관계를 데이터로 파악할 수 있도록, 또 편견과혐오를 구분지어 생각할 수 있도록 <code>편견</code>에 관련된 label과<code>혐오</code>에 관련된 label을 구분짓기로 했다.</p><p>언어학에 관심있는 사람들이라면 label을 바탕으로 댓글을 분석하는것으로도 재밌는 연구가 될 것 같다.</p><h3 id="표현의-자유와-혐오의-경계">표현의 자유와 혐오의 경계</h3><p>이 둘을 구분짓는 좋은 threshold를 결정하는 것은 무슨 목적으로활용하냐에 달려있다. 우리의 목적은 혐오 댓글의 피해자가 줄어들기를바라는 것이었으므로 익명인의 표현의 자유보다는 기사의 대상이 되는 사람의기분을 좀 더 신경쓰기로 했다. 그래서 태깅을 할 때에 당사자의 입장에서생각하도록 가이드했다.</p><h3 id="어노테이션이-어려웠던-댓글">어노테이션이 어려웠던 댓글</h3><blockquote><p>예전에 데뷔작에서 수영복입고 수중씬 기억난다 진짜 섹씨했는데</p></blockquote><p>연예인이라는 직업이 가지는 특수성 때문에 판단하기 어려웠던 경우이다.특히 여자연예인에 대해서는 외모에 대해 품평하는 댓글이 많았는데,스스로가 연예인이었던 적이 없으니 감정이입을 해서 이를 모욕이라고봐야할지도 모르겠고, 만약 의도적으로 외모를 부각해서 유명세를 얻은경우라면 모욕이라고 보기가 더 어렵다고 생각했다. 결국 각자의 판단에맡겨서 majority voting을 했지만 정말 어려웠던 케이스였다.</p><blockquote><p>신천지?</p></blockquote><p>"일반적으로 비난받을만한 행위로 인한 혐오는 어떻게 판단해야할까?" 를고민하게 만든 댓글이었다. 신천지 교도로 인해 코로나가 빠르게 퍼졌던 사건이후로 "신천지"는 부정적인 이미지로 굳어져 버렸는데, 이 맥락을 고려해서위의 댓글을 <code>혐오</code>라고 태깅하면 "신천지"라는 가치 중립적인단어가 <code>혐오</code>로 태깅되기에 굉장히 고민이 많았습니다.</p><blockquote><p>살빠진 마닷같애</p></blockquote><p>위와 비슷한 케이스로 이 댓글 또한 판단하기 무척이나 어려웠다 ㅠㅠ<code>offensive</code>로 판단하자니 마닷은 뭐가 되냐는...</p><h3 id="other-biases-라는-label"><code>Other biases</code> 라는label</h3><p>현재는 <code>bias</code> label이 <code>gender bias</code>,<code>other biases</code>, <code>none</code> 의 세가지로 구성되어 있다.이렇게 할 수 밖에 없었던 가장 큰 이유는 예산 문제였다 ㅠㅠ 돈이 많았다면gender 외에도 정치, 지역, 인종 등에 대한 편견도 label을 수집할 수있었을텐데 하는 아쉬움이... 인당 150만원 이상은 부담하고 싶지 않아서,그리고 연예 도메인은 성 편견이 가장 많은 비중을 차지하고 있어서 이런결정을 하게 되었다.</p><p>그러다보니 <code>others</code> 라는 label 은 온갖 종류의 편견이 모두모아져 있다. 아마도 모델이 곧장 others 를 예측하는 것은 쉽지 않을것이라고 생각한다. 이 task는, <a href="https://arxiv.org/abs/2005.12503" rel="external nofollow noopener noreferrer" target="_blank">논문</a>에 적혀있듯이, 2-stepclassification 문제를 푸는 방식이 낫지 않을까라고 생각한다. 먼저<code>gender / no-gender</code> 를 예측하고, 그 이후에<code>bias 유 / 무</code> 를 예측하면 <code>gender</code>,<code>others</code>, <code>none</code> 을 좀 더 쉽게 예측할 수 있을것이라고 생각한다.</p><h3 id="어노테이션-작업-시-context-미제공">어노테이션 작업 시 context미제공</h3><p>댓글에 포함된 편견 및 혐오를 더욱 정확하게 판단하기 위해서는 댓글이작성된 뉴스 기사에 대한 정보가 필요하다. 하지만 현실적인 이유들로포기했었다.</p><blockquote><p>"작업자가 기사를 읽어야 하는 번거로움을 감수할까?" <br> "태깅플랫폼에서 이 기능을 제공해줄까?" <br> "뉴스 기사의 내용에 대한 저작권은우리에게 없기 때문에 공개 데이터셋에 포함할 수 없고, 그럴거라면 태깅을컨텍스트 없이 하는게 좋지 않을까?"</p></blockquote><p>등의 질문들에 대해 명쾌한 답변을 내리지 못했고, 결국 댓글의 내용만보고 판단하는 방식을 가져갔다. 지나고나니 아쉬움이 남는 건 어쩔 수없는듯하다.</p><hr><h2 id="testset-구성">Testset 구성</h2><p>현재 testset은 함께 작업했던 저와 <a href="https://www.facebook.com/warnik.chow?__tn__=K-R&amp;eid=ARCNVgdXVouckswETyWV9lkDr_cQtrkWPysMCRo0j12ERGUOBQc35o_roiDziJvD-AI5QCjiPW9EQsqA&amp;fref=mentions&amp;__xts__%5B0%5D=68.ARBryv_ZUhMCc7-7G69xtrgWU5FmnqoTL_lLX8bkOVrEnrZ2TRtHphUFuujbvKft8qDDUco0ZJHr7AF9qnijRGiz1J7DMHiWCEXVK61XNr9g40o-7TImObl04dnqssnBZr1-Msp6i8aN8PFC2L9jDTvuS5DtI6w4tTkyVz4KvHHrjO-_oUPHzg6yhuxC7A8v2KWbj2wxuNs52cgI" rel="external nofollow noopener noreferrer" target="_blank">조원익</a>,<a href="https://www.facebook.com/Junbum.L?__tn__=K-R&amp;eid=ARAQ7IJ8TMPDz9gEkoGREztDVOhQ35RZDphJGLftwahcbbh0jQfEyAdsaWSGsuVPrtfxpv1Twpuw3vgF&amp;fref=mentions&amp;__xts__%5B0%5D=68.ARBryv_ZUhMCc7-7G69xtrgWU5FmnqoTL_lLX8bkOVrEnrZ2TRtHphUFuujbvKft8qDDUco0ZJHr7AF9qnijRGiz1J7DMHiWCEXVK61XNr9g40o-7TImObl04dnqssnBZr1-Msp6i8aN8PFC2L9jDTvuS5DtI6w4tTkyVz4KvHHrjO-_oUPHzg6yhuxC7A8v2KWbj2wxuNs52cgI" rel="external nofollow noopener noreferrer" target="_blank">이준범</a>이직접 작업한 라벨이 달려있다. 우리의 의도와 부합하는, 가장 어노테이션이잘 되었다고 보장할 수 있는 데이터셋이라고 볼 수 있다. 하지만 지나고나니"시간 순으로 train, validation, testset을 구성했다면 어땠을까?" 하는아쉬움이 남았다.</p><p>댓글에는 많은 사회적 배경지식이 녹아져있다. 특히 인물의 이름이 가지고있는 정보가 있는데, 우리가 수집한 기간에는 승리와 정준영 등의 연예인이얽혀있던 단톡방 사건이 포함되어 있었다. 그래서 "승리"가 포함된 댓글은부정적인 맥락 속에서 판단되었다.</p><p>예를 들어 "승리가 뭘 잘못했다고 난리들인지...그냥 승리 부럽고베알꼴린 애들이 화난거로밖에 안보임ㅎ" 라는 댓글에서 "승리"를 제거하면성편견이 없는 것으로 태깅되었겠지만, "승리"가 포함되었기 때문에 성편견이존재하는 것으로 태깅된다.</p><p>Generalization을 잘 하는 모델이 진짜 잘하는 모델이라고 했을 때, 학습데이터에 "승리"가 없어도 위의 댓글에 달린 라벨을 잘 예측할 수 있는모델을 판별할 수 있게 testset을 구성했다면 더 좋았을 것 같다.</p><hr><h2 id="kobert-tokenization">KoBERT tokenization</h2><p><img src="/assets/images/korean-hate-speech-model-result.png?style=centerme" width="50%"><br></p><p>baseline으로 CharCNN, BiLSTM, BERT를 사용한 모델의 결과를논문<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://arxiv.org/abs/2005.12503&gt;">[2]</span></a></sup>에첨부했다. 여러 task 모두 BERT가 가장 좋은 성능을 보였다.</p><p>댓글은 맞춤법을 지키는 문장과는 거리가 멀고, 줄임말, 신조어,연예인명, 그리고 그 외의 고유명사 등이 많이 등장한다. 그래서 BERTtokenization 결과를 보면 한 글자 씩 분리되는 경우가 빈번했다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">페지해라 가세연. 페지가 답이다 아님말고식 증거도없이 유재석 언급하노</span><br><span class="line">[&#39;▁페&#39;, &#39;지&#39;, &#39;해&#39;, &#39;라&#39;, &#39;▁&#39;, &#39;가&#39;, &#39;세&#39;, &#39;연&#39;, &#39;.&#39;, &#39;▁페&#39;, &#39;지가&#39;, &#39;▁답&#39;, &#39;이다&#39;, &#39;▁아&#39;, &#39;님&#39;, &#39;말&#39;, &#39;고&#39;, &#39;식&#39;, &#39;▁증거&#39;, &#39;도&#39;, &#39;없이&#39;, &#39;▁유재석&#39;, &#39;▁언급&#39;, &#39;하&#39;, &#39;노&#39;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">god 박준형이 이 기사를 싫어합니다.</span><br><span class="line">&#96;[&#39;▁&#39;, &#39;go&#39;, &#39;d&#39;, &#39;▁박&#39;, &#39;준&#39;, &#39;형&#39;, &#39;이&#39;, &#39;▁이&#39;, &#39;▁기사&#39;, &#39;를&#39;, &#39;▁싫어&#39;, &#39;합니다&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure><p>koBERT 학습 데이터에 자주 등장했던 연예인 이름은 원본 그대로 보존되는반면, 그렇지 못한 연예인은 이름이 쪼개진다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">조개갖고 개ㅈㄹ하는 태콩이나 개촐싹대는조샌징들이나 ㅈㄴ웃김ㅋㅋㅋ </span><br><span class="line">[&#39;▁O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;▁조&#39;, &#39;개&#39;, &#39;갖&#39;, &#39;고&#39;, &#39;▁개&#39;, &#39;ᄌᄅ&#39;, &#39;하는&#39;, &#39;▁태&#39;, &#39;콩&#39;, &#39;이나&#39;, &#39;▁개&#39;, &#39;촐&#39;, &#39;싹&#39;, &#39;대&#39;, &#39;는&#39;, &#39;조&#39;, &#39;샌&#39;, &#39;징&#39;, &#39;들이&#39;, &#39;나&#39;, &#39;▁&#39;, &#39;ᄌᄂ&#39;, &#39;웃&#39;, &#39;김&#39;, &#39;ᄏ&#39;, &#39;ᄏ&#39;, &#39;ᄏ&#39;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ㅅㅂ 더럽게 메갈어로 제목뽑는거 봐라</span><br><span class="line">[&#39;▁&#39;, &#39;ᄉᄇ&#39;, &#39;▁더&#39;, &#39;럽&#39;, &#39;게&#39;, &#39;▁메&#39;, &#39;갈&#39;, &#39;어&#39;, &#39;로&#39;, &#39;▁제&#39;, &#39;목&#39;, &#39;뽑&#39;, &#39;는&#39;, &#39;거&#39;, &#39;▁봐&#39;, &#39;라&#39;]</span><br></pre></td></tr></table></figure><p>"ㅈㄴ", "ㅈㄹ", "ㅅㅂ" 같은 단어가 tokenization에서는 쪼개지지않는다.</p><hr><p>어려웠던 작업이었고, 완벽했다고는 할 수 없지만 좋은 시작점이 될 수있는 프로젝트였다고는 생각한다. 이번에 해결할 수 없었던 여러 한계점들을극복하는 다른 좋은 결과들이 많이 나올 수 있길 :)</p><p>이제 진짜 끝!</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kocohub/korean-hate-speech" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/kocohub/korean-hate-speech</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/2005.12503" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/abs/2005.12503</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;약 4-5개월동안 사이드로 진행했던 혐오 댓글
프로젝트&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot; aria-label=&quot;&amp;lt;https://github.com/kocohub/korean-hate-speech&amp;gt;
&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;가 성공적으로 마무리되었다. 같은 문제의식을 가진
사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이
무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는
활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고
원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의
상호보완적인 역량 덕분이 아니었을까 싶다.&lt;/p&gt;
&lt;p&gt;사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한
이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작
작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던
부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는
사람들에게도 좋은 팁이 되지 않을까?
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/categories/ML/NLP/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="dataset" scheme="https://inmoonlight.github.io/tags/dataset/"/>
    
      <category term="news comments" scheme="https://inmoonlight.github.io/tags/news-comments/"/>
    
      <category term="social good" scheme="https://inmoonlight.github.io/tags/social-good/"/>
    
      <category term="hate speech" scheme="https://inmoonlight.github.io/tags/hate-speech/"/>
    
  </entry>
  
  <entry>
    <title>Attention in NLP</title>
    <link href="https://inmoonlight.github.io/2020/01/27/Attention-in-NLP/"/>
    <id>https://inmoonlight.github.io/2020/01/27/Attention-in-NLP/</id>
    <published>2020-01-27T12:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.144Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>You can't cram the meaning of a whole %&amp;!# sentence into a single&amp;!#* vector!</p><footer><strong>Raymond Mooney</strong></footer></blockquote><p>Attention은 single vector에 한 문장의 의미를 완벽하게 담을 수 없기때문에 필요한 순간에, 필요한 정보를 사용하기 위한 방법이다. 기본적으로<strong>query</strong> vector와 <strong>key</strong> vector의 조합으로attention weight가 계산된다. 여기서 "조합"의 방법에는 크게 두가지가있다. 하나는 Additive Attention으로 query vector와 key vector에feed-forward network를 적용한 것이고, 다른 하나는 Dot-ProductAttention으로 문자그대로 query vector와 key vector의 dot-product를이용한 것이다. 이번 글에서는 각 Attention 방법들과 이들의 장단점을소개하려고 한다.</p><a id="more"></a><h2 id="additive-attention">Additive Attention</h2><p>Additive Attention은 query vector와 key vector의 조합으로 attention값을 얻을 때 단일 hidden layer를 가진 feed-forward network를 이용한다.query vector (<span class="math inline">q</span>) 와 key vector (<span class="math inline">k</span>) 가 같은 dimension을 가질 필요가 없으며,dimension의 크기에 상관없이 좋은 성능을 보인다는 장점이 있다.</p><h3 id="bilinear2">Bilinear<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Effective Approaches to Attention-based Neural Machine Translation](https://www.aclweb.org/anthology/D15-1166.pdf)">[2]</span></a></sup></h3><p>\[a(q, k)= q^{T} Wk\]</p><p>\(q\) 와 \(k\) 에 \(W\) 를 곱하는 방법이다. \(q\) 를 linear transform시킨 후, \(k\) 와 dot-product를 한 것과 같다.</p><h3 id="multi-layer-perceptron3">Multi-layerPerceptron<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)">[3]</span></a></sup></h3><p>\[a(\boldsymbol{q}, \boldsymbol{k})=\boldsymbol{w}_{2}^{\top} \tanh\left(W_{1}[\boldsymbol{q} ; \boldsymbol{k}]\right)\]</p><p>\(q\) 와 \(k\) 를 concat시킨 후 single hidden layer와 activation함수로 tanh를 사용한 feed-forward network를 사용하였다. 이 방법은 익숙할것이다. 왜냐하면 <a href="https://arxiv.org/abs/1409.0473" rel="external nofollow noopener noreferrer" target="_blank">NeuralMachine Translation by Jointly Learning to Align and Translate</a>에서이 attention을 이용했기 때문이다. 아래의 오른쪽 수식에서 \(a\)는alignment model로 위에서 언급한 feed-forward network와 같은 역할을한다.</p><p><img src="/assets/images/seq2seq_attention.png?style=centerme" width="55%"></p><h2 id="dot-product-attention">Dot-Product Attention</h2><p>Dot-Product attention은 \(^{} \)을 기반으로 attention weight를 구하는방법이다. Additive attention과는 달리 hidden layer를 곱하는 과정이추가되지 않아서 연산 속도와 space 측면에서 효율적이다. 하지만 반드시\(q\)와 \(k\)의 dimension이 같아야 한다는 제약조건이 있으며, dimension이클 때 학습에 방해가 된다는 단점이 있다.</p><h3 id="dot-product">Dot-Product</h3><p>\[a(\boldsymbol{q}, \boldsymbol{k})=\boldsymbol{q}^{\top}\boldsymbol{k}\]</p><p>\(q\)와 \(k\)의 elment-wise product의 합이다. 이 연산의 특성 상,반드시 dimension이 같아야 한다.</p><p>만약 \(q\)와 \(k\)의 각 요소가 독립적이고 평균이 0, 분산이 1 인분포의 random variable이라면, \({q}^{\top}{k}\) 는 평균이 0이고 분산이dimension의 크기인 분포를 따른다. 분산이 증가되면 dot-product에softmax를 취했을 때 어떤 값은 굉장히 크지만 대부분의 값은 굉장히 작게만든다. 작은 값들은 back-propagation 시 gradient도 작기 때문에전체적으로 학습이 잘 되지 않게 만든다. 따라서 dimension이 큰 경우 성능이좋지 않다.</p><h3 id="scaled-dot-product4">ScaledDot-Product<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Attention is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)">[4]</span></a></sup></h3><p>\[a(\boldsymbol{q}, \boldsymbol{k})=\frac{\boldsymbol{q}^{\top}\boldsymbol{k}}{\sqrt{|\boldsymbol{d}|}}\]</p><p>Scaled Dot-Product는 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="external nofollow noopener noreferrer" target="_blank">Attentionis All You Need</a> 에서 처음으로 소개된 방법이다. Dot-Product의dimension이 클 때 학습이 잘 되지 않는 단점을 극복하게 위해 dot-product결과를 \(q\)의 dimension \(d\) (\(k\) 의 dimension 이기도 하다) 의 root값으로나누어주었다.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="root로 나누어 준 이유는 scaled dot-product의 결과를 평균이 0, 분산이 1 인 분포로 만들기 위해서다.">[5]</span></a></sup></p><h2 id="conclusions">Conclusions</h2><p>Additive attention은 dimension에 상관없이 좋은 결과를 내고,attention을 계산하는 재료인 query vector와 key vector의 dimension에상관없이 사용할 수 있다. 하지만 최근의 NLP trend라고 할 수 있는, 대량의데이터를 굉장히 큰 모델로 학습시키는 방법에는 안그래도 많은 연산량에부담이 되는 방법이다. 그래서 계산의 부담이 적으면서 dimension이 큰경우에 대해서도 좋은 성능을 보이는 scaled dot-product 기반의 attention이잘 쓰이는 편이다.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">전반적인내용은 Graham Neubig의<a href="http://phontron.com/class/nn4nlp2019/assets/slides/nn4nlp-09-attention.pdf" rel="external nofollow noopener noreferrer" target="_blank">NN4NLPAttention 강의</a>를 참고했습니다.<a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/D15-1166.pdf" rel="external nofollow noopener noreferrer" target="_blank">EffectiveApproaches to Attention-based Neural MachineTranslation</a><a href="#fnref:2" rev="footnote"> ↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1409.0473" rel="external nofollow noopener noreferrer" target="_blank">NeuralMachine Translation by Jointly Learning to Align andTranslate</a><a href="#fnref:3" rev="footnote"> ↩︎</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="external nofollow noopener noreferrer" target="_blank">Attentionis All You Need</a><a href="#fnref:4" rev="footnote"> ↩︎</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">root로나누어 준 이유는 scaled dot-product의 결과를 평균이 0, 분산이 1 인분포로 만들기 위해서다.<a href="#fnref:5" rev="footnote"> ↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;You can&#39;t cram the meaning of a whole %&amp;amp;!# sentence into a single
&amp;amp;!#* vector!&lt;/p&gt;
&lt;footer&gt;&lt;strong&gt;Raymond Mooney&lt;/strong&gt;&lt;/footer&gt;&lt;/blockquote&gt;
&lt;p&gt;Attention은 single vector에 한 문장의 의미를 완벽하게 담을 수 없기
때문에 필요한 순간에, 필요한 정보를 사용하기 위한 방법이다. 기본적으로
&lt;strong&gt;query&lt;/strong&gt; vector와 &lt;strong&gt;key&lt;/strong&gt; vector의 조합으로
attention weight가 계산된다. 여기서 &quot;조합&quot;의 방법에는 크게 두가지가
있다. 하나는 Additive Attention으로 query vector와 key vector에
feed-forward network를 적용한 것이고, 다른 하나는 Dot-Product
Attention으로 문자그대로 query vector와 key vector의 dot-product를
이용한 것이다. 이번 글에서는 각 Attention 방법들과 이들의 장단점을
소개하려고 한다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/categories/ML/NLP/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Positional Encoding in NLP</title>
    <link href="https://inmoonlight.github.io/2020/01/26/Positional-Encoding/"/>
    <id>https://inmoonlight.github.io/2020/01/26/Positional-Encoding/</id>
    <published>2020-01-26T08:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게sequential information을 얻지 못하는 경우에 대해 정보를 강제하는방식이다. 보통 sequential data를 Recurrent Neural Network (RNN) 외의다른 모델로 다루고 싶을 때 많이 사용된다. 이번 글에서는 ConvolutionalNeural Network (CNN), End-to-End Memory Network (MemN2N),Transformer에서 sentence embedding을 위해 사용된 positional encoding에대해 소개하려고 한다. <a id="more"></a></p><h2 id="introduction">Introduction</h2><p>일반적으로 NLP 모델은 각 문장을 구성하는 token을 one-hot vector가아닌 distributed vector로 표현한다. 그 이유는 distributedrepresentation이 1) 비슷한 의미지만 다른 lexical form을 가진 token을 더잘 표현할 수 있기 때문이고, 2) embedding dimension을 감소시킬 수 있기때문이다.</p><p>문장의 embedding은 문장을 이루는 각 token의 embedding을 조합하는방식으로 얻어진다. 이 때 position에 대한 정보가 없다면 모델은<code>handful of chocolate</code>과 <code>chocolate of handful</code>을같은 의미로 인식하게 된다. RNN은 모델 구조 자체에 time information이녹아져 있다. 그래서 <code>handful --&gt; of --&gt; chocolate</code> 의순서가 담긴 sentence embedding을 자연스럽게 얻을 수 있다. 반면 CNN이나Attention 기반의 Transformer는 순서에 대한 정보를 강제해야 하고, 이 때positional encoding이 사용된다.</p><p>Positional encoding (PE) 은 token embedding vector에 곱해지는 정보로,sentence에서 해당 token이 어디에 위치해 있는지를 나타낸다. J개의 token\(t_j \in \mathbb{R}^d\)으로 구성된 sentence \(s = [t_1, t_2, ...,t_J]\)가 있다고 하자. PE \(\in \mathbb{R}^{J \times d}\) 의 row\(j\)마다 다른 값을 가지도록 하여 문장 맨 처음의 <code>handful</code>과맨 뒤의 <code>handful</code>을 다르게 인식하도록 한다.</p><p>PE를 구성하는 방식에는 크게 두 종류가 있다. 하나는 학습기반, 다른하나는 position과 dimension을 입력으로 한 함수를 이용하는 방법이다.</p><h2 id="learned-positional-embeddings">Learned PositionalEmbeddings</h2><p>학습기반의 PE를 구성하는 방식은 Convolutional Sequence to SequenceLearning (ConvS2S)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)">[1]</span></a></sup>에서 사용되었다. 평균 0, 표준편차 0.1을 따르는normal distribution으로 initialize되고 학습을 통해 position 정보를배우길 기대한다.</p><p>PE를 encoder와 decoder 모두에 사용한 경우, encoder에만 사용한 경우,decoder에만 사용한 경우, 아예 사용하지 않은 경우로 나누어 번역 task에실험해보았을 때의 결과는 다음과 같다.</p><p><img src="/assets/images/learned_pe_table.png?style=centerme" width="50%"></p><p>BLEU를 기준으로 분석해보면 encoder에서의 PE역할이 decoder보다 조금 더중요하다. PE를 아예 쓰지 않을 때의 점수가 가장 낮지만 점수 차이를생각해보면 모델 성능에는 크게 영향을 미치지 않는다고 해석해 볼 수있다.</p><p>학습 기반이므로 학습 시 다루지 않았던 길이의 문장이 입력으로 들어온경우, 외삽이 불가능하다는 단점이있다.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)">[5]</span></a></sup></p><h2 id="function-based-positional-encoding">Function-based PositionalEncoding</h2><p>함수 기반의 PE는 문장에서 몇 번째에 위치한 토큰인지, 토큰의 embeddingdimension이 무엇인지를 정해주면 값이 정해진다. 이 때, 다른 위치의 정보가같은 값으로 mapping되지 않아야 한다. 어떻게 구현할 수 있을까?</p><h3 id="the-intuition">The Intuition</h3><p>0부터 15까지의 숫자를 2진법으로 나타내보자.</p><p><img src="/assets/images/PE_intuition.png?style=centerme" width="55%"></p><p>다른 색으로 구분지어 표현한 2진수의 자리수마다 다른 주기를 가지는것을 볼 수 있다. 붉은색은 주기가 1이고, 노란색은 주기가 2, 초록색은주기가 4, 파란색은 주기가 8이다.</p><p>위 예시에서의 자리수를 embedding dimension이라고 생각해보면 PE에도같은 원리를 확장시켜볼 수 있다.</p><h3 id="in-memn2n">In MemN2N</h3><p>End-to-End Memory Network(MemN2N)<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb](https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb)">[4]</span></a></sup>에서는 아래의 함수를 사용했다.</p><p>\[PE_{k j}=(1- \frac{j}{J})-\frac{k}{d}(1- \frac{2j}{J})\]</p><p>\[j \in {1, ..., J}\]</p><p>\[k \in {1, ..., D}\]</p><p>임의의 문장 <em>"The same representation is used for questions,memory inputs and memory outputs."</em>에 적용되는 PE를 시각화해보면다음과 같다.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)">[5]</span></a></sup></p><p><img src="/assets/images/PE_example_1.png?style=centerme" width="90%"></p><p>여기서는 dimension에 관계없이 같은 주기를 가지지만 시작값이 전부다르다. 결과적으로는 position마다 다른 vector를 곱하게 되어 position정보를 전달할 수 있다.</p><p>다른 문장 길이를 가지는 경우에 대해서 적용해보면 어떨까? 이번에는<em>"We therefore propose a second representation that encodes theposition of words within the sentence."</em>에 대해시각해보았다.<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)">[5]</span></a></sup></p><p><img src="/assets/images/PE_example_2.png?style=centerme" width="90%"></p><p>position이 늘어난만큼 position encoding 값의 변화도가 줄었다. J는문장마다 달라지므로 첫번째, 두번째의 절대적인 위치보다는 각 순서를구분짓기 위한 목적에 치중하였다.</p><p>ConvS2S에서와는 달리 MemN2N에서 PE의 효과는 꽤나 컸던 것으로보인다.</p><p><img src="/assets/images/MemN2N_PE.png?style=centerme" width="90%"></p><h3 id="in-transformer">In Transformer</h3><p>Attention is all youneed<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)">[5]</span></a></sup>에서사용된 PE는 <strong>주기</strong>함수로 유명한 sin 함수와 cos 함수를기반으로 한다. (a.k.a, sinusoidal functions)</p><p>\[ \begin{aligned} P E_{(\text {pos, 2k} )} &amp;=\sin \left(\text{pos} / 10000^{2 k / d}\right) \\ P E_{(\text {pos,2k+1})} &amp;=\cos\left(\text {pos} / 10000^{2 k / d}\right) \end{aligned} \]</p><p>잠시 고등학교 때 배운 수학을 떠올려보자. \(sin(ax + b)\) 의 주기는\(\frac{2\pi}{|a|}\) 이다. 따라서 PE의 특정 position vector 값의 주기는\(2\pi \cdot 10000^{2 k / d}\) 와 같다.</p><p>MemN2N에서의 PE와는 달리, position vector의 주기가 vector의dimension마다 변화한다. 전체 벡터 크기(\(d\))가 128이라고 가정할 때,\(k\)가 작을수록 주기가 짧고 \(k\)가 클수록 주기도 길어진다. (아래 그림참고)</p><p><img src="/assets/images/positional_encoding.png?style=centerme" width="85%" alt="Image credit: https://kazemnejad.com/blog/transformer_architecture_positional_encoding"></p><p>왜 Transformer에서는 MemN2N과 다르게 sinusoidal 함수를 썼을까?논문에서 그 이유를 짧게 기술하고 있다.</p><blockquote><p>We chose this function because we hypothesized it would allow themodel to easily learn to attend by relative positions, since for anyfixed offset \(k\), \(P E_{pos+k}\) can be represented as a linearfunction of \(P E_{pos}\).</p></blockquote><p>sinusodial 함수의 특징을 이용해 첫번째, 두번째마다 같은 position정보를 주면서도 \(n + k\)번째 vector가 \(n\)번째 vector와 관계가 있을 때이를 학습할 수 있는 여지를 남겨주기 위함이다. (참고로 이에 대한 수학적인증명은 <a href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/" rel="external nofollow noopener noreferrer" target="_blank">이article</a>에 기술되어 있다.)</p><p>또한 PE vector 간의 distance는 대칭적이고 거리에 따라 일정한 비율로감소한다. Transformer의 self-attention 연산에서 빛을 발하는특징이다.</p><p><img src="/assets/images/PE_pros_1.png?style=centerme" width="50%" alt="Image credit: https://kazemnejad.com/blog/transformer_architecture_positional_encoding"></p><h2 id="conclusions">Conclusions</h2><p>PE는 크게 학습을 통해 정해질 수 있고 미리 지정한 함수로 정해질 수도있다. 학습을 통한 방식은 학습시 보지 않았던 새로운 길이가 등장했을 때외삽이 불가능하지만 함수 기반의 PE는 가능하다. 함수도 어떤 함수를쓰느냐에 따라 종류가 구분되는데, 절대적인 위치에 따라 같은 값을가지면서도 상대적 위치의 관계도 학습할 수 있는 \(sin\)과 \(cos\) 기반의함수가 가장 좋은 방법이라고 생각된다.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1705.03122" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/abs/1705.03122</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="external nofollow noopener noreferrer" target="_blank">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/1503.08895" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/abs/1503.08895</a><a href="#fnref:3" rev="footnote">↩︎</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/inmoonlight/notebooks/blob/master/notebooks/2020-01-26-MemN2N-Position-Encoding.ipynb</a><a href="#fnref:4" rev="footnote">↩︎</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a><a href="#fnref:5" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Positional encoding 혹은 position encoding은 모델 구조에서 자연스럽게
sequential information을 얻지 못하는 경우에 대해 정보를 강제하는
방식이다. 보통 sequential data를 Recurrent Neural Network (RNN) 외의
다른 모델로 다루고 싶을 때 많이 사용된다. 이번 글에서는 Convolutional
Neural Network (CNN), End-to-End Memory Network (MemN2N),
Transformer에서 sentence embedding을 위해 사용된 positional encoding에
대해 소개하려고 한다.
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/categories/ML/NLP/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>빛의 과거</title>
    <link href="https://inmoonlight.github.io/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung/"/>
    <id>https://inmoonlight.github.io/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung/</id>
    <published>2020-01-07T13:00:00.000Z</published>
    <updated>2023-11-02T02:50:49.148Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>새해를 맞이하자마자 작년의 목표와 기대에는 없던 일이 일어났다.오랜만에 뵌 분께 은희경 작가님의 "빛의 과거"라는 책을 선물받은 것이다.그 동안 읽었던 텍스트라고는 오직 논문이었기에 선물받자마자 들었던 생각은"아, 내가 과연 책을 읽을 수 있을까?" 였다.</p><p>그 와중에 작가님의 성함이 눈에 띄었다. 은희경 작가님... 왜 이렇게익숙한 이름인가 했더니 개인적으로 존경하고 흠모하는 언니로부터추천받았던 작가님이었다는 것이 떠올랐다. 단지 이 작은 이유만으로오랜만에 접하는 소설의 벽이 낮아지는 느낌이었다.</p><p>이 책을 읽을 운명이었던 것인지, 마침 다음 날은 주말이었고 미용실에오랜만에 가기 위해 예약을 잡아두었다. 자리에 앉자마자 책을 펼쳐들었다.그리고 단숨에 그 소설의 세계에 몰입하게 되었다. <a id="more"></a></p><h2 id="내용-소개">내용 소개</h2><p>그래도 공개적인 곳에 쓰는 글이니만큼, 적어도 이 글에 어쩌다 접근하게된 누군가를 위해 줄거리를 적어두어야 할 것 같다.</p><p>주인공 '나'는 2017년 현재, 남편을 사별한 번역일을 하면서 살아가는평범한 주부다. 1977년 대학생 시절 기숙사에서 처음으로 인연을 맺은'친구'와 끊어질듯 끊어지지 않는 관계가 이어지고 그 '친구'와는 그렇게친하지는 않지만 계속 연락을 하며 지낸다. 이 '친구'는 소설가다. 작가가꿈이었던 적은 없었지만 여러 직업을 거쳐 여기에 이르렀다. 그리고 본격적인이야기는 그 친구의 책을 읽는 것으로 시작된다. 책은 함께 기억을 공유하고있는 대학생 시절을 배경으로 한다.</p><h2 id="책의-구성">책의 구성</h2><p>한 편의 영화와 같은 구성이었다. 1977년과 2017년을 오가며 이야기가진행된다. 처음에는 "흔한 구성"이라고 생각했지만 읽어나가다보니 시간을오가지 않으면 불가능했을 이야기였다.</p><p>'2017년의 나'가 있었기 때문에 '1977년의 나'의 어리숙했던 부분을어른의 시점에서 돌이켜 볼 수 있었고, '1977년의 나'가 있었기 때문에어렸을 시절의 나의 시점에서 어리숙했을 수 밖에 없었던 일을 설득력있게전달할 수 있었다.</p><p>그리고 1977년은 지금과는 달리 더 보수적인 시대였다. '1977년의 나'는그 시대를 '여대생'의 시점에서 덤덤하게 이야기한다. '2017년의 나'였다면시니컬한 톤이 묻어날 수 밖에 없지 않았을까?</p><h2 id="마음에-닿았던-구절">마음에 닿았던 구절</h2><blockquote><p>회사의 관례에 따라 여성 기혼자에게 주어지는 계약직 전환 서류를 내책상 위에 갖다 놓은 것도 그녀였다. <br> - p.9</p></blockquote><p>마음에 닿았던 구절이라고 했지, 그 구절이 감동적일 것이라는 이야기는하지 않았다 :P '정말 이랬다고?' 라는 놀라움 때문에 바로 형광펜을들이밀었다. 나는 이런 대우가 당연했었던 시절을 상상조차 할 수 없다.</p><blockquote><p>끊어진 건 아니지만 밀착될 일도 없는, 간격이 불규칙한 점선 같은관계였다. <br> - p.11</p></blockquote><p>'나'와 '친구'의 관계를 이야기하는 문장이다. 대학을 졸업하고사회생활을 하는 사람들이라면 누구나 공감할 수 있는 관계일 것이다. 어떻게이렇게 표현할 수 있을까 싶어서 밑줄을 그었다.</p><blockquote><p>그러나 그녀에게는 사람을 대할 때 미묘한 권력관계를 만드는 습성이있었다. 끊임없이 자신을 중심으로 돌아가는 관계의 자장을 만들어내고우월감과 피해 의식을 번갈아 써가며 그것을 정당화했다. 거기에는 증인이필요했다. 결국 나로 하여금 위성처럼 그녀의 궤도를 따라 돌며 그녀라는일방적이고 변덕스러운 광원을 반사하도록 만들어 버리는 것이다. <br> 나는나대로 소심함과 자기 합리화의 조합인 어정쩡한 온검함 뒤에 숨어 그녀의그런 태도를 순순히 받아들이곤 했다. 열정은 단호한 구석이 있어서 금세꺾이지만 친근함은 어느 정도 안이한 감정이라서 사소한 기억의 공유만으로도쉽게 환기되었다. <br> - p.12</p></blockquote><p>'나'와 '친구'의 관계를 묘사한 내용이다. 둘 사이의 미묘한 권력관계와그 속에서 우월감을 느끼는 친구, 그리고 그 것이 느껴지지만 크게 동요하지않고 순종적인 '나'의 모습이 참으로 세련되게 표현되었다. "자신을 중심으로돌아가는 관계의 자장"과 "소심함과 자기 합리화의 조합인 어정쩡한온건함".</p><blockquote><p>부분적으로나마 모범생 흉내를 내서 그 시스템에 순종했고 그 대가로서울의 한 여자대학에 합격하여 고향과 부모로부터 벗어날 수 있었다. <br> -p.27</p></blockquote><p>정말 나의 이야기였다. 고등학생 시절의 나도 이 책의 화자처럼 소심했다.시스템을 거스를 힘과 배짱이 없었기 때문에 그 안에서 내가 할 수 있는최대한의 반항을 했다. 그 곳에서 벗어나고 시스템을 바꿀 힘을 갖기 위해좋은 대학을 가는 것. 분명 이 것은 1977년의 문장일텐데, 어째서 2010년에도똑같은 모습이었던걸까.</p><blockquote><p>그 때까지 다름이란 걸 전혀 겪어보지 못했냐고 묻는다면 그렇지는 않다.12년간이나 중단 없이 지긋지긋했던 초중등학교 생활 속에서도 타인과 부딪힐기회는 얼마든지 있었다. 그러나 내가 그 시절 겪없던 것은 다름이라기보다수직적인 위계와 시비였다. 그때그때 적용되는 일관성 없는 규율이 있었고,없으면 교사나 반장이나 힘센 애들이 만들었다. 남과 다른 것이 그대로결격사유가 되는 단체 생활에서 내가 누군지 따위를 고민할 기회는아무에게도 주어지지 않았다. <br> -p. 27</p></blockquote><p>이 페이지 속의 많은 문장에서 머물렀다. 나도 책의 주인공처럼고등학교를 탈출하고 대학교에 합격을 하자마자 기숙사 생활을 했다. 6명이서함께 사는 방이었고 한 방은 두 명이 함께 썼다. 합격과 입학이라는 들뜬마음도 잠시, 나와 '다른' 누군가와 시간과 장소를 공유하는 것의 어려움이크게 다가왔었다. 그 때는 이유를 몰랐지만 여기에 서술된 것처럼, 내가고등학교 시절까지 경험했던 삶 속에서 진정한 '다름'에 대해 이해하는시간은 전혀 없었기 때문이었던 것은 아니었을까.</p><blockquote><p>혼자라는 건 어떤 공간을 혼자 차지하는게 아니라 타인의 시선에서 벗어나익명으로 존재하는 시간을 뜻하는 거였다. <br> -p. 84</p></blockquote><p>단 한 번도 혼자 보내는 시간을 이렇게 정의해보지 않았었다. 사람들 속에있으면서도 혼자라고 느끼는 이유는 그 들에게 난 철저히 '익명'의누군가이기 때문이다. 이 글을 읽은 이후, 지하철에 있을 때마다 괜히피식되게 된다.</p><blockquote><p>나는 내 앞의 문을 열지 못하고 번번이 과거의 나로 굴러떨어지곤 했다.<br> -p. 86</p></blockquote><p>왜 자꾸 나의 대학생활이 떠오르는 것일까. 1977년이든, 2011년이든청춘의 고민은 비슷했다. 그리고 여기에 적진 않았지만 행복의 순간도비슷했다. 사람을 둘러싼 정치적, 사회적, 경제적, 기술적 상황은 너무나도달라졌지만 사람은 결국 똑같았다. 나를 행복하게 만드는 선택과 행복을느끼는 방법만 달라졌다.</p><blockquote><p>첫인상 역시 두번째와 크게 다르지 않았다. <br> -p. 287</p></blockquote><p>이 문장은 반드시 두번 읽어야 한다. 무심결에 넘기면 평범한 문장이지만전혀 그렇지 않다. 어떻게 첫인상이 두번째 인상 이후에 느껴질 수 있단말인가?! 이 문장은 첫번째 만남을 기억하지 못한 채로 두번째 만남을첫번째라고 생각했기 때문에 가능했다. 작가님의 재치에 빵- 터졌다.</p><h2 id="개인적인-감상">개인적인 감상</h2><p>작가님의 문체와 삶에서 느끼는 감각에 모두 공명할 수 있었다. 내가느꼈던 "그 감정"을 거부감없이 재치있고 한편으로는 날카로운 문장으로군더더기 없이 표현한다.</p><p>군더더기 없음은 자칫하면 82년생 김지영처럼 어떤 메시지를 품을 수 있는배경의 소설을 담백하게 만든다. 책을 읽으면서 나는 1977년을 관찰하고 있지않았다. 그 시대 속에 머물러 있으며 소설 속의 "나"가 겪어나가는 감정에공감하기 위해 시대적 배경을 이해하려고 했다. 내가 77년대에 부자로살았더라면, 똑똑한 남자였다면, 똑똑한 여자였다면, 소심한 남자였다면,적극적인 여자였다면 어땠을까. 나의 기질과 나의 주어진 환경에 의해2020년, 지금의 나와 다른 모습이었겠지.</p><p>책 속에서 사회가 남자에게 부여하는 모습은 능력있고 가족을 책임지고이끌어나가는 가부장적인 것이었다. 반대로 여성은 자원이 부족하다면 언제든가장 먼저 포기를 강요받은 대상이었고 남자의 내조를 위한 모습을요구받았다. 불현듯 보수적인 할머니와 보수적인 집안에서 가족의 기대를한몸에 받았던 아버지가, 그리고 이 보수적인 집안으로 시집와서 많은 것을포기했던 어머니가 떠올랐다.</p><p>논문과는 다르게 소설은 "그래서 앞으로 무엇을 하면 좋을까?"에 대한생각을 남기기보다는 내가 경험했지만 의식하지 않았던 회색 지대 속의감정과 기억을 재구성하고 새롭게 의미를 붙여준다. 그래서 과거 속으로탐험하는 기분이 들었고 특히나 대학생 시절이 많이 떠올랐다. 2020년은 내가한국나이로 30대가 된 해이기도 하다. 개인적으로는 20대의 나를돌이켜보기에 너무나도 좋았던 책이었다. 그 때 무엇을 어떻게 해야할지몰랐던, 나만의 다름은 인지하지 못하고 다르다는 것이 두렵기만 했던 내가지금은 이렇게 달라졌구나.</p><p>짜식 많이 컸구나. 수고했다.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;새해를 맞이하자마자 작년의 목표와 기대에는 없던 일이 일어났다.
오랜만에 뵌 분께 은희경 작가님의 &quot;빛의 과거&quot;라는 책을 선물받은 것이다.
그 동안 읽었던 텍스트라고는 오직 논문이었기에 선물받자마자 들었던 생각은
&quot;아, 내가 과연 책을 읽을 수 있을까?&quot; 였다.&lt;/p&gt;
&lt;p&gt;그 와중에 작가님의 성함이 눈에 띄었다. 은희경 작가님... 왜 이렇게
익숙한 이름인가 했더니 개인적으로 존경하고 흠모하는 언니로부터
추천받았던 작가님이었다는 것이 떠올랐다. 단지 이 작은 이유만으로
오랜만에 접하는 소설의 벽이 낮아지는 느낌이었다.&lt;/p&gt;
&lt;p&gt;이 책을 읽을 운명이었던 것인지, 마침 다음 날은 주말이었고 미용실에
오랜만에 가기 위해 예약을 잡아두었다. 자리에 앉자마자 책을 펼쳐들었다.
그리고 단숨에 그 소설의 세계에 몰입하게 되었다.
    
    </summary>
    
    
      <category term="Book" scheme="https://inmoonlight.github.io/categories/Book/"/>
    
    
      <category term="essay" scheme="https://inmoonlight.github.io/tags/essay/"/>
    
      <category term="book" scheme="https://inmoonlight.github.io/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>General Language Understanding Evaluation (GLUE) benchmark</title>
    <link href="https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/"/>
    <id>https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/</id>
    <published>2019-12-22T13:22:00.000Z</published>
    <updated>2023-11-02T02:50:49.145Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>General Language Understanding Evaluation benchmark, 줄여서 GLUEbenchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish되었으며, <a href="https://openreview.net/pdf?id=rJ4km2R5t7" rel="external nofollow noopener noreferrer" target="_blank">전자</a>는설명이 상세하고 <a href="https://www.aclweb.org/anthology/W18-5446.pdf" rel="external nofollow noopener noreferrer" target="_blank">후자</a>는 요약되어있다. 이 글은 가장 최근(2019.2.22)에 업데이트된 <a href="https://arxiv.org/pdf/1804.07461.pdf" rel="external nofollow noopener noreferrer" target="_blank">arXiv에 있는 논문</a>을기반으로 작성되었다.</p><a id="more"></a><h2 id="glue-overall">GLUE overall</h2><p>GLUE는 총 9개의 task로 구성되었으며 각 task는 언어의 특정한 성질을평가하기 위한 목적으로 만들어졌고, 최종 점수는 각 task 별 점수의 평균값을 가져간다. task는 크게 3가지 - Single-Sentence Tasks (CoLA, SST-2),Similarity and Paraphrase Tasks (MRPC, QQP, STS-B), Inference Tasks(MNLI, RTE, QNLI, WNLI) - 로 구분할 수 있다. 세부 task에 대해 살펴보기전에 전반적인 task의 특징을 아래의 표에 정리했다. 원 논문에 정리되어있는 것을 바탕으로 재구성하였고 직접 다운로드 받은 데이터를 기준으로측정했기 때문에 corpus의 size가 다를 수 있다.</p><table><colgroup><col style="width: 37%"><col style="width: 7%"><col style="width: 5%"><col style="width: 6%"><col style="width: 8%"><col style="width: 2%"><col style="width: 4%"><col style="width: 27%"></colgroup><thead><tr class="header"><th>data</th><th>train</th><th>dev</th><th>test</th><th>domain</th><th>input</th><th>task</th><th>metrics</th></tr></thead><tbody><tr class="odd"><td><a href="#corpus-of-linguistic-acceptability-cola1">Corpus ofLinguistic Acceptability (CoLA)</a></td><td>8.5k</td><td>1.0k</td><td>1.2k</td><td>linguistics literature</td><td>single-sentence</td><td>- grammatical acceptability <br> - binary classification <br>(grammatical / ungrammatical)</td><td>Matthews correlation</td></tr><tr class="even"><td><a href="#stanford-sentiment-treebank-sst-2">Stanford SentimentTreebank (SST-2)</a></td><td>67k</td><td>872</td><td>1.8k</td><td>movie reviews</td><td>single-sentence</td><td>- sentiment <br> - binary classification <br> (positive /negative)</td><td>acc.</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td><a href="#microsoft-research-paraphrase-corpus-mrpc">MicrosoftResearch Paraphrase Corpus (MRPC)</a></td><td>3.7k</td><td>408</td><td>1.7k</td><td>news</td><td>two sentences</td><td>paraphrase</td><td>acc./F1</td></tr><tr class="odd"><td><a href="#quora-question-pairs-qqp2">Quora Question Pairs(QQP)</a></td><td>364k</td><td>40k</td><td>391k</td><td>social QA questions</td><td>two sentences</td><td>paraphrase</td><td>acc./F1</td></tr><tr class="even"><td><a href="#semantic-textual-similarity-benchmark-sts-b">SemanticTextual Similarity Benchmark (STS-B)</a></td><td>5.8k</td><td>1.5k</td><td>1.4k</td><td>news <br> caption <br> forum</td><td>two sentences</td><td>- sentence similarity <br> - regression</td><td>Pearson / Spearman corr.</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td><a href="#multi-genre-nli-corpus-mnli">Multi-Genre NLI corpus(MNLI)</a></td><td>393k</td><td>20k</td><td>20k</td><td>fiction <br> face-to-face <br> government <br> letters <br> 9/11<br> oxford university press (oup) <br> slate<br> telephone <br> travel<br> verbatim</td><td>two sentences</td><td>ternary classification <br> (entailment / contradiction /neutral)</td><td>matched acc. / mismatched acc.</td></tr><tr class="odd"><td><a href="#the-recognizing-textual-entailment-rte">The RecognizingTextual Entailment (RTE)</a></td><td>2.5k</td><td>276</td><td>3.0k</td><td>news <br> wikipedia</td><td>two sentences</td><td>binary classification <br> (entailment / not_entailment)</td><td>acc.</td></tr><tr class="even"><td><a href="#the-stanford-question-answering-nli-qnli">The StanfordQuestion Answering NLI (QNLI)</a></td><td>105k</td><td>5.5k</td><td>5.5k</td><td>wikipedia</td><td>two sentences (question, sentence)</td><td>binary classification <br> (entailment / not_entailment)</td><td>acc.</td></tr><tr class="odd"><td><a href="#the-winograd-schema-challenge-nli-wnli">The WinogradSchema Challenge NLI (WNLI)</a></td><td>634</td><td>71</td><td>146</td><td>fiction books</td><td>two sentences</td><td>binary classification <br> (entailment / not_entailment)</td><td>acc.</td></tr></tbody></table><h2 id="corpus-of-linguistic-acceptability-cola1">Corpus of LinguisticAcceptability(CoLA)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://arxiv.org/pdf/1805.12471.pdf&gt;">[1]</span></a></sup></h2><p>CoLA는 공개된 언어학 문헌(publised liguistics literature)에서 추출된약 21k 문장들로 구성되어 있다. 이 문장들은 문법적으로 옳은지, 그른지가표기되어 있다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 They drank the pub dry.</span><br><span class="line">0 * They drank the pub.</span><br></pre></td></tr></table></figure><p>문법적으로 옳고 그름을 판단하는 기준은 다양하다. 아래의 표는 corpus를제작하면서 기준에서 포함된 것들과 제외된 것들을 나타낸다.</p><p><img src="/assets/images/cola_dataset_description.png?style=centerme" width="80%"></p><h3 id="included">Included</h3><p><strong>(a) Morphological Violation</strong>: "should leave" 가올바른 표현이지만 "should leaving"으로 작성되었다. 동사의 형태(verbalinflection)가 맞지 않는 경우에 해당한다. <strong>(b) SyntacticViolation</strong>: "What did Bill buy?" 혹은 "Bill bought potatoes and_" 이 되어야 한다. 통사 구조가 틀린 경우에 해당한다. <strong>(c)Semantic Violation</strong>: 의미적으로 말이 되지 않는 문장에해당한다.</p><h3 id="excluded">Excluded</h3><p><strong>(d) Pragmatic Anomalies</strong>: grammar와 상관없는 외부지식이 필요하므로 제외되었다. <strong>(e) Unavailable Meanings</strong>:문장만보고는 판단이 애매하므로 제외되었다. <strong>(f) PrescriptiveRules</strong>: 사람도 누군가의 가르침없이는 터득하기 어려운 rule이기때문에 제외되었다. <strong>(g) Nonce Words</strong>: "arrivable"과 같이typical word-level NLP 모델의 vocab에는 등장하지 않는 단어가 포함된경우이다. NLP 모델의 scope이 아니라고 판단되어 제외되었다.</p><h3 id="testset-and-metrics">testset and metrics</h3><p>testset은 In-Domain과 Out-of-Domain으로 구성되어 있다. In-Domain은training set이 추출된 source와 같은 source에서, Out-of-Domain은 trainingset이 추출되지 <em>않은</em> source에서 구성되었다. GLUE는 원래 구분된두 testset을 하나로 합쳐 단일 testset을 구축하였고 총 1,160문장이다.</p><p><img src="/assets/images/cola_by_source.png?style=centerme" width="40%"></p><p>이 task의 평가는 unbalanced binary classification task에서 사용되는Matthews correlation으로 한다.</p><h2 id="stanford-sentiment-treebank-sst-2">Stanford Sentiment Treebank(SST-2)</h2><p><code>rottentomatoes.com</code>의 영화 리뷰 corpus로 구성되었으며AMT(Amazon Mechanical Turk)를 통해 리뷰의 sentiment가 labeling 되었다.1은 긍정, 0은 부정을 나타낸다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">that loves its characters and communicates something rather beautiful about human nature 1</span><br><span class="line">on the worst revenge-of-the-nerds clichés the filmmakers could dredge up 0</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-1">testset and metrics</h3><p>일반적인 binary classification 문제로 accuracy를 통해 평가한다.</p><h2 id="microsoft-research-paraphrase-corpus-mrpc">Microsoft ResearchParaphrase Corpus (MRPC)</h2><p>MRPC는 온라인 뉴스에서 추출된 문장들로 구성되었으며 2개의 문장이의미적으로 같은지 다른지를 평가하는 task이다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . </span><br><span class="line">On June 10 , the ship &#39;s owners had published an advertisement on the Internet , offering the explosives for sale .</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">Yucaipa owned Dominick &#39;s before selling the chain to Safeway in 1998 for $ 2.5 billion . </span><br><span class="line">Yucaipa bought Dominick &#39;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-2">testset and metrics</h3><p>testset이 label이 불균등(68% positive, 32% negative)하므로 accuracy와F1 score를 metric으로 한다.</p><h2 id="quora-question-pairs-qqp2">Quora Question Pairs(QQP)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs&gt;">[2]</span></a></sup></h2><p>QQP는 <code>https://www.quora.com/</code>의 질문들로 구성되었으며, 두개의 질문이 의미상 같은지 다른지가 표기되어있다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How do you start a bakery?</span><br><span class="line">How can one start bakery business?</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">What are natural numbers?</span><br><span class="line">What is a least natural number?</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-3">testset and metrics</h3><p>MRPC와 마찬가지로 불균등(37% positive, 63% negative)하므로 accuracy와F1 score가 metric으로 활용된다.</p><h2 id="semantic-textual-similarity-benchmark-sts-b">Semantic TextualSimilarity Benchmark (STS-B)</h2><p>문장의 유사도는 번역, 요약, 문장 생성, QA, 대화 모델링 등등 다양한NLP 분야에서 중요하게 다뤄진다. STS shared task는 모델이 문장들의유사도를 얼마나 잘 파악하는지를 평가하기 위해 등장하였고, 2012년부터2017년까지 매년 개최되었으며 그 때마다 다른 dataset이 사용되었다. 이때문에 각 연도의 데이터셋을 적절히 조합한 common evaluation set으로STS-B가 소개되었다.</p><p>이 전의 task와는 다르게 STS는 regression task이다. humanannotator들은 두 문장의 의미적인 유사도를 1~5점으로 평가하였고 모델은score를 예측해야한다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A plane is taking off.  An air plane is taking off.     5.000</span><br><span class="line">Three men are playing chess.    Two men are playing chess.      2.600</span><br><span class="line">A man is smoking.       A man is skating.       0.500</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-4">testset and metrics</h3><p>Regression task이므로 human label과의 Pearson correlation으로평가된다.</p><h2 id="multi-genre-nli-corpus-mnli">Multi-Genre NLI corpus (MNLI)</h2><p>MNLI<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/N18-1101.pdf&gt;">[3]</span></a></sup>는SNLI(Stanford NLI) dataset의 단점을 개선시킨 데이터셋이다. SNLI는 imagecaption으로만 구성되었기 때문에 장면을 표현하는 짧고 간단한 문장이 많고NLU(Natural Language Understanding) task와 무관한 단어들이 많이등장한다. 그래서 NLU task의 benchmark로 사용되기는 어렵기 때문에 다양한도메인(논문에서는 genre라고 표현)의 조합인 MNLI benchmark dataset이등장하였다.</p><p><img src="/assets/images/mnli_dataset_description.png?style=centerme" width="80%"></p><p>위의 표에서 나와있듯이 MNLI는 총 10개의 Genre로 구성되었다. Fiction을제외한 9개의 domain은 Open American National Corpus에서 추출되었고Fiction은 fiction literature에서 가져왔으며 mystery, humor, sci-fi 등 그안에서도 다양한 장르로 구성되었다.</p><blockquote><p>OANC data constitutes the following nine genres: transcriptions fromthe Charlotte Narrative and Conversation Collection of two-sided,in-person conversations that took place in the early 2000s(FACE-TO-FACE); reports, speeches, letters, and press releases frompublic domain government websites (GOVERNMENT); letters from the IndianaCenter for Intercultural Communication of Philanthropic FundraisingDiscourse written in the late 1990s–early 2000s (LETTERS); the publicreport from the National Commission on Terrorist Attacks Upon the UnitedStates released on July 22, 2004 2 (9/11); five non-fiction works on thetextile industry and child development published by the OxfordUniversity Press (OUP); popular culture articles from the archives ofSlate Magazine (SLATE) written between 1996–2000; transcriptions fromUniversity of Pennsylvania’s Linguistic Data Consortium Switchboardcorpus of two-sided, telephone conversations that took place in 1990 or1991 (TELEPHONE); travel guides published by Berlitz Publishing in theearly 2000s (TRAVEL); and short posts about linguistics fornon-specialists from the Verbatim archives written between 1990 and 1996(VERBATIM).</p></blockquote><blockquote><p>For our tenth genre, FICTION, we compile several freely availableworks of contemporary fiction written between 1912 and 2010, spanningvarious genres, including mystery (The Mysterious Affair at Styles, 3Christie, 1921; The Secret Adversary, 4 Christie, 1922; Murder in theGun Room, 5 Piper, 1953), humor (Password Incorrect, 6 Name, 2008),western (Rebel Spurs, 7 Norton, 1962), science fiction (Seven Swords, 8Shea, 2008; Living History,9 Essex, 2016; The Sky Is Falling, 10 DelRey, 1973; Youth, 11 Asimov, May 1952), and adventure (Captain Blood, 12Sabatini, 1922).</p></blockquote><p>선별된 문장을 premise로 두고 human annotator들이 premise와 같은결론을 도출하는 문장(entailment), 반대되는 문장(contradiction), 두 경우모두 아닌 문장(neutral)을 생성하고 label을 단다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How do you know? All this is their information again.   </span><br><span class="line">This information belongs to them.      </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">Vrenna and I both fought him and he nearly took us.     </span><br><span class="line">Neither Vrenna nor myself have ever fought him.      </span><br><span class="line">contradiction</span><br><span class="line"></span><br><span class="line">There was nothing like that emotion now.        </span><br><span class="line">There are few emotions that come close.      </span><br><span class="line">neutral</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-5">testset and metrics</h3><p>testset은 CoLA처럼 matched(in-domain)와 mismatched(cross-domain)로구성되었다. mismatched에는 9/11, FACE-TO-FACE, LETTERS, OUP,VERBATIM처럼 training set에는 없는 domain이 포함되어 있다. (위의 표참고) 각각의 경우를 나누어서 accuracy로 평가한다.</p><h2 id="the-recognizing-textual-entailment-rte">The Recognizing TextualEntailment (RTE)</h2><p>RTE도 STS처럼 RTE1부터 RTE7까지의 데이터셋에서 만들어졌다.구체적으로는 RTE1, RTE2, RTE3, RTE5로 구성되었고, 나머지 데이터셋 중RTE4는 공개되지 않아서, RTE6와 7은 NLI task로는 부적합해서 제외했다고한다. 취합하는 과정에서 일부는 세 개의 class, 일부는 두 개의 class로labeling이 되어있어 이를 일괄적으로 두 개의 class(entailment,not_entailment)로 구분지었다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Swansea striker Lee Trundle has negotiated a lucrative image-rights deal with the League One club.      </span><br><span class="line">Lee Trundle is in business with the League One club.    </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">No Weapons of Mass Destruction Found in Iraq Yet.       </span><br><span class="line">Weapons of Mass Destruction Found in Iraq.      </span><br><span class="line">not_entailment</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-6">testset and metrics</h3><p>일반적인 binary classification 문제이므로 accuracy로 측정한다.</p><h2 id="the-stanford-question-answering-nli-qnli">The Stanford QuestionAnswering NLI (QNLI)</h2><p>Stanford에서 구축한 Machine Comprehension 목적의 QA Dataset, a.k.aSQuAD,을 NLI task에 맞게 변형한 데이터셋이다. SQuAD는 wikipedia에서paragraph를 가져와서 annotator들이 적절한 질문을 던지는데 이에 대한 답을paragraph 내에 있는 문장, 구, 단어로 답할 수 있게 구성되었다. QNLI는질문과 paragraph 내의 한 문장을 비교하여 이 둘이 entailment되었는지아닌지를 판단하도록 바뀌었다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">What two things does Popper argue Tarski&#39;s theory involves in an evaluation of truth?   </span><br><span class="line">He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.   </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">Who was elected as the Watch Tower Society&#39;s president in January of 1917?      </span><br><span class="line">His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner.       </span><br><span class="line">not_entailment</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-7">testset and metrics</h3><p>일반적인 binary classification 문제이므로 accuracy로 측정한다.</p><h3 id="the-winograd-schema-challenge-nli-wnli">The Winograd SchemaChallenge NLI (WNLI)</h3><p>이 데이터셋도 entailment를 평가하는 목적으로 만들어졌다. originalsentence와 이 문장에서 대명사를 일반명사로 치환한 문장 사이의entailment가 있는지 없는지가 label로 달려있다. 아래 예시의 첫 번째문장에서 "it" had a hole의 it이 "The carrot"으로 바뀐 문장이 두 번째문장이고 이 두 문장의 관계가 entailment 되어 있으므로 label 1이달린다.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">I stuck a pin through a carrot. When I pulled the pin out, it had a hole.       </span><br><span class="line">The carrot had a hole.  </span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">John was jogging through the park when he saw a man juggling watermelons. He was very impressive.       </span><br><span class="line">John was very impressive.       </span><br><span class="line">0</span><br></pre></td></tr></table></figure><h3 id="testset-and-metrics-8">testset and metrics</h3><p><a href="https://gluebenchmark.com/faq" rel="external nofollow noopener noreferrer" target="_blank">GLUE FAQ</a>의 12번 문항에는WNLI에서 이상한 결과를 얻을 수 있는 이유가 적혀있다. 같은 문장이 포함된다른 example 끼리는 반대의 label이 달려있는데 이 때문에 training set에overfit된 모델은 dev set에서 성능이 매우 나쁠 수 있다는 것이다. 실제로<a href="https://arxiv.org/pdf/1810.04805.pdf" rel="external nofollow noopener noreferrer" target="_blank">BERT</a>는 이 이유로WNLI의 성능은 report 하지 않았다.</p><h2 id="download">Download</h2><p><a href="https://github.com/nyu-mll/jiant/blob/master/scripts/download_glue_data.py" rel="external nofollow noopener noreferrer" target="_blank">링크</a>에있는 python script를 다운로드한 이후 실행시키면 된다. 지정한 dir에 전체task를 받을 수도 있고 일부 task만 받을 수도 있다.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python download_glue_data.py --data_dir data --tasks all</span><br></pre></td></tr></table></figure><h2 id="leaderboard">Leaderboard</h2><p>여태까지 제출한 모델의 성능은 <a href="https://gluebenchmark.com/leaderboard" rel="external nofollow noopener noreferrer" target="_blank">GLUE leaderboard</a>에정리되어있다.</p><p><img src="/assets/images/glue_leaderboard.png" alt="2019.12.22 기준 top 3"></p><p>Leaderboard에는 순기능과 역기능이 모두 공존하지만, 아직까지는순기능이 더 많다고 생각한다. 상대적으로 공정하게 비교할 수 있는데이터셋이고 덕분에 다양한 Language Model이 주목받을 수 있었기 때문이다.너무 낡아버리기 전에 새로운 데이터셋이 나와야한다고도 생각했는데,Neurips 2019에 "Stickier Benchmark"라는 부제와 함께 <a href="http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems" rel="external nofollow noopener noreferrer" target="_blank">SuperGLUE</a>가등장했다!!</p><p>이로 인해 열릴 새로운 LM들의 등장을 기대해본다 :)</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1805.12471.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/pdf/1805.12471.pdf</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs" rel="external nofollow noopener noreferrer" target="_blank">https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/N18-1101.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/N18-1101.pdf</a><a href="#fnref:3" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;General Language Understanding Evaluation benchmark, 줄여서 GLUE
benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을
위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish
되었으며, &lt;a href=&quot;https://openreview.net/pdf?id=rJ4km2R5t7&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;전자&lt;/a&gt;는
설명이 상세하고 &lt;a href=&quot;https://www.aclweb.org/anthology/W18-5446.pdf&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;후자&lt;/a&gt;는 요약되어
있다. 이 글은 가장 최근(2019.2.22)에 업데이트된 &lt;a href=&quot;https://arxiv.org/pdf/1804.07461.pdf&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;arXiv에 있는 논문&lt;/a&gt;을
기반으로 작성되었다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/categories/ML/NLP/"/>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://inmoonlight.github.io/tags/NLP/"/>
    
      <category term="dataset" scheme="https://inmoonlight.github.io/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>수학으로 이해하는 양자컴퓨터의 기초</title>
    <link href="https://inmoonlight.github.io/2019/11/07/Basics-of-Quantum-Computings/"/>
    <id>https://inmoonlight.github.io/2019/11/07/Basics-of-Quantum-Computings/</id>
    <published>2019-11-06T15:07:00.000Z</published>
    <updated>2023-11-02T02:50:49.144Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을넘나들며 관련 지식을 습득해보았다. <a id="more"></a></p><p>아마 나와 같이 관련 기사나 여러 블로그 글, 유튜브 등을 찾아본사람들이라면 어렵지 않게 아래의 정보는 얻었을 것이다.</p><ul><li>n개의 qbit은 bit와 달리 \(2^n\)의 state를 표현할 수 있다.</li><li>superposition이란 동시에 0과 1의 상태를 띠는 성질로, 병렬연산이가능해져서 고전컴퓨터에 비해 계산 속도의 이점이 생긴다.</li></ul><p>텍스트만 보면 "아 그렇구나." 싶은 내용들이다. 이해가 된 것일까싶었지만 스스로에게 세 질문을 던졌을 때 답하지 못하는 것을 보며 제대로이해하지 못했음을 인지했다. <br></p><p>    <strong>Q1.</strong> n개의 bit로도 \(2^n\)을 표현할 수 있는거아닌가? 3개의 bit가 000, 001, 010, 011, 100, 101, 110, 111 이렇게 8개의상태를 표현할 수 있으니까. <br>     <strong>Q2.</strong> 양자 세계는불확정성 원리에 지배받는다고 하는데, 대체 양자컴퓨터로 어떻게 연산하고있는 것이며, 이 성질이 어떻게 계산 비용을 감소시킬 수 있는걸까? <br>    <strong>Q3.</strong> Entanglement는 qbit들이 어떻게 된상태인거지?</p><p>이 질문들에 제대로 답하기 위해서는 수학이 필요하다는 생각이 들었다.4차원 이상의 공간을 제대로 시각화하지 못하듯이 양자 세계를 자연어로표현한다는 것 자체가 말이 되지 않는 것 같았기 때문이다. 그래서 수학으로설명된 자료를 찾으려고 부던히 애를 썼고, 끝내 "내 수준에 맞는" (= 이글을 읽을 모두가 다 이해할 수 있는) 수학으로 설명된 자료를 찾았다.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/F_Riqjdh2oM?style=centerme" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a href="https://speakerdeck.com/ahelwer/quantum-computing-for-computer-scientists" rel="external nofollow noopener noreferrer" target="_blank">[slide]</a></p><p>이 영상을 보는 것을 추천하지만 무려 한시간이 넘는지라 글로도 정리를해보았다. 아래에 기술된 내용은 내 방식대로 위의 영상과 자료를 재구성한것이다.</p><p>사실 이 자료를 다 보더라도 양자컴퓨터에 대해 많은 것을 알았다고 보긴어렵다. python을 처음 접한 사람이 <code>print("Hello World!")</code>를성공했다고 해서 python을 잘 알았다고 하기 어려운 것처럼. 그리고 딥러닝에관심있는 사람이 tutorial을 따라해보며 CNN을 돌려봤다고 해서 딥러닝을 잘알았다고 하기 어려운 것처럼.</p><p><strong>그렇지만 양자 세계에 한 번은 <code>Hello World!</code>를날려봐야 하지 않을까?</strong></p><h2 id="introduction">Introduction</h2><p>The Deutsch-Jozsa problem 이라는 아주 간단한 문제를 통해 양자컴퓨터가고전컴퓨터에 비해 어떻게 연산 속도에서 이점을 보이는지 알아보려고 한다.이 과정을 이해하기 위해 양자컴퓨터가 연산하는 방법에 대해 소개할 것이며matrix 연산과 기초적인 논리회로에 대한 내용을 짚고 넘어갈 것이다.</p><p>추가로, entanglement에 대한 간단한 설명이 있다.</p><h2 id="basics">Basics</h2><h3 id="qubit-qbit">Qubit / Qbit</h3><p>Qubit 혹은 Qbit은 양자컴퓨터 계산의 기본적인 단위이다. 조금이라도양자컴퓨터에 대해 알아본 사람들이라면 qbit은 지겹도록 보고 들었을것이다.</p><p>Qbit은 언제나 다음의 조건을 만족시킨다.</p><blockquote><p>A qbit, represented by \(\begin{pmatrix} \alpha \\ \beta\end{pmatrix}\) where \(\alpha\) and \(\beta\) are complex numbers mustbe constrained by the equation \(||\alpha||^2 + ||\beta||^2 = 1\)</p></blockquote><p>따라서 아래의 예시들은 qbit에 해당된다.</p><p>\(\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}\) \(\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2}\end{pmatrix}\) \(\begin{pmatrix} 1 \\ 0 \end{pmatrix}\)\(\begin{pmatrix} 0 \\ -1 \end{pmatrix}\)</p><p>그리고 이 모든 벡터들의 basis가 되는 \(\begin{pmatrix} 1 \\0\end{pmatrix}\)과 \(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\)은 각각 \(\mid0\rangle\)과 \(\mid 1\rangle\)이라는 특별한 기호로 정의한다.</p><h3 id="superposition">Superposition</h3><p>양자컴퓨터의 qbit을 설명할 때 빠지지 않는 성질이다. "동시에 0과 1을가진다."는 문장으로 자주 설명되지만 이보다는 슈뢰딩거의 고양이 느낌이물씬 나는 "When we measure a qbit, it collapses to an actual value of 0or 1." 이라는 문장이 더 좋은 설명인 것 같다.</p><p>위에서 qbit이라고 언급했던 벡터 하나를 예시로 들어보자.</p><p>\[\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}\]</p><p>이 qbit은 \(0\) 혹은 \(1\)로 collapse될 확률이 \(\frac{1}{2}\) ( \(=|| \frac{1}{\sqrt{2}} || ^2\)) 이다.</p><p>감사하게도 <a href="https://quantum-computing.ibm.com/" rel="external nofollow noopener noreferrer" target="_blank">IBM은 자사의양자컴퓨터를 사용할 수 있는 API</a>를 만들어 놓았다. 여기서 이 qbit을만들고 1024번 관측해보면 0과 1이 50%씩 나오는 것을 확인할 수 있다.</p><p><img src="/assets/images/qubit_1_2_example.png?style=centerme" width="20%" alt="H gate를 통해 |0> 은 예시로 든 qbit으로 바뀐다. (이 내용은 밑에서 다룬다.)"></p><p><img src="/assets/images/qubit_1_2_result.png?style=centerme" width="95%" alt="1024번 관측한 결과다. 50% 확률로 0과 1을 나타낸다."></p><p>\(\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix}\)은 \(0\)으로 collapse 될 확률이 \(\frac{1}{4}\), \(1\)로 collapse 될확률이 \(\frac{3}{4}\)인 qbit이다.</p><p>\(|0\rangle\)은 0으로만 collapse 한다.</p><p><img src="/assets/images/qubit_0_example.png?style=centerme" height="20%" alt="|0>"></p><p><img src="/assets/images/qubit_0_result.png?style=centerme" height="95%" alt="1024번 관측한 결과로, 100% 0으로 관측된다."></p><h3 id="tensor-product">Tensor product</h3><p>여러 개의 qbit을 나타내기 위해 Tensor product 개념이 필요하다.수학적으로 엄밀한 표현은 아니지만, n개의 qbit 연산을 표현하기 위해서는아래의 표기 방식을 따르는 것이 좋다.</p><p>\[ \binom{x_0}{x_1} \otimes \binom{y_0}{y_1} = \begin{pmatrix} x_0\binom{y_0}{y_1} \\ x_1 \binom{y_0}{y_1} \end{pmatrix} = \begin{pmatrix}x_0 y_0 \\ x_0 y_1 \\ x_1 y_0 \\ x_1 y_1 \end{pmatrix} \]</p><p>이를 응용하면 2개, 3개의 qbit도 벡터처럼 표현할 수 있다.</p><p>\[ |01\rangle = \binom{1}{0} \otimes \binom{0}{1} = \begin{pmatrix} 0\\ 1 \\ 0 \\ 0 \end{pmatrix} \hspace{10pt} |100\rangle = \binom{0}{1}\otimes \binom{1}{0} \otimes \binom{1}{0} = \begin{pmatrix} 0\\ 0\\0\\0\\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} \]</p><p>이와 같이 tensor product의 결과로 표현된 벡터는 product state라고한다. 여기서 우리는 \(n\)개 qbit의 product state 크기가 \(2^n\) 이라는것을 알 수 있다. 만약 \( \binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}}\otimes \binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} = \begin{pmatrix}\frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{pmatrix}\) 의 multiple qbits 이 있다면 \(\mid 00\rangle\), \(\mid 01\rangle\),\(\mid 10\rangle\), \(\mid 11\rangle\)으로 collapse될 확률이 모두\(\frac{1}{4}\)이므로 동시에 4개의 state를 표현할 수 있게 된다. 즉,qbit이 bit와는 다르게 \(2^n\)개의 state를 표현할 수 있다고 한 것은<em>동시에</em> 가질 수 있는 최대 state 관점에서 비교한 것이다. bit는절대로 동시에 2개 이상의 state를 가질 수 없으므로 한 번에 계산할 수 있는정보는 1개 뿐이다.</p><p>또한 product state는, 뒤의 entanglement와 구분되는 중요한 성질로,<strong>독립적인 state들로 factorize가 가능</strong>하다는 점이있다.</p><p>Multiple qbits의 product state 또한 single qbit과 같은 성질을만족시킨다.</p><p>\[ \binom{a}{b} \otimes \binom{c}{d} = \begin{pmatrix} ac \\ ad \\ bc\\ bd \end{pmatrix} \]</p><p>\[ \text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1\]</p><h3 id="bit-operations">1-bit operations</h3><p>1-bit에서 가능한 연산은 Identity, Negation, Constant-0, Constant-1의총 4가지가 있다.</p><p><img src="/assets/images/1bit_operations.png?style=centerme" width="40%" alt="1-bit 연산의 4 종류"></p><p>각각의 연산은 matrix로 표현할 수 있다.</p><p>\[ \text{Identity} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix} \] \[ \text{Negation} = \begin{pmatrix} 0 &amp; 1 \\ 1&amp; 0 \end{pmatrix} \] \[ \text{Contant-0} = \begin{pmatrix} 1 &amp; 1\\ 0 &amp; 0 \end{pmatrix} \] \[ \text{Contant-1} = \begin{pmatrix} 0&amp; 0 \\ 1 &amp; 1 \end{pmatrix} \]</p><p><img src="/assets/images/1bit_matrix.png?style=centerme" width="70%" alt="1-bit 연산의 matrix 표현"></p><h3 id="cnot-one-of-the-2-bit-operations">CNOT (one of the 2-bitoperations)</h3><p>CNOT 연산은 control bit와 target bit로 구성된 2-bit가 있을 때 controlbit가 0이면 target bit를 바꾸지 않고, control bit가 1일 때 target bit를바꾸는 연산이다.</p><p><img src="/assets/images/CNOT_operation.png?style=centerme" width="25%" alt="CNOT"></p><p>마찬가지로 이 연산도 matrix로 표현할 수 있다.</p><p>\[ C = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0&amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \]</p><p><img src="/assets/images/CNOT_examples.png?style=centerme" width="60%" alt="2-qbits에 적용한 CNOT 예시"></p><p><a href="#bit-operations">2.4</a>와 <a href="#cnot-one-of-the-2-bit-operations">2.5</a>에서 operation들을matrix화 한 것에 주목하자. 확률이 지배하는 양자 세계에서 deterministic한연산을 하기 위해서는 matrix를 관측하지 않은 qbit에 곱하는 것이 유일한방법이기 때문이다. 아래의 예시에서 우리가 확신할 수 있는 정보는 qbit이0과 1로 관측될 확률이 반대가 되었다는 것이다.</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix} =\begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix} \]</p><p>항상 0 혹은 1로 관측되는 \(\mid0\rangle\)이나 \(\mid1\rangle\)을 쓰면matrix 연산을 고집하지 않아도 되지만 이런 qbit만 사용할거라면고전컴퓨터를 쓰면 그만이다. 굳이 0K 가까이 되는 험악한 조건을 유지해가며계산할 필요가 없다.</p><p>그래서 matrix 연산은 양자 컴퓨팅에서 굉장히 중요하다. 여기에는 한가지추가조건이 있는데, 반드시 연산에 사용되는 matrix는<strong>reversible</strong>해야한다는 것이다. 따라서 앞서 본 1-bit 연산중 Constant-1과 Constant-0를 계산하기 위해서는 단순 matrix를 곱하는 것외의 다른 방법이 필요하다.</p><h2 id="the-deutsch-jozsa-problem">The Deutsch-Jozsa problem</h2><p>이 문제<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement&gt;">[1]</span></a></sup>는 양자컴퓨터가 고전컴퓨터에 비해 계산적인 이점을가지는 아주 간단한 (<del><em>동시에 쓸데없는</em></del>) 문제다.</p><blockquote><p>1-bit를 입력받아서 1-bit를 내뱉는 어떤 함수가 있다고 하자. 만약 이함수가 Constant(Contant-0, Constant-1)인지, 아니면 Variable(Identity,Negation)인지 알기 위해서는 최소 몇 번의 query를 날려야 할까?</p></blockquote><h3 id="classical-computer">Classical computer</h3><p>고전컴퓨터에서는 0과 1을 입력해야하므로 총 두 번의 연산이필요하다.</p><h3 id="quantum-computer">Quantum computer</h3><p>예상했듯이 정답은 한 번이다. 왜인지 알기위해서는 추가적인 개념이필요하다.</p><h4 id="hadamard-gate">Hadamard gate</h4><p>앞서 언급된 적 있는 H gate이다.</p><p>\[ H = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>Hadamard gate는 0- 혹은 1-qbit을 받아서 0과 1을 같은 확률로 가지는qbit으로 바꿔준다.</p><p>\[ H\mid0\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>\[ H\mid1\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>Hadamard gate는 또 다른 중요한 성질이 있다. 0과 1을 같은 확률로가지는 qbit을 다시 0- 과 1-qbit으로 돌려보낸다는 것이다.</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} =\begin{pmatrix} 1 \\ 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} 0 \\ 1 \end{pmatrix} \]</p><h4 id="x-gate">X gate</h4><p>X gate는 qbit의 위 아래를 바꿔준다.</p><p>\[ X = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 0\end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}\end{pmatrix} \]</p><p>H gate와 X gate 연산을 이해하기 쉽게 표현하면 아래의 그림이 된다.붉은색이 X gate, 노란색이 H gate 연산의 방향이다.</p><p><img src="/assets/images/gates-visualization.png?style=centerme" width="80%"></p><p>\(\begin{pmatrix} 1 \\ 0 \end{pmatrix}\)에 X - H - X - H - X gate를씌운 결과는 그림으로 보면 더 쉽게 이해된다. \(\begin{pmatrix} 1 \\ 0\end{pmatrix}\)에서 출발해 각 gate가 연산되는 방향으로 화살표를 움직이면마지막에 도달하는 곳이 결과값이 된다.</p><p><img src="/assets/images/quantum_logic_operation_example.png?style=centerme" width="80%"></p><h4 id="non-reversible-matrix">non-reversible matrix</h4><p>앞서 양자컴퓨터는 non-reversible한 matrix 를 곱하는 연산은불가능하다고 했다. 1-bit 연산 중에서 Constant-0과 Constant-1은non-reversible하다. 그래서 양자 컴퓨팅에서는 2개의 qbit을 사용한다.</p><p><img src="/assets/images/quantum_non_reversible.png?style=centerme" width="45%"></p><p>이 그림의 input과 output notation을 보면 "응?" 이라는 생각이 절로 들것이다. 영상에서도 사람들이 대체 왜 "Output"이 Input 쪽에 가 있는지에대해 끊임없이 묻는다. 아쉽게도 강연자는 속시원하게 답변을 해주지 않아서그런가보다 하고 넘어갔는데 다시 보니 이해가 된 부분이 있어 글로설명해보려고 한다.</p><p><code>Input'</code>과 <code>Output'</code>이 실제 1-bit 연산의input과 output을 나타낸다. 그리고 <code>Input</code>과<code>Output</code>은 <code>Input'</code>과 <code>Output'</code>이 BB이후에 있기 때문에 BB 이전에 <code>Input'</code>과<code>Output'</code>이 1-bit 연산의 input과 output이 되도록 넣어주는,양자컴퓨터 연산 방식 때문에 필요한 input들이다.</p><p>이 약속에 따라서 양자컴퓨터가 1-bit 연산을 어떻게 수행하는지 아래의예시를 통해 좀 더 이해해보자.</p><p><img src="/assets/images/quantum_constant_0.png?style=centerme" width="55%" alt="Constant-0"></p><p><img src="/assets/images/quantum_constant_1.png?style=centerme" width="55%" alt="Constant-1"></p><p><img src="/assets/images/quantum_identity.png?style=centerme" width="55%" alt="Identity. 이미지의 operation은 CNOT"></p><p><img src="/assets/images/quantum_negation.png?style=centerme" width="55%" alt="Negation"></p><p>Constant-0은 <code>Input'</code>이 \(\mid 0 \rangle\)일 때와 \(\mid 1\rangle\)일 때 모두 <code>Output'</code>이 \(\mid 0 \rangle\)이어야한다. 어떤 gate도 없는 왼쪽 위 그림의 회로에서 <code>Input</code>에\(\mid 0 \rangle\) 혹은 \(\mid 1 \rangle\)을 대입해보면<code>Input'</code>과 <code>Output'</code>이 Constant-0의 관계를 가지는것을 확인할 수 있다.</p><p>Indentity는 <code>Input'</code>이 \(\mid 0 \rangle\)일 때는<code>Output'</code>이 \(\mid 0 \rangle\)이고 <code>Input'</code>이\(\mid 1 \rangle\)일 때는 <code>Output'</code>이 \(\mid 1 \rangle\)인함수다. 왼쪽 아래 그림의 회로는 CNOT gate를 표현하고 있다. 색이 채워진원이 control bit 쪽을 나타내고 그렇지 않은 쪽 원은 target bit를나타낸다. <code>Input</code>이 \(\mid 0 \rangle\) 이면 control bit가0이므로 target bit도 그대로 유지한다. 그래서 <code>Input'</code>도\(\mid 0 \rangle\), <code>Output'</code>도 \(\mid 0 \rangle\)이 된다.<code>Input</code>이 \(\mid 1 \rangle\) 이면 control bit가 1이므로target bit가 바뀐다. 그래서 <code>Input'</code>도 \(\mid 1 \rangle\),<code>Output'</code>도 \(\mid 1 \rangle\)이 된다.</p><p>그럼 다시 The Deutsch-Jozsa problem로 돌아가서, 양자컴퓨터에서는어떻게 한 번에 구할 수 있을까? 정답은 아래의 그림이 설명해준다.</p><p><img src="/assets/images/quantum_one_query.png?style=centerme" width="60%" alt="양자컴퓨터가 한 번에 문제를 푸는 법"></p><p>이 연산대로라면 BB가 Constant(Contant-0, Constant-1)이었을 경우, 측정결과가 \(\mid11\rangle\)이고, Variable(Identity, Negation)이었을경우에는 \(\mid01\rangle\)이 된다.</p><p>BB의 경우의 수를 따져가며 이해해보자.</p><h4 id="preprocessing-bb-입력-직전까지의-연산">preprocessing (BB 입력직전까지의 연산)</h4><p><img src="/assets/images/quantum_preprocessing.png?style=centerme" width="80%"></p><p>BB에 들어가기 전 input (\(\mid 0 \rangle\)) 과 output qbit (\(\mid 0\rangle\)) 모두 X와 H gate를 거쳐서 \(\begin{pmatrix} \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}} \end{pmatrix}\) 가 된다.</p><h4 id="case-1-bb가-constant-0-이었을-경우">case 1) BB가 Constant-0이었을 경우</h4><p>Constant-0은 input과 output에 어떤 gate도 씌우지 않는다. 따라서 BB가Constant-0이었을 때 Input과 Output은 H gate만 통과한 이후 관측된다.</p><p><img src="/assets/images/const_0.png?style=centerme" width="50%" alt="Constant-0"></p><p><img src="/assets/images/quantum_bb_const_0.png?style=centerme" width="80%" alt="BB가 Constant-0인 경우 Input'과 Output'"></p><h4 id="case-2-bb가-contstant-1-이었을-경우">case 2) BB가 Contstant-1이었을 경우</h4><p>Constant-1은 output에만 X gate를 적용한다. 따라서 BB가Constant-1이었을 때는 Output에 X gate가 추가되고, 이후 Input과 Output모두에 H gate가 적용된다.</p><p><img src="/assets/images/const_1.png?style=centerme" width="55%" alt="Constant-1"></p><p><img src="/assets/images/quantum_bb_const_1.png?style=centerme" width="80%" alt="BB가 Constant-1인 경우 Input'과 Output'"></p><h4 id="case-3-bb가-identity-이었을-경우">case 3) BB가 Identity 이었을경우</h4><p>Identity는 CNOT gate를 통해 연산된다. 앞에서 CNOT 연산은 \(\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \) 을 곱하는 것과 같다고 설명했다. Preprocessing을 거친Input과 Output은 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 이므로 CNOT연산은 아래와 같이표현할 수 있다.</p><p>\[ C \begin{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \otimes \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \end{pmatrix} =C \begin{pmatrix} \frac{1}{2} \\ \frac{-1}{2} \\ \frac{-1}{2} \\\frac{1}{2} \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 &amp; 0 &amp;0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0&amp; 0 &amp; 1 &amp; 0 \\ \end{pmatrix} \begin{pmatrix} 1 \\ -1 \\ -1\\ 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1\end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix} \otimes \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>즉 Input은 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 에서 \( \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\) 로 바뀌고Output은 그대로 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 이 된다. 이 상태에서 H gate가 각각적용되어 최종 결과는 \(\mid 01 \rangle\)이 된다.</p><p><img src="/assets/images/identity.png?style=centerme" width="55%" alt="Identity"></p><p><img src="/assets/images/quantum_bb_identity.png?style=centerme" width="80%" alt="BB가 Identity인 경우 Input'과 Output'"></p><h4 id="case-4-bb가-negation-이었을-경우">case 4) BB가 Negation 이었을경우</h4><p>Negation은 Indentity의 결과 중 Output에만 X gate가 추가되는 연산이다.따라서 아래 그림처럼 연산이 이루어지고 Identity와 마찬가지로 최종 결과는\(\mid 01 \rangle\)이 된다.</p><p><img src="/assets/images/negation.png?style=centerme" width="55%" alt="Negation"></p><p><img src="/assets/images/quantum_bb_negation.png?style=centerme" width="80%" alt="BB가 Negation인 경우 Input'과 Output'"></p><p>정리하면, 양자컴퓨터에서는 특정 설계 상황에서 고정된 BB input에 대한BB output을 "한 번"만 관측하면 BB가 Constant인지 Variable인지 확인할 수있다는 것이다!</p><h2 id="entanglement">Entanglement</h2><p>Entanglement는 지금까지의 흐름에서는 동떨어진 이야기지만양자컴퓨터에서 항상 소개되는 내용이기 때문에 추가하였다.</p><p>앞서 qbit과 product state의 성질을 수학적으로 나타낸 것처럼entanglement도 수학적인 성질로 표현할 수 있다.</p><p>\( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} \) 는 entangle된 qbit인데, 그 모양새가 product state와닮아있다. 하지만 product state와는 중요한 성질에서 차이를 보인다. 위에서설명했듯이 product state는 개별적인 qbit으로 factorize된다. 하지만entanlged qbit은 개별적인 qbit으로 factorize 되지 않는다. (If theproduct state of two qbits <strong>cannot be factored</strong>, they aresaid to be <strong>entanlged</strong>.) 이 때문에 entangled qbit은차원이 늘어난 하나의 qbit으로 볼 수 있으며 일부를 관측했을 때 나머지일부의 상태가 유추된다.</p><p>\( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} \) 이 entangle 되었음을 증명하는 것은 간단하다. \(\begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} = \begin{pmatrix} a \\ b \end{pmatrix} \otimes\begin{pmatrix} c \\ d \end{pmatrix} \) 를 만족하는 \(a\), \(b\), \(c\),\(d\)는 존재하지 않기 때문에 이는 entanlge되어 있는 qbit이다.</p><p>Entanlged qbit은 CNOT과 H gate를 통해 쉽게 생성할 수 있다.</p><p><img src="/assets/images/entanlge.png?style=centerme" width="50%" alt="Entangled qbit"></p><p>\[ CH_1 \begin{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \otimes\begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} = C \begin{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\otimes \begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} =\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\\frac{1}{\sqrt{2}} \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>만약 이후에 이런 게이트의 조합을 본다면 곧바로 'entanlge 되었군!'이라고 생각하면 된다 :)</p><h2 id="conclusion">Conclusion</h2><p>개인적으로 이 영상을 본 이후, 속이 뻥 뚫리는 기분이 들었다. 아직matrix로 표현되는 qbit이 물리적으로 어떤 모습인지, gate들이 물리적으로어떻게 qbit에 적용되는지는 모르지만 (이건 실제 양자컴퓨터를 눈으로 보면이해가 되지 않을까) 이 정도라도 양자컴퓨터와 고전컴퓨터의 연산과정에서의차이를 구체적으로 알 수 있었기 때문에 만족할 수 있었다.</p><p>양자컴퓨터의 연산 과정을 이해하고나니 양자 우월성은 그냥 달성되는것은 아니었으며, 잘 설계된 gate가 뒷받침되었을 때 가능한 것임을 깨닫게되기도 했다.</p><p>이 정도면 양자 세계에 <code>Hello World!</code>를 했다고 볼 수 있지않을까?</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement" rel="external nofollow noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Deutsch–Jozsa_algorithm#Problem_statement</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은
전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이
때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을
넘나들며 관련 지식을 습득해보았다.
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="Quantum Computing" scheme="https://inmoonlight.github.io/categories/ML/Quantum-Computing/"/>
    
    
      <category term="quantum" scheme="https://inmoonlight.github.io/tags/quantum/"/>
    
      <category term="quantum computing" scheme="https://inmoonlight.github.io/tags/quantum-computing/"/>
    
  </entry>
  
  <entry>
    <title>Naver News Comment Analysis (3)</title>
    <link href="https://inmoonlight.github.io/2019/09/23/Naver-News-Comments-Analysis-(3)/"/>
    <id>https://inmoonlight.github.io/2019/09/23/Naver-News-Comments-Analysis-(3)/</id>
    <published>2019-09-23T08:25:00.000Z</published>
    <updated>2023-11-02T02:50:49.147Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="tldr">TL;DR</h2><p>어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까?</p><p>네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어무의식적으로 다양한 사고에 대한 가능성을 차단받는다.</p><p>그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sortingalgorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다.<a id="more"></a></p><h2 id="introduction">Introduction</h2><p>모두가 다 알고 있는 사실이지만, 어뷰저는 존재한다. 드루킹과 에서 나온 결론으로도 뒷받침될 수 있지만 트위터에<code>m.news.naver.com/comment</code> 라고 검색하기만 해도 아래와 같이댓글 조작의 흔적을 쉽게 발견할 수 있다.</p><p><img src="/assets/images/twitter.png?style=centerme" width="60%" alt="https://twitter.com/search?q=m.news.naver.com%2Fcomment&src=recent_search_click"><br></p><p>이렇듯 쉽게 어뷰저의 존재를 찾을 수 있음에도 네이버가 어뷰저를 잡지않는 이유는 그 일이 생각처럼 쉬운 일이 아니기 때문이다.</p><p>n초 안에 여러번 공감과 비공감을 지속적으로 받은 댓글은 어뷰징의결과로 의심한다. 그 댓글을 지워야 할까? 만약 댓글을 쓴 유저가 어뷰저가아니었다면 문제가 될 수 있다.</p><p>사후 분석을 통해 어뷰저로 의심되는 댓글의 내용을 지우는 방법은어떨까? 뉴스라는 매체의 특성 상 시간이 지난 기사는 사람들이 관심있게보지 않는다. 그러므로 이 방법은 어뷰저를 막는다고 볼 수 없다.</p><p>분석을 통해 어뷰저라고 강하게 의심되는 유저를 차단한다고 하더라도새로운 패턴으로 어뷰징을 하는 유저들이 생겨날 것이다. 어뷰저의 기준을세우는 것은 어려운 반면 새로운 방식으로 어뷰징을 하는 것은 좀 더 쉽기때문에 이렇게 물고 물리는 싸움은 어뷰저에게 유리하다.</p><p>그렇다면 어뷰저를 차단하는 것에만 집중하지 말고, 어뷰징은 내버려두되그 효과를 완화시키는 방법은 어떨까? <strong>지금 네이버 뉴스 댓글 랭킹방식은 그것이 미치는 영향력에 비해 너무 간단하고 단편적이다.</strong>구글의 검색 랭킹이 신뢰도를 가지고 있는 이유는 상위에 랭크된 글이'조작'을 통해 만들어지지 않았다는 점 때문일 것이다. 그 이유는 정보가되는 글에 대한 정보량, 품질 기준이 보다 엄격하고 단편적인 면으로만순위를 매기지 않기 때문이다. 만약 구글 랭킹이 웹문서의 클릭수로만 되어있었다면 어땠을까? 많은 기업들이 본인의 홈페이지를 상위에 랭크시키기위해 많은 조작이 일어났을 것이다.</p><p>그래서 이번 글에서는 그렇게 간단하다고는 볼 수 없는 다른 랭킹algorithm에 대해 소개해보려고 한다. 현재 네이버 뉴스 댓글 랭킹 방식 중순공감순, 공감비율순, 답글순의 한계점을 살펴보고 reddit과 yelp에서신뢰도있게 쓰이는 best 랭킹과 새로운 관점의 controversial 랭킹algorithm을 소개한다.</p><h2 id="naver-news-comment-sorting-system">Naver News Comment SortingSystem</h2><h3 id="sorting-algorithms">Sorting Algorithms</h3><p>2019년 9월 기준, 총 5개의 정렬방으로 서비스되고 있다. 드루킹 논란이후 댓글 제공 여부와 정렬방식을 언론사가 선택하는 방식으로바뀌었다.</p><ul><li><strong>순공감순</strong>: 공감 -비공감<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="2017년 11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의 호감순처럼 호감도를 “공감-비공감”으로 계산하게 되었다.">[1]</span></a></sup></li><li><strong>공감비율순:</strong> 공감 / (공감 + 비공감)</li><li><strong>답글순</strong></li><li><strong>최신순</strong></li><li><strong>과거순</strong></li></ul><p>이 중, 댓글에 대한 사용자의 인터랙션(공감, 비공감, 답글)으로 순위를매기는 순공감순, 공감비율순, 답글순에 대한 문제점을 하나씩 짚어보고자한다.</p><h3 id="limitations">Limitations</h3><h4 id="순공감순">순공감순</h4><p>순공감순은 우리의 직관과 벗어나는 랭킹이라는 점에서 한계가 있다.우리는 절대적인 공감 수치보다, 공감비율로 댓글의 신뢰도를 평가한다.</p><p>아래의 사례는 네이버 뉴스댓글<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="홍준표 “나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버 뉴스">[2]</span></a></sup>의 실제 예시이다. 첫번째 댓글은 순공감 344개(= 455- 111) 로, 300개(= 316 - 16)의 순공감을 지니는 두번째 댓글보다 더 높은순위에 자리한다. 하지만 각각의 댓글의 공감비율은 80.4%(= 455 / (455 +11)) 로, 두번째 댓글의 공감비율인 95.2% (= 316 / (316 + 16)) 보다작다.</p><p><img src="/assets/images/sgg_limit.png?style=centerme" width="70%" alt="paid****: 순공감 344, 공감비율 80.4% | adam****: 순공감 300, 공감비율 95.2%"><br></p><h4 id="공감비율순">공감비율순</h4><p>앞서 설명한 것처럼 공감비율순이 좀 더 우리의 직관과 유사한 척도이다.하지만 공감비율순은 전체 공감, 비공감 수가 적을 때 문제가 된다.</p><p>소수의 사람들에게만 노출된 댓글은 공감과 비공감의 개수가 모두 적어100% 라는 공감비율이 쉽게 만들어지는 반면, 여러 명에게 노출된 댓글은하나의 비공감만 달리더라도 그보다 낮은 공감비율을 지니게 되는 문제가발생한다.</p><p>아래의 네이버 뉴스 댓글예시<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="재판에 넘겨진 조국 부인 정경심 교수…검찰 &#39;소환 임박&#39;, 네이버 뉴스">[3]</span></a></sup>에서 공감수가 20, 비공감수가 0인 댓글이 비공감을전혀 받지 않아 공감비율 100%가 되어 더 많은 사람들이 읽고 공감을 표한공감수 1021, 비공감수 58인 댓글보다 더 상단에 위치한다.</p><p><img src="/assets/images/ggratio_limit.png?style=centerme" width="70%" alt="euic****, qkrs**** 공감비율 100% 공감+비공감수 20 | hang**** 공감비율 94.6% 공감+비공감수 1,079"><br></p><h4 id="답글순">답글순</h4><p>여러 개의 답글이 달리는 댓글은 주로 일찍 남겨진 댓글 중에인신공격이나 뉴스 외 주제에 대한 댓글인 경우가 많다. 댓글 공간에서는명확한 내용으로 구성된 댓글에 대해서는 대댓글 보다도 공감 혹은비공감으로 본인의 주장을 표시하는 것이 일반적이다. 그러나 감정적으로쓰여진 댓글은 그 댓글에 자극을 받은 다른 사용자의 답글로 이어지고 되므로답글 개수를 기준으로 댓글을 정렬하면 뉴스 내용과는 무관한 자극적인댓글들이 우선적으로 노출된다.</p><p>또한 일찍 쓰여진 댓글일수록 더 많은 사람들에게 노출될 가능성이있으므로 대부분 뉴스 작성 시점과 가까운 댓글이 상위에 랭크된다.</p><p>랭킹 algorithm으로 보기에는 정렬 기준이 controllable하지 않으며댓글의 유익한 속성이 높게 평가되어 정렬되는 랭킹이라고 볼 수 없다.</p><p>아래의 네이버 뉴스 댓글예시<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="대학교수 이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버 뉴스">[4]</span></a></sup>를 보면 vote 수가 많지 않아도, 공감수가 전혀 없고비공감만 받더라도 top 10에 위치할 수 있다.</p><p><img src="/assets/images/replyCount_limit.png?style=centerme" width="70%"></p><h2 id="reddit-comment-sorting-algorithms">Reddit Comment SortingAlgorithms</h2><p>댓글이 활발하게 생성되는 플랫폼은 비단 네이버 뉴스 뿐만은 아니다.네이버 쇼핑, 네이버 호텔, 망고 플레이트, reddit, stackoverflow, yelp,amazon 등의 다양한 플랫폼에서 수집되며 플랫폼에서는 다시 이 데이터를가공하여 사용자에게 유익한 정보를 제공한다.</p><p>그 중에서도 reddit의 랭킹 시스템이 앞서 비판했던 순공감순,공감비율순의 한계를 극복한 sorting algorithm을 제공하고 있기에 자세히살펴보려고 한다. reddit의 랭킹 방식에는 best, top, new, controversial,old, q&amp;a가 있다. top이 순공감순, new가 최신순, old가 과거순이다.</p><p><img src="/assets/images/reddit_sorting.png?style=centerme" width="20%" alt="reddit의 sorting algorithms"></p><h3 id="best">Best</h3><p>Best ranking<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system&gt;">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html&gt;">[6]</span></a></sup> 은 Wilsonscore<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval&gt;">[7]</span></a></sup>로 정렬한 것으로, 공감비율순의 단점으로언급되었던, 전체 vote수가 적은 상황을 smoothing시켜준 algorithm이다.reddit뿐 아니라 yelp에서도 사용한다고한다<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is&gt;">[8]</span></a></sup>.</p><p>Wilson score는 주어진 positive와 negative vote가 binomialdistribution을 따른다고 가정했을 때, positive 발생 확률을 95% 신뢰구간의최소값으로 추정한 값이다.</p><p>동전 뒤집기 상황에서 앞면을 positive, 뒷면을 negative라고 하자. n번던진 후 앞면이 나올 확률(\(p\))을 추정할 때 n이 충분히 큰 경우 centrallimit theorem에 의해 \(p\)는 normal distribution을 따른다. 따라서 95%의신뢰도로 \(p\)를 추정하여 \(p\)의 최소값, 최대값을 구할 수 있고 이 때최소값이 Wilson score가 된다. 자세한 수식은 <a href="#appendix-a-wilson-score">Appendix A</a>에 정리해두었다.</p><p>\[ w^- = max(0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)})=\text{wilson score} \]</p><p>위의 식을 함수로 구현하면 다음과 같다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># ref: http://www.evanmiller.org/how-not-to-sort-by-average-rating.html</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">best</span><span class="params">(up, down)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        z = <span class="number">1.96</span>  <span class="comment"># 95% confidence level</span></span><br><span class="line">        n = up + down</span><br><span class="line">        p_up = up / n</span><br><span class="line">        p_down = <span class="number">1</span> - p_up</span><br><span class="line">        denominator = <span class="number">2</span> * (n + z**<span class="number">2</span>)</span><br><span class="line">        numerator = <span class="number">2</span> * n * p_up + z**<span class="number">2</span> - z * np.sqrt(z**<span class="number">2</span> + <span class="number">4</span> * n * p_up * p_down)</span><br><span class="line">        lower = numerator / denominator</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">        lower = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> max(<span class="number">0</span>, lower)</span><br></pre></td></tr></table></figure><p>아래의 예시는 네이버 뉴스 댓글에 Best ranking algorithm을 적용해본결과이다. 공감비율순 정렬이었다면 "<em>원칙대로만 하시면 됩니다 역사에부끄럽지 않게 잘 해 주세요</em>"는 1000개 이상의 vote를 가진 "<em>법대로해라 법은 만인 앞에 평등하다</em>"는 댓글을 제치고 상위에 랭크되었을것이다. 하지만 Best 정렬방식에서는 vote 수가 적은 경우 약간의 penalty를받기 때문에 하위에 랭크되었다.</p><ul><li>MB '정치보복' 반발에 문무일 총장 "법적 절차대로하겠다"<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="MB &#39;정치보복&#39; 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버 뉴스">[9]</span></a></sup></li></ul><table style="width:100%;"><colgroup><col style="width: 59%"><col style="width: 7%"><col style="width: 8%"><col style="width: 10%"><col style="width: 14%"></colgroup><thead><tr class="header"><th>comments</th><th>공감수</th><th>비공감수</th><th>best score</th><th>공감비율</th></tr></thead><tbody><tr class="odd"><td>법대로 해라 법은 만인 앞에 평등하다</td><td>1091</td><td>55</td><td>0.938</td><td>0.952006980803</td></tr><tr class="even"><td>법대로 하면 사형인데 !!</td><td>562</td><td>39</td><td>0.936</td><td>0.935108153078</td></tr><tr class="odd"><td>제발 법대로만 해주세요. 그래도 나라를 지옥으로 만든 죄는 물을 법도없다. 이 악마야!!!</td><td>252</td><td>14</td><td>0.933</td><td>0.947368421053</td></tr><tr class="even"><td>지금까지 반발하고 나서 살아남은 넘을 못봤다.</td><td>565</td><td>38</td><td>0.933</td><td>0.936981757877</td></tr><tr class="odd"><td>혓바닥몇번 낼름거릴까나했더니 찔렸나보네ㅎㅎ</td><td>595</td><td>37</td><td>0.932</td><td>0.941455696203</td></tr><tr class="even"><td>본인이 구린짓을 했으니까 먼저 발광하는거겠지..</td><td>686</td><td>43</td><td>0.931</td><td>0.941015089163</td></tr><tr class="odd"><td>법대로 하는 것보다 더 정의로운 절차는 세상에 없다</td><td>4146</td><td>317</td><td>0.926</td><td>0.928971543805</td></tr><tr class="even"><td>당연히 법대로 하셔야죠</td><td>296</td><td>14</td><td>0.921</td><td>0.954838709677</td></tr><tr class="odd"><td>원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요</td><td>302</td><td>13</td><td>0.921</td><td>0.95873015873</td></tr><tr class="even"><td>법대로 합시다</td><td>919</td><td>51</td><td>0.92</td><td>0.947422680412</td></tr></tbody></table><p>기본적으로 공감수가 많은 댓글을 상위에 랭크시키는 알고리즘이기 때문에어뷰징 작업으로 공감수가 부풀려진 댓글이 top 10 밖으로 밀려나지는못한다. 하지만 vote수가 적더라도 경향성을 파악해 댓글을 정렬시키기때문에 단순한 순공감이나 공감비율순으로는 하위권에 있던 댓글이 상위권에위치할 기회를 증가시켰다.</p><p>어뷰저 입장에서는 쉽게 계산할 수 있는 정렬방식이 아니기 때문에 조작이어려워질 것이다. 어뷰징을 할 때 고의로 공감과 비공감을 섞어서 해당댓글을 상위에 랭크시키는데, Best 정렬이라면 "적당"한 비율을 맞추기까다로워질 것이다.</p><h3 id="controversial">Controversial</h3><p>controversial<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence&gt;">[10]</span></a></sup>은 이름 그대로, 공감과 비공감이 팽팽하게 맞서는댓글을 상위에 위치시키려는 알고리즘이다. 단순히 팽팽하기만 하면 공감과비공감이 1:1인 상황과 10:10인 상황이 같다고 생각할 수 있기에 vote수도sorting algorithm에 포함시켜서 10:10이 1:1인 상황보다 더 controversial할수 있도록 만들어졌다.</p><p>아래의 식에서 upvote는 공감을, downvote는 비공감을 의미한다. upvote와downvote의 차이가 같아서 분모가 같아진 경우에는 그 크기가 큰 쪽이 높고,vote의 크기가 같은 경우에는 차이가 작은 쪽이 높다.</p><p>\[ \text{controversial} = \frac{match \times log(match + 1)}{| upvote- downvote | + 1},\text{ where }match=min(upvote, downvote) \]</p><p>python으로 구현한 식이다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">controversial</span><span class="params">(upvote, downvote)</span>:</span></span><br><span class="line">    match = min(upvote, downvote)</span><br><span class="line">    top = match * math.log(match + <span class="number">1</span>)</span><br><span class="line">    bottom = abs(upvote - downvote) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> float(top) / bottom</span><br></pre></td></tr></table></figure><p>좀 더 직관적인 이해를 돕기 위해 가공한 아래의 예시를 보자.</p><table><thead><tr class="header"><th>upvote</th><th>downvote</th><th>controversial score</th></tr></thead><tbody><tr class="odd"><td>1001</td><td>1000</td><td>3454.38</td></tr><tr class="even"><td>999</td><td>1000</td><td>3450.42</td></tr><tr class="odd"><td>100</td><td>100</td><td>461.52</td></tr><tr class="even"><td>101</td><td>100</td><td>230.76</td></tr><tr class="odd"><td>1000</td><td>700</td><td>15.24</td></tr><tr class="even"><td>130</td><td>100</td><td>14.89</td></tr><tr class="odd"><td>100</td><td>130</td><td>14.89</td></tr><tr class="even"><td>1</td><td>1</td><td>0.69</td></tr><tr class="odd"><td>1</td><td>2</td><td>0.35</td></tr></tbody></table><p>upvote, downvote의 비율이 비슷한 댓글 순서로 정렬되고, 그 비율내에서는 vote 수가 큰 댓글이 더 위에 놓이게 된다.</p><p>controversial algorithm을 네이버 뉴스 댓글에 적용해보았다. 예상대로공감과 비공감 수치가 비슷하면서도 vote수가 많은 댓글이 가장 먼저 보인다.vote수가 작은 이유는 이미 순공감 노출로 인해 vote를 받을 기회를 박탈당한댓글들이기 때문이다.</p><p>수치와는 무관하게 top 10 댓글의 내용은 얼마나 controversial하게구성되어 있는지 정성적으로 평가해보았다. 보도자료에 대한 찬성은 <span style="color:blue">푸른색</span>, 반대는 <span style="color:red">붉은색</span> , 애매한 문장은 표기하지 않았다.controversial하다면 뉴스 기사의 주제에 대해 찬성과 반대가 골고루섞여있어야 할 것이다.</p><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 9%"><col style="width: 67%"><col style="width: 9%"><col style="width: 12%"></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 1</td><td>대통령 각하, ‘사드 문제’ 갖고 거품무는 중국에도 내정 간섭이라고 거침없이 말씀해주세요</td><td>26</td><td>26</td></tr><tr class="even"><td>user 2</td><td><span style="color:red">이제는 한미일군사훈련을 해야한다.</span></td><td>81</td><td>85</td></tr><tr class="odd"><td>user 3</td><td><span style="color:red">근데 왜 중국한테는 대놓고 내정간섭 받는거죠,대통령님? 치욕스러웠던 조선시대가 그리운건가요?</span></td><td>22</td><td>22</td></tr><tr class="even"><td>user 4</td><td><span style="color:red">봐라 ㅋㅋㅋ 연기하지?철수 얘기나온다 백퍼ㅋㅋㅋㅋ 베트남꼴 나는거야 ㅋㅋㅋ 정신 좀 차리자</span></td><td>33</td><td>31</td></tr><tr class="odd"><td>user 5</td><td><span style="color:red">아베만도못한 문통; </span></td><td>11</td><td>11</td></tr><tr class="even"><td>user 6</td><td><span style="color:red">문재인 아가라 닥쳐라. 사드도 내정문제인데중국한테는 끽소리 못 하던 색히가 어디서 주둥아리 씨부리노. </span></td><td>10</td><td>10</td></tr><tr class="odd"><td>user 7</td><td><span style="color:red">ㅋㅋㅋㅋㅋㅋ 곧 양념단와서 또 평화올림픽울부짖겠네. </span></td><td>66</td><td>57</td></tr><tr class="even"><td>user 8</td><td><span style="color:red">미국이 한국을 버려야 할 듯.없네.</span></td><td>15</td><td>14</td></tr><tr class="odd"><td>user 9</td><td><span style="color:red">미국을 대변하는거다.국익을 최우선으로하는거지싫지만 아베가 똑똑하지않는냐.살자. </span></td><td>8</td><td>8</td></tr><tr class="even"><td>user 10</td><td><span style="color:red">얼마나 답답하면 저런말을 할지 생각안해보셨나요?? 북에서 원하는 대로 흘러가네요. 앞으로 한미군사훈련연기뿐만 아니라 축소되고 없어지고 난리나겠네</span></td><td>8</td><td>8</td></tr></tbody></table><p>분명 공감수와 비공감수는 controversial하지만 대부분이 당시의 여론과반대대는 내용으로 치우쳐있다. 정성적으로 controversial한 댓글은 공감:비공감이 1:1이 아닌 좀더 공감 비율이 높은 비율을 가진다는 사실을유추해볼 수 있다.</p><p>공감비율과 비슷하게 controversial도 vote수가 많은 경우에 불리해진다.controversial의 분모는 upvote와 downvote의 차이값인데, vote수가 많을수록한두개차이를 유지하기가 어려워진다. 공감 66, 비공감 57을 가진 댓글이공감 10, 비공감 10보다 아래에 놓인다.</p><h2 id="new-sorting-algorithms">New Sorting Algorithms</h2><p>reddit ranking algorithm 중에서 controversial의 문제점을 해결한새로운 controversial algorithm과 비공감이 많은 의견도 노출하는 best anti정렬방식을 제안하고자 한다.</p><h3 id="new-controversial">New controversial</h3><p>앞서 지적했듯이 controversial은 공감: 비공감의 비율 재조정과 vote수가 많은 경우 분모값의 기준을 완화시켜야하는 이슈가 있다.</p><ul><li><p><strong>공감 : 비공감</strong> 정성적으로 확인해보았을 때 공감:비공감 = 6.5 : 3.5 정도에서 기사 내용에 대한 찬성과 반대의 댓글이 골고루등장하였다. 때문에 new controversial에서 upvote와 downvote의 값을조정해주어야 한다.</p></li><li><p><strong>vote수가 많은 경우</strong> 이 문제는 공감비율순과비슷했다. upvote와 downvote의 절대치에 의존하기보다 wilson score로도출된 값을 upvote와 downvote로 대체하면 vote수가 많고 적음을고려하면서도 0과 1 사이의 값을 가지게 되어 upvote와 downvote의 차이에대한 효과가 완화된다.</p></li></ul><p>변경된 내용을 정리하면 다음과 같다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">controversial</span><span class="params">(upvote, downvote)</span>:</span></span><br><span class="line">    p_up = best(upvote, downvote) * <span class="number">3.5</span></span><br><span class="line">    p_down = best(downvote, upvote) * <span class="number">6.5</span></span><br><span class="line">    match = min(p_up, p_down)</span><br><span class="line">    top = match * math.log(match + <span class="number">1</span>)</span><br><span class="line">    bottom = abs(p_up - p_down) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> float(top) / bottom</span><br></pre></td></tr></table></figure><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론 곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 19%"><col style="width: 38%"><col style="width: 19%"><col style="width: 23%"></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 11</td><td><span style="color:red">아베한테 대하듯 똑같이 김정은하고 북한,중국한테도 당당하게 나와라! </span></td><td>16</td><td>11</td></tr><tr class="even"><td>user 12</td><td><span style="color:red">개~~새끼 아베 한테는 그렇게 당당하면서김정은한테는 왜 그렇게 꼬리를 내린다냐? 핵이 무섭긴 무서운가보다</span></td><td>11</td><td>7</td></tr><tr class="odd"><td>user 13</td><td><span style="color:blue">한미 동맹도 좋다 그러나 우리 나라 스스로강한 나라가 되어야 한다. 문대통형 수고 많으십니다 !!</span></td><td>9</td><td>6</td></tr><tr class="even"><td>user 14</td><td><span style="color:blue">아베에게 일침을 놔주신문 대통령님 지지합니다.나대지 마시오</span></td><td>9</td><td>6</td></tr><tr class="odd"><td>user 15</td><td><span style="color:blue">쪽바리 추종자들 많네!! 특히 벌레틀딱들~~</span></td><td>8</td><td>5</td></tr><tr class="even"><td>user 16</td><td><span style="color:blue">반대로 우리나라가 일본보고 자위대훈련하는거 보고 참견하면 일본이 가많이 있겠냐?벌레들아! 비판을 하려면국내 내정에 간섭하는 아베를 비판해야지 아베를 두둔하냐? 이스레기들아...</span></td><td>8</td><td>5</td></tr><tr class="odd"><td>user 17</td><td><span style="color:red">아베가 옳은말했네 지금이라고 김정은 참수한미연합훈련을 시작하라 빨갱이한테 이 나라를 줄 수 없다 </span></td><td>6</td><td>4</td></tr><tr class="even"><td>user 18</td><td><span style="color:red">대한민국은 다시 한번 망해봐야정신차리지..안된다.</span></td><td>6</td><td>4</td></tr><tr class="odd"><td>user 19</td><td><span style="color:blue">일본이 우방이란애들 멍청한거 아니냐일본애들도 그렇게 생각안하는데 왜 니혼자 망상해찐따새끼인가ㅋㅋㅋㅋㅋㅋ</span></td><td>6</td><td>4</td></tr><tr class="even"><td>user 20</td><td><span style="color:red">문재인씨 당신의 국적은 어디입니까? 다스실소유주를 밝히는 것보다 훨씬 더 중요한 문제입니다. </span></td><td>6</td><td>4</td></tr></tbody></table><p>공감 비율을 조금 높여주었을 때 기사 내용에 찬성하는 댓글과 반대하는댓글이 top 10에 골고루 섞이게 되었다. 또 wilson score로 변환한 상태에서비율을 조정해주게되어 vote수가 높은 경우에 up과 down의 차이에 덜민감해질 수 있었다.</p><h3 id="best-anti">Best-Anti</h3><p>꼭 공감수가 많은 것만 괜찮은 의견이라고 볼 수 있을까? 비공감수가 많은의견 또한 반대 진영의 입장을 대변하는 좋은 의견이라고도 볼 수 있지않을까?</p><p>네이버 뉴스 댓글은 대부분 당시의 여론에 따라 분위기가 흘러간다.순공감순이든 공감비율순이든 한가지 주장을 다른 방식으로 표현하고 있는댓글들이 top 10이 된다. 이를 보는 대중은 한쪽의 영향만 받게 되어 생각이더욱 치우쳐진다.</p><p>정치적 다양성을 수용하는 것은 의견의 객관성을 유지하는데에 도움이된다. 그런 의미에서 당시 여론과 반대되는 내용의 댓글 또한 보여주는 것은댓글에 영향을 받을 다른 사용자를 위해서도, 플랫폼의 중립성을 담보하기위해서도 중요하다고 생각한다.</p><p>Best-Anti는 negative vote에 대한 Wilson score를 구한 것이다.</p><p>\[ w_{neg}^- = max(0, \frac{2n(1-\hat{p}) + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)}) \]</p><p>python 구현식은 다음과 같다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">best_anti</span><span class="params">(up, down)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        z = <span class="number">1.96</span>  <span class="comment"># 95% confidence level</span></span><br><span class="line">        n = up + down</span><br><span class="line">        p_up = up / n</span><br><span class="line">        p_down = <span class="number">1</span> - p_up</span><br><span class="line">        denominator = <span class="number">2</span> * (n + z**<span class="number">2</span>)</span><br><span class="line">        numerator = <span class="number">2</span> * n * p_down + z**<span class="number">2</span> - z * np.sqrt(z**<span class="number">2</span> + <span class="number">4</span> * n * p_up * p_down)</span><br><span class="line">        lower = numerator / denominator</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">        lower = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> max(<span class="number">0</span>, lower)</span><br></pre></td></tr></table></figure></p><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 15%"><col style="width: 50%"><col style="width: 15%"><col style="width: 18%"></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 21</td><td>평화협정후 미군철수 바랍니다</td><td>0</td><td>5</td></tr><tr class="even"><td>user 22</td><td>홍발정씨..트럼프도 좌파 빨갱이죠??</td><td>0</td><td>4</td></tr><tr class="odd"><td>user 23</td><td>늙다리 미치광이는 빠져 줄래!자주통일좀 하자!</td><td>0</td><td>4</td></tr><tr class="even"><td>user 24</td><td>자국당은 사형감 많던데... 미국철수 애기했다고 파면? 자국당 5월에는문정인으로 놀고먹겠군~!</td><td>0</td><td>4</td></tr><tr class="odd"><td>user 25</td><td>봐라. 지도자 하나가 이렇게나 세상을 바꿀 수 있다. 물론 촛불 들고,직접민주주의를 구현한 국민 또한 위대하지. 지방선거 때 투표 잘 하자.</td><td>0</td><td>4</td></tr><tr class="even"><td>user 26</td><td>아직도. 미국이 인계철선이라믿고 50년대 사고방식이 존재하는구나군사력 세계10위안에들고 1-1붙어도 안지니 너무 미군철수로 여론전말고참신한거없어요 ? 자한당분들?</td><td>1</td><td>6</td></tr><tr class="odd"><td>user 27</td><td>극우 자한당은 미국도 빨갱이란다 제비가 왔다고 봄은 아니람서ㅋㅋㅋ</td><td>0</td><td>3</td></tr><tr class="even"><td>user 28</td><td>원샷-빅딜!</td><td>0</td><td>3</td></tr><tr class="odd"><td>user 29</td><td>자한당분들께서 트럼프도 좌파래요..</td><td>0</td><td>3</td></tr><tr class="even"><td>user 30</td><td>잊지마세요 지금도 북한은 세계 최악의 인권유린 국가입니다 이시간에도북한 주민들은 김정은한테 총살당하거나 아오지탄광으로 끌려가고 있습니다북한 여성들은 김정은의 성노예가 되고 있구요 대한한공 조현민의 갑질화가나죠 미투운동으로 드러난 권력자들의 성폭력 정말 싫습니다 그런데이것보다 수백배는 더심한 갑질과 성폭력을 일삼는게 북한 김정은입니다</td><td>0</td><td>3</td></tr></tbody></table><h2 id="conclusions">Conclusions</h2><p>현재의 네이버 뉴스 댓글 정렬방식은 공감수가 높은 댓글을 위주로보여주고 있고, 기준 또한 쉽다. 조작에 들어가는 비용 대비 얻을 수 있는효과가 큰 상황에서 조작으로 인해 이익을 볼 집단은 당연히 어뷰징을 할 수밖에 없다. 그리고 이미 조직적인 세력이 되어버린 어뷰저들은 완벽히 차단할수 없다. 때문에 어뷰징을 해결할 수 있는 가장 좋은 방법은 현재의 정렬방식의 단점을 극복하면서 자연스럽게 기준이 복잡해지게 만드는 것과사람들이 조작된 의견에 크게 흔들리지 않을 수 있도록 다양한 의견을보여주는 것이다.</p><p>현재의 네이버 정렬 방식 중 순공감순과 공감비율순이 가지는 한계는reddit에서 사용하고 있는 best 정렬방식으로 해결된다. 공감수에 가중치를둔 정렬방식 외에 공감수와 비공감수가 비슷한 댓글에 가중치를 두는 방식,비공감수에 가중치를 두는 방식을 제안하였다.</p><p>한 쪽의 의견만 듣는 것은 언제나 편향된 결과를 야기한다고 생각한다. 한쪽이 명백히 잘못한 것처럼 보도될 때, 그 반대의 의견에도 귀를 기울일 수있는 플랫폼이 되길 바란다.</p><h2 id="future-works">Future works</h2><p>지금까지는 댓글의 contents보다는 댓글에 부과된 공감, 비공감의interaction 데이터로 문제점과 해결방식을 제안했다. controversial로의견의 다양성을 추구했지만 text를 보지 않았기 때문에 의견의 다양성을간접적으로 보장하기엔 불안정할 수 있다.</p><p>쇼핑 리뷰에서 가격, 내구성, 디자인 등 다양한 측면을 보여주듯이 정치적의견도 기사에서 언급된 중요한 단어들에 대한 사람들의 반응을 보는 방식도생각해보면 좋을 것 같다.</p><h2 id="appendix-a-wilson-score">Appendix A: Wilson score</h2><p>사실 본문에서 기술한 내용은 일반적인 Normal approximationinterval이다.</p><p>\[ p = \hat{p} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \]</p><p>여기서 \(\)은 Bernoulli process의 성공확률을 의미한다.</p><p>Wilson score는 confidence interval을 \(\)가 아닌 \(p\)로 추정한 scoreinterval의 최소값이다.</p><p>\[ p = \hat{p} \pm z\sqrt{\frac{p(1-p)}{n}} \]</p><p>\(p\)에 대해 정리하여 \(p\)에 대한 2차방정식을 만든다.</p><p>\[ (1 + \frac{z^2}{n}) p^2 - (2\hat{p} + \frac{z^2}{n})p + \hat{p}^2= 0 \]</p><p>근의 공식을 사용해 \(p\)를 구한다.</p><p>\[ p = \frac{2n\hat{p} + z^2 \pm z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)} \]</p><p>Wilson score는 \(p\)의 lower bound이므로 - 에 대해 정리하면 다음과같다.</p><p>\[ w^- = max(0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)}) = \text{wilson score} \]</p><p>95%의 신뢰도로 고정하는 경우 z에 1.96을 대입할 수 있다. 그리고 이경우 본문의 python 함수에서 구현한 <code>best</code>가 된다.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">2017년11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의호감순처럼 호감도를 “공감-비공감”으로 계산하게되었다.<a href="#fnref:1" rev="footnote"> ↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">홍준표“나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버뉴스<a href="#fnref:2" rev="footnote"> ↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">재판에넘겨진 조국 부인 정경심 교수…검찰 '소환 임박', 네이버뉴스<a href="#fnref:3" rev="footnote"> ↩︎</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">대학교수이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버뉴스<a href="#fnref:4" rev="footnote"> ↩︎</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system" rel="external nofollow noopener noreferrer" target="_blank">https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system</a><a href="#fnref:5" rev="footnote">↩︎</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html" rel="external nofollow noopener noreferrer" target="_blank">http://www.evanmiller.org/how-not-to-sort-by-average-rating.html</a><a href="#fnref:6" rev="footnote">↩︎</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval" rel="external nofollow noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval</a><a href="#fnref:7" rev="footnote">↩︎</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is" rel="external nofollow noopener noreferrer" target="_blank">https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is</a><a href="#fnref:8" rev="footnote">↩︎</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">MB'정치보복' 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버뉴스<a href="#fnref:9" rev="footnote"> ↩︎</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence" rel="external nofollow noopener noreferrer" target="_blank">https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence</a><a href="#fnref:10" rev="footnote">↩︎</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">아베“한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버뉴스<a href="#fnref:11" rev="footnote"> ↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은
불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은
내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까?&lt;/p&gt;
&lt;p&gt;네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을
상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연
정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할
의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱
획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어
무의식적으로 다양한 사고에 대한 가능성을 차단받는다.&lt;/p&gt;
&lt;p&gt;그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sorting
algorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을
비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고
비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다.
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="Data Analysis" scheme="https://inmoonlight.github.io/categories/ML/Data-Analysis/"/>
    
    
      <category term="data analysis" scheme="https://inmoonlight.github.io/tags/data-analysis/"/>
    
      <category term="news comments" scheme="https://inmoonlight.github.io/tags/news-comments/"/>
    
      <category term="social good" scheme="https://inmoonlight.github.io/tags/social-good/"/>
    
  </entry>
  
  <entry>
    <title>노르웨이에서의 나홀로 여행: 피오르드, 브라운 치즈</title>
    <link href="https://inmoonlight.github.io/2019/08/09/Travel-to-Norway-fjord-brown-cheese/"/>
    <id>https://inmoonlight.github.io/2019/08/09/Travel-to-Norway-fjord-brown-cheese/</id>
    <published>2019-08-09T13:26:00.000Z</published>
    <updated>2023-11-02T02:50:49.148Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>북유럽은 오로라를 보고 싶어서 겨울에 갈 곳으로 내심 정해두고있었는데, 그럼에도 불구하고 한 여름에 노르웨이를 행선지로 정했던 까닭은<strong>피오르드(Fjord)</strong> 였다. 웅장한 자연을 보길 좋아하는편인데다가 (오로라만 봐도 알 수 있다) 덥고 습한 여름에서, 그리고 틀에박힌듯한 답답한 삶에서 잠시 벗어나고 싶었다.</p><a id="more"></a><p><img src="/assets/images/fjords.png?style=centerme" width="75%" alt="노르웨이의 피오르드들"><br></p><p>좀 더 찾아보니 노르웨이는 피오르드의 천국이었다. 대부분의 관광객이찾는 피오르드이자 가장 길고깊은<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="적어도 나의 가이드에 따르면, 가장 긴 피오르드는 사실 Greenland에 있는 피오르드라고 한다. 하지만 사람이 살지 않기 때문에 *안* 쳐준다고...">[1]</span></a></sup> 송네피오르드(Sognefjord), 영화&lt;<겨울왕국>&gt;의 배경이자 그 아름다움으로 UNESCO 세계 유산에 지정된내로이피오르드(Nærøyfjord), 베르겐(Bergen)에서 플럼(Flåm)으로 가는 길에송네피오르드를 거쳐 지나는Aurlandsfjord<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="한국어로 어떻게 표기하는 것이 옳은지 몰라 영어로 남겨두었다.">[2]</span></a></sup> 등등. "피오르드"가 빙하로 만들어진 좁고 깊은 만을뜻하다보니 노르웨이 곳곳에 피오르드로 끝나는 이름이 많았다. 나는 이번여행에서 베르겐을 들르지 않고 플럼에서만 머무르기로 했기 때문에 <a href="https://www.fjordsafari.com/activities/may-september/heritage-taste-fjordsafari" rel="external nofollow noopener noreferrer" target="_blank">Aurlandsfjord에서내로이피오르드(Nærøyfjord)를 거쳐 운드레달(Undredal)에서 브라운 치즈를체험할 수 있는 투어</a>를 신청했다.</겨울왕국></p><p><img src="/assets/images/fjord_guide.JPG?style=centerme" width="65%" alt="열심히 설명 중인 가이드"><br></p><p>ferry 투어와는 다르게 RIB(Rigid Inflated Boat) 투어는 야생의 투어에가까웠다. 보트의 운전을 담당하는 가이드는 때때로 ferry가 가르고 간 물결위를 지나며 스릴감을 주기도 하고, 피오르드 협곡에 사는 작은 고래를만나면 근처에서 구경을 시켜주기도 했다. 이에 더해 그 지역에 대한친절하고 자세한 설명도 함께 들을 수 있어서 노르웨이가 더 가깝게느껴졌다.</p><p><img src="/assets/images/naeroyfjord.jpg?style=centerme" width="75%" alt="내로이피오르드(Nærøyfjord)와 저 멀리 보이는 구드방겐(Gudvangen)"><br></p><p>피오르드는 정말 아름다웠다. 특히 내로이피오르드(Nærøyfjord)는 협곡의곡선이 섬세했고 조화로웠다. 아름다움과 실용성은 반비례한다고생각해왔는데, 피오르드도 마찬가지였다. 보기에는 아름답고 경이롭지만, 그곳에서 살아가기 위해서는 비옥한 평지에서 살아가는 사람들에 비해 많은노력이 필요했다. 특히 "식"을 해결하는 것이 가장 어려운 문제였다.노르웨이는 경작할 수 있는 토지의 비율이 작다. 특히 피오르드 근처는 그비율의 1/10이다. 이런 척박함 덕분에 석유의 발견으로 지금과 같이 부유한국가가 되기 전에는 많은 사람들이 미국으로 이민을 갔다고 한다.</p><p><img src="/assets/images/undradel.jpg?style=centerme" width="75%" alt="아렌델(Arendelle)의 배경이 되었다고 하는 운드라델(Undradel)"><br></p><p>남은 사람들은 농업대신 낙농업으로 생계를 유지해 나갔다. 이 곳의낙농업은 다른 곳에 비해 특이한 점이 있었다. 하나는 염소 치즈가 주를이룬다는 점, 다른 하나는 브라운 치즈라는 독특한 치즈가 발전되었다는점.</p><p><img src="/assets/images/goats.jpg?style=centerme" width="65%" alt="험준한 피오르드를 알아서 잘 타는 염소떼들"><br></p><p>염소 치즈가 소 치즈보다 발달한 이유는 염소가 산을 잘 타기 때문이다.추운 지방이다 보니 여름과 겨울에 풀을 먹을 수 있는 면적이 눈에 띄게달라진다. 눈이 녹을 시기에는 그 기회를 십분 활용해 좀 더 위에서 풀을먹는 것이 이득이다. 다행히 염소의 천적은 이 추위를 견디며 살 수 없어서염소 떼를 풀어두기만 하면 알아서 산을 올라 제일 맛있는 풀을 알아서뜯어먹고 안전히 마을로 귀가한다고 한다.</p><p><img src="/assets/images/brown_cheese.jpg?style=centerme" width="65%" alt="브라운 치즈의 모습. 색이 정말 갈색이다."><br></p><p>브라운 치즈는 우리가 흔히 아는 화이트 치즈의 잔여물로 만든 독특한치즈다. 아무리 낙농업이 발달했다고 하더라도 그 양이 충분하지 않았기때문에 사람들은 일반 화이트 치즈를 만들고 남은 것도 식량으로 활용해야했다. 잔여물은 "유청"이라고 불리는 것인데 이를 충분히 졸이면 갈색으로변하고 카라멜처럼 단 맛이 난다. 여기에 얼마 되지 않는 소의 젖으로 만든크림을 1:1 비율로 섞어주면 브라운 치즈가 된다.</p><p><img src="/assets/images/cows.jpg?style=centerme" width="65%" alt="힙하다."><br></p><p>화이트 치즈를 만들 때 지방이 많이 빠져나가다보니 브라운 치즈는 지방이덜 함유되어 있다. 브라운 치즈는 노르웨이에서 많이 파는 크래커같은 빵에버터를 발라 그 위에 얹고, 그 지방의 잼을 발라먹는다. 마트에서 파는브라운 치즈는 인공적인 향이 가미되어 있어 좀 더 단 맛이 많이 난다.</p><p><img src="/assets/images/cheese_jam.jpg?style=centerme" width="65%" alt="운드레달(Undredal)에서 제공했던 잼(blueberry, cloudberry, lingonberry)과 다양한 숙성도의 염소치즈, 브라운 치즈, 빵"><br></p><p>내로이피오르드의 끝에는 구드방겐(Gudvangen)이라는 마을이 있었고,운드레달(Undredal)과는 달리 바이킹 족의 전통을 간직하고 있었다. 집의지붕은 풀이덮어져있었고<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="나의 가이드에 따르면, 풀을 덮은 이유는 지붕을 더 단단히 엮기 위함과 아름다움(?!) 때문이라고 한다. ">[3]</span></a></sup>, 어린 아이들이 (특히 남자) 낯선 이를 따라가지못하도록 "트롤이 너를 잡아먹는다!"며 겁을 줬다고 한다.</p><p><img src="/assets/images/gudvangen.jpg?style=centerme" width="75%" alt="구드방겐(Gudvangen) 마을의 전경"><br></p><p>놀랍게도 이 트롤은 노르웨이의 유명한 극작가 헨릭크 입센의 소설&lt;<페르귄트>&gt;에 처음으로 등장했다고 한다. 주인공 페르 귄트가 산속에서 판타지스러운 여정을 할 때 초록 옷을 입은 여자가 나타나 그를유혹했는데 알고보니 트롤의 딸이었다고 한다. 그 이후 트롤은 판타지 소설속에서 괴기스럽게 발전되기도 하고, &lt;<겨울왕국>&gt;에 등장하는트롤처럼 귀여운 모습으로도 발전했다.</겨울왕국></페르귄트></p><p>생각보다 노르웨이 사람들의 입센 사랑은 엄청났다.릴리함메르(Lillehammer)라는 지역에서는 매년 8월 초마다&lt;&lt;페르귄트(Peer Gynt)&gt;&gt; 페스티벌을 연다. 노르웨이의 자연을배경으로 야외에서 하는 연극이 가장 인기가 많다. 그리그의&lt;<페르 귄트 모음곡>&gt;으로만 접했던 페르 귄트의 이야기가 궁금해서,그리고 그들 고유의 축제가 궁금해서, 나도 한 번 연극 티켓을 구매했다.이에 대한 내용은 언젠가...</페르></p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">적어도나의 가이드에 따르면, 가장 긴 피오르드는 사실 Greenland에 있는피오르드라고 한다. 하지만 사람이 살지 않기 때문에 <em>안</em>쳐준다고...<a href="#fnref:1" rev="footnote"> ↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">한국어로어떻게 표기하는 것이 옳은지 몰라 영어로남겨두었다.<a href="#fnref:2" rev="footnote"> ↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">나의가이드에 따르면, 풀을 덮은 이유는 지붕을 더 단단히 엮기 위함과아름다움(?!) 때문이라고 한다.<a href="#fnref:3" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;북유럽은 오로라를 보고 싶어서 겨울에 갈 곳으로 내심 정해두고
있었는데, 그럼에도 불구하고 한 여름에 노르웨이를 행선지로 정했던 까닭은
&lt;strong&gt;피오르드(Fjord)&lt;/strong&gt; 였다. 웅장한 자연을 보길 좋아하는
편인데다가 (오로라만 봐도 알 수 있다) 덥고 습한 여름에서, 그리고 틀에
박힌듯한 답답한 삶에서 잠시 벗어나고 싶었다.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Essay" scheme="https://inmoonlight.github.io/categories/Essay/"/>
    
    
      <category term="travel" scheme="https://inmoonlight.github.io/tags/travel/"/>
    
      <category term="norway" scheme="https://inmoonlight.github.io/tags/norway/"/>
    
  </entry>
  
  <entry>
    <title>Naver News Comment Analysis (2)</title>
    <link href="https://inmoonlight.github.io/2019/08/03/Naver-News-Comments-Analysis-(2)/"/>
    <id>https://inmoonlight.github.io/2019/08/03/Naver-News-Comments-Analysis-(2)/</id>
    <published>2019-08-03T08:59:00.000Z</published>
    <updated>2023-11-02T02:50:49.146Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><span style="color:red">NOTICE: 앞으로 소개될 내용은 NAVER와무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을알립니다.</span></p><h2 id="tldr">TL;DR</h2><p>2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다.</p><ol type="1"><li>타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.</li><li>발생하기 어려운 패턴을 보여야 한다.</li></ol><p>이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다.<a id="more"></a></p><h2 id="abuser-who-are-you">Abuser, who are you?</h2><h3 id="introduction">Introduction</h3><p><strong>"정말 2016년 4분기부터 정말 댓글 조작을 했던 사용자들이있었을까?"</strong> 라는 단순한 의문과 궁금증에서 분석을 시작하게되었다. 다만, 데이터에 <공감> <비공감> 을 눌렀던 interaction 정보가누락되어 있기에 (이 데이터는 네이버 뉴스 측에서 제공해주지 않는 이상얻을 수 없다) 적어도 댓글을 한 번이라도 남겼던 사용자에 대해서만어뷰저로 의심해 볼 수 있었다.</비공감></공감></p><p>label이 없는 상황에서 어뷰저를 특정짓는 것과 그 사용자가 어뷰저임을다른 사람에게 설득하는 것은 어려운 일이다. 또한 무죄추정의 원칙에 의거해댓글 작성자를 함부로 어뷰저라고 단정지을 수도 없었다. 그래서 이번분석에서는, "모든 작성자는 어뷰저가 아니다." 라는 가정을 기반으로 특정패턴이 등장할 확률을 계산해서 <strong>어뷰저였을 가능성</strong>을간접적으로 추측하는 방식을 취했다.</p><p>이 과정에서 누군가는 그 정도 확률로는 어뷰저라고 단정짓기 어렵다고판단할 수도 있고, 아닐 수도 있다. 또 추가적인 분석 결과가 있다면 어뷰저가능성이 더 높아질 수도 있다. 후자라면 언제든 댓글로 추가 분석할 내용을요청했으면 하는 바람이다.</p><h3 id="abuser-criteria">Abuser Criteria</h3><p>어뷰저는 어떤 존재일까? 이에 답하기 앞서, 어뷰징의 목적과 어뷰징이문제가 되는 상황에 대해 먼저 정리해보았다.</p><h4 id="어뷰징의-목적">어뷰징의 목적</h4><p>어뷰저들의 목표는 네이버의 댓글 정렬 기준에 맞추어 10위권 내에 드는것이다. 네이버 뉴스의 UI 상, top 10 내에 들면 그 기사를 읽는 누구나 쉽게그 댓글의 내용에 접하게 되기 때문이다. 그리고 그 내용이 대중을대표한다고 생각하기 때문에 쉽게 타인의 생각에 영향을 미칠 수 있다.</p><h4 id="어뷰징이-문제가-되는-상황">어뷰징이 문제가 되는 상황</h4><p>어뷰징이 문제가 되었던 이유는 공정하고 자연스러운 방식으로집계되었다고 믿었던 top 10 댓글이 실제로는 어떤 세력에 의해 의도를가지고 조작되었기 때문이었다. top 댓글이 특정 집단에 의해 조작되었다면,그것들이 과연 네이버 뉴스 플랫폼에 참여하는 사용자들의 생각을 대표하는댓글이라고 볼 수 있을까?</p><p>그래서, 이 글에서 이야기 할 어뷰저의 criteria는 다음과 같다.</p><blockquote><p>어뷰징의 목적을 달성해야 한다. 즉, 타인의 생각에 영향을 미칠 수있도록 <strong>작성한 댓글이 top 10 내에 한 번 이상 들었어야한다</strong>. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.<del><em>(안습)</em></del></p></blockquote><blockquote><p>자연적으로 발생하기 어려운, <strong>확률이 낮은 패턴이등장</strong>해야 한다.</p></blockquote><h3 id="data-preprocessing">Data preprocessing</h3><p>에서 사용했던 데이터에서 추가로 필터링이 필요했다. 크롤링한 댓글데이터에 hashing 된 아이디가 포함된 것이 2015년 12월 이후였기때문이다.</p><ul><li>사용한 댓글 데이터 기간: 2015.12.08 ~ 2018.05.25</li></ul><h3 id="analysis">Analysis</h3><p>먼저, <strong>정치</strong> 분야에서 댓글이 top 10 내에 들었던 횟수를작성자 별로 집계한 후, 횟수가 높은 순서대로 정렬하였을 때의 추이를살펴보았다.</p><p><img src="/assets/images/politics_top_user.png?style=centerme" width="50%"><br></p><table><thead><tr class="header"><th>mean</th><th>stdev</th><th>max</th><th>75%</th><th>med</th><th>min</th></tr></thead><tbody><tr class="odd"><td>2.240442</td><td>4.607621</td><td>369</td><td>2</td><td>1</td><td>1</td></tr></tbody></table><p><br> 대부분의 작성자는 1~2번 정도 댓글이 top 10 내에 드는 반면, 일부사용자들은 100번 이상 순위권 내에 든다. 이 그래프만 본다면 자주 top 10에드는 사용자들 모두가 의심스러울 수 있지만 이런 skewed graph는 대부분의사회과학 데이터에서 발견되므로 이들을 어뷰저로 속단하긴 이르다. 검증을위해 다른 섹션(사회, 경제, 문화, IT, 세계)에 대해서도 마찬가지 방법으로그래프를 그려보았다.</p><figure><img src="/assets/images/paretto.png" alt="갓 파레토..."><figcaption aria-hidden="true">갓 파레토...</figcaption></figure><p>자주 순위권 내에 드는 댓글을 작성한 사용자를 <strong><em>topuser</em></strong> 라고 했을 때, 다른 분야에서도 top user는 쉽게 찾아볼수 있었다. 어쩌면 이들은 (어뷰저가 아닌 이상) 네이버 뉴스 플랫폼에서높은 "공감수-비공감수"를 받을 수 있는 전략이 학습된 것은 아닐까? 기사가나오고 얼마 지나지 않아 댓글을 남기거나, 그 당시 분위기에 맞는 댓글의내용을 남기거나, 사실로 보여지는 데이터와 함께 댓글을 작성하거나 하는 등자신만의 전략이 있을 것이다.</p><p>그러나 이 전략들이 100%의 확률로(=항상) 통하지는 않았을 것이다.때로는 일찍 댓글을 작성했음에도 뒤늦게 작성한 댓글이 폭발적인 공감을이끌어내서 top 10에 들지 못했을 수도 있고, 당시의 전반적인 분위기에탑승하는 댓글을 남겼음에도 다른 댓글 중에 두드러지지 못해 공감을 받지못했을 수도 있다.</p><p>top user 간에 일반적인 top 10 성공률이 존재할 것이고 이는 normaldistribution을 따른다는 가설을 바탕으로 "top user가 작성한 전체 댓글 수대비 top 10에 들었던 댓글 수(=top 10 성공률)"를 계산해보았다.</p><h4 id="정치-top-users">정치 top users</h4><table><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td><strong>user 1</strong></td><td>369</td><td>386</td><td><strong>95.60</strong></td></tr><tr class="even"><td>user 2</td><td>339</td><td>380</td><td>89.21</td></tr><tr class="odd"><td><strong>user 3</strong></td><td>269</td><td>289</td><td><strong>93.08</strong></td></tr><tr class="even"><td>user 4</td><td>178</td><td>610</td><td>29.18</td></tr><tr class="odd"><td>user 5</td><td>178</td><td>1090</td><td>16.33</td></tr><tr class="even"><td>user 6</td><td>175</td><td>424</td><td>41.27</td></tr><tr class="odd"><td>user 7</td><td>174</td><td>818</td><td>21.27</td></tr><tr class="even"><td>user 8</td><td>155</td><td>316</td><td>49.05</td></tr><tr class="odd"><td>user 9</td><td>143</td><td>950</td><td>15.05</td></tr><tr class="even"><td>user 10</td><td>141</td><td>583</td><td>24.19</td></tr></tbody></table><p><br></p><h4 id="경제-top-users">경제 top users</h4><table><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 11</td><td>289</td><td>1185</td><td>24.39</td></tr><tr class="even"><td>user 12</td><td>226</td><td>2935</td><td>7.70</td></tr><tr class="odd"><td>user 13</td><td>219</td><td>1656</td><td>13.22</td></tr><tr class="even"><td>user 14</td><td>183</td><td>1636</td><td>11.19</td></tr><tr class="odd"><td>user 15</td><td>173</td><td>1378</td><td>12.55</td></tr><tr class="even"><td>user 16</td><td>161</td><td>989</td><td>16.28</td></tr><tr class="odd"><td>user 17</td><td>160</td><td>654</td><td>24.46</td></tr><tr class="even"><td>user 18</td><td>157</td><td>2589</td><td>6.06</td></tr><tr class="odd"><td>user 19</td><td>139</td><td>1514</td><td>9.18</td></tr><tr class="even"><td>user 20</td><td>127</td><td>742</td><td>17.12</td></tr></tbody></table><p><br></p><h4 id="사회-top-users">사회 top users</h4><table><colgroup><col style="width: 14%"><col style="width: 26%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 21</td><td>366</td><td>953</td><td>38.41</td></tr><tr class="even"><td>user 22</td><td>308</td><td>1636</td><td>18.83</td></tr><tr class="odd"><td>user 23</td><td>271</td><td>935</td><td>28.98</td></tr><tr class="even"><td>user 24</td><td>241</td><td>1254</td><td>19.22</td></tr><tr class="odd"><td>user 25</td><td>233</td><td>1656</td><td>14.07</td></tr><tr class="even"><td>user 26</td><td>204</td><td>328</td><td>62.20</td></tr><tr class="odd"><td>user 27</td><td>191</td><td>719</td><td>26.56</td></tr><tr class="even"><td>user 28</td><td>168</td><td>625</td><td>26.88</td></tr><tr class="odd"><td>user 29</td><td>149</td><td>1190</td><td>12.52</td></tr><tr class="even"><td>user 30</td><td>148</td><td>1489</td><td>9.94</td></tr></tbody></table><p><br></p><h4 id="문화-top-users">문화 top users</h4><table><colgroup><col style="width: 14%"><col style="width: 26%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 31</td><td>373</td><td>1636</td><td>22.80</td></tr><tr class="even"><td>user 32</td><td>367</td><td>935</td><td>39.25</td></tr><tr class="odd"><td>user 33</td><td>301</td><td>1417</td><td>21.24</td></tr><tr class="even"><td>user 34</td><td>243</td><td>890</td><td>27.30</td></tr><tr class="odd"><td>user 35</td><td>220</td><td>1656</td><td>13.29</td></tr><tr class="even"><td>user 36</td><td>188</td><td>1943</td><td>9.68</td></tr><tr class="odd"><td>user 37</td><td>178</td><td>3245</td><td>5.49</td></tr><tr class="even"><td>user 38</td><td>172</td><td>2738</td><td>6.28</td></tr><tr class="odd"><td>user 39</td><td>164</td><td>200</td><td>82.00</td></tr><tr class="even"><td>user 40</td><td>151</td><td>719</td><td>21.00</td></tr></tbody></table><p><br></p><h4 id="it-top-users">IT top users</h4><table><colgroup><col style="width: 14%"><col style="width: 26%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 41</td><td>714</td><td>3123</td><td>22.86</td></tr><tr class="even"><td>user 42</td><td>572</td><td>3886</td><td>14.72</td></tr><tr class="odd"><td>user 43</td><td>442</td><td>2287</td><td>19.33</td></tr><tr class="even"><td>user 44</td><td>399</td><td>1468</td><td>27.18</td></tr><tr class="odd"><td>user 45</td><td>231</td><td>810</td><td>28.52</td></tr><tr class="even"><td>user 46</td><td>234</td><td>3010</td><td>7.77</td></tr><tr class="odd"><td>user 47</td><td>275</td><td>1622</td><td>16.95</td></tr><tr class="even"><td>user 48</td><td>317</td><td>1493</td><td>21.23</td></tr><tr class="odd"><td>user 49</td><td>346</td><td>2349</td><td>14.73</td></tr><tr class="even"><td>user 50</td><td>364</td><td>1185</td><td>30.72</td></tr></tbody></table><p><br></p><h4 id="세계-top-users">세계 top users</h4><table><colgroup><col style="width: 14%"><col style="width: 26%"><col style="width: 29%"><col style="width: 29%"></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 51</td><td>237</td><td>1636</td><td>14.49</td></tr><tr class="even"><td>user 52</td><td>214</td><td>1432</td><td>14.94</td></tr><tr class="odd"><td>user 53</td><td>145</td><td>615</td><td>23.58</td></tr><tr class="even"><td>user 54</td><td>148</td><td>1709</td><td>8.66</td></tr><tr class="odd"><td>user 55</td><td>155</td><td>611</td><td>25.37</td></tr><tr class="even"><td>user 56</td><td>156</td><td>1076</td><td>14.50</td></tr><tr class="odd"><td>user 57</td><td>162</td><td>864</td><td>18.75</td></tr><tr class="even"><td>user 58</td><td>165</td><td>2778</td><td>5.94</td></tr><tr class="odd"><td>user 59</td><td>165</td><td>1254</td><td>13.16</td></tr><tr class="even"><td>user 60</td><td>175</td><td>575</td><td>30.43</td></tr></tbody></table><p><br></p><p>top user의 top 10 성공률을 확률 변수 X라고 했을 때의 histogram과 모든유저가 전략을 바탕으로 활동하는 그룹이라고 가정했을 때 Gaussiandistribution으로 추정한 확률 분포이다. (Gaussian Mixture Model로distribution fitting한 결과는 <a href="#Appendix-A-Gaussian-Mixture-Model-Fitting">Appendix A</a>참고)</p><p><img src="/assets/images/abuser-gaussian.png?style=centerme" width="80%" alt="mean: 25.0, std: 20.5"><br></p><p>정치 섹션에서만 유일하게 <code>ratio &gt; 90%</code> 인 topuser(<code>user 1</code>, <code>user 3</code>)가 존재했으며 이들의 top10 전략 성공률은 다른 top user 대비 발생하기 어려울 정도로 (0.0053%,0.0079%) 높다고 해석할 수 있다.</p><p>숫자 이면의 패턴을 보기 위해 전체 정치면 기사들의 댓글 수와<code>user 1</code>, <code>user 3</code>의 전체 댓글 수, top 10 내에 든댓글 수를 그래프로 시각화 해보았다.</p><ul><li>c.f. 2016.3 ~ 2018.5 까지 굵직한이슈들<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://ko.wikipedia.org/wiki/2016년_대한민국&gt;">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://ko.wikipedia.org/wiki/2017년_대한민국&gt;">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://ko.wikipedia.org/wiki/2018년_대한민국&gt;">[3]</span></a></sup><ul><li>이세돌 vs. 알파고 (2016.3)</li><li>옥시 (2016.4)</li><li>최순실 태블릿 pc (2016.10)</li><li>박근혜 탄핵 소추안 (2016.12)</li><li>사드배치 / 박근혜 수감 / 세월호 인양(2017.3)</li><li>19대 대통령 선거 (문재인 당선) (2017.5)</li><li>이대목동 신생아 사망 (2017.12)</li><li>평창 동계 올림픽 (2018.2)</li><li>이명박 수감 (2018.3)</li><li>드루킹 (2018.4)</li><li>남북1차정상회담 <span class="citation" data-cites="판문점">@판문점</span> (2018.4)</li><li>남북2차정상회담 (2018.5)</li></ul></li></ul><h4 id="user-1-ratio-96"><code>user 1</code> (ratio: 96%)</h4><p><img src="/assets/images/user1.png?style=centerme" width="90%"><br></p><p><span style="color:darkgray">회색 line</span>이 정치면 기사 댓글,<span style="color:blue">파란색 line</span>이 작성자가 쓴 전체 댓글 수,<span style="color:green">초록색 line</span>이 작성자가 쓴 댓글 중 top10 내에 들었던 댓글 수를 나타낸다.</p><p><code>user 1</code>이 주로 활동했던 시기는, 최순실 태블릿 pc 사건,박근혜 탄핵 및 19대 대통령 선거, 평창 동계올림픽 및 MB 다스 사건과맞물려 있었다. 댓글 내용을 시기 별로 뜯어보면, 다음과 같다.</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>title</th><th>article date</th><th>user top comments</th></tr></thead><tbody><tr class="odd"><td>朴대통령, '29일까지 대면조사' 檢 요청에 사흘째 묵묵부답</td><td>2016-11-25 15:12:00</td><td>대통령인 자가 자신의 관할하에 있는 검찰을 부정한다면 곧 국가도부정하겠다는 의미다. 이런 대통령은 더이상 대한민국 대통령이 아니다.</td></tr><tr class="even"><td>차은택 변호인 "차씨, 최순실 지시로 김기춘 실장 공관서 면담"</td><td>2016-11-27 16:04:00</td><td>김기춘의 진두지휘하에 박근혜 정권의 모든 불법들이 자행되었다. 정말악마같은 인간이다.</td></tr><tr class="odd"><td>변호인 "차은택, 崔 지시로 김기춘 만나…우병우 장모와골프도"(종합)</td><td>2016-11-27 16:46:00</td><td>김기춘의 진두지휘하에 박근혜 정권의 불법들이 자행되었다.구속수사해서 감옥에서 못나오게 만들어야 한다. 공작정치부터 공안탄압정경유착의 죄를 물어야 한다.</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>청문회장도 지배한 '촛불민심'…與野 '재벌 봐주기' 없었다</td><td>2016-12-06 12:34:00</td><td>뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈일이니까... 강요죄는 확실할듯...</td></tr><tr class="even"><td>재벌 총수들 "청와대 거절 어려워"…하나같이 대가성 부인</td><td>2016-12-06 12:45:00</td><td>뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈일이니까... 강요죄는 확실할듯...</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>박한철 前소장 한표, '캐스팅보트' 될뻔한 아슬아슬 상황 나올까</td><td>2017-03-04 08:00:00</td><td>탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에동참하겠다는 선언일 뿐이다.</td></tr><tr class="odd"><td>85시간 재판, 속기록 3000쪽…탄핵심판 이번주 결론날까</td><td>2017-03-05 09:00:00</td><td>탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에동참하겠다는 선언일 뿐이다.</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>[리얼미터] 다자 文 42.6% vs 安 37.2%…양자 文 47.6% vs 安 43.3%</td><td>2017-04-10 09:15:00</td><td>여론몰이에 흔들릴 국면이 아니다. 무쏘의 뿔처럼 나아가면 야합은흩어지고 굳건함이 승리할 것이다.</td></tr><tr class="even"><td>文 "김부겸 동지 미안하다…꼭 국민통합 해내겠다"</td><td>2017-04-22 08:01:00</td><td>김부겸의 진심이 느껴지고 그를 위로하고 뜻을 같이 하는 문재인의진심도 느껴진다. 남자들에게 이런 동지애는 죽음도 불사하게 만드는마력과도 깉다. 조~오타!!!</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>文 대통령 "내게 반대하라" 파격적 수석회의 시동(상보)</td><td>2017-05-25 12:56:00</td><td>요새 대통령의 행동과 지시사항을 보면 정말 준비된 겸손한 사람이란게진솔하게 느껴진다. 대한민국 국민인게 자랑스럽고 행복해진다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>文대통령 "사드 임시배치, 현재 정부가 취할 수 있는 최선의조치"(종합)</td><td>2017-09-08 21:13:00</td><td>국가의 지도자는 자신의 굳은 신념까지도 국가와 국민을 위해 잠시접어야할 용기가 필요할 때가있다. 그 지도자라고 왜 자신의 신념을 꺾음에자괴감과 고민이 없겠는가? 그는 자신을 지지하는 사람들만의 지도자가아니라 대한민국의 지도자이기 때문이다. 그의 고뇌찬 결단을 위로하며지켜보고 힘을 실어주고 싶다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>與 "안철수, 나라다운 나라 만드는 일 폄훼 말라"</td><td>2017-11-04 16:10:00</td><td>명버기 구하기에 혈안이 된 명바기 아바타!!!</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>김정은 위원장 "이른 시일내 만날 용의"…문 대통령에 방북요청(종합)</td><td>2018-02-10 15:56:00</td><td>남북 정상회담애서 허심탄회하게 모든 할 말 다해서 기필코 한반도비핵화와 평화를 이루어야 한다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표</td><td>2018-03-09 09:11:00</td><td>한반도 평화가 세계 평화다. 이런 평화 모드가 얼마만인가...</td></tr></tbody></table><ul><li>같은 내용의 댓글이 top 에 오른 것도 확인할 수 있었다.<ul><li>다른 섹션에 같은 댓글을 남기고도 top에 오른 적도 있다. (아래 표참고)</li></ul><table><colgroup><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"><col style="width: 25%"></colgroup><thead><tr class="header"><th>title</th><th>section</th><th>article date</th><th>top user comments</th></tr></thead><tbody><tr class="odd"><td>[현장영상] 박근혜 前 대통령 법원으로 출발</td><td>society</td><td>2017-03-30 10:18:00</td><td>최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의감시로부터 벗어날수 있다.</td></tr><tr class="even"><td>구속 갈림길에 선 박근혜 '웅변 대신 침묵' 선택</td><td>politics</td><td>2017-03-30 10:22:00</td><td>최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의감시로부터 벗어날수 있다.</td></tr></tbody></table></li></ul><p><br></p><h4 id="user-3-ratio-93"><code>user 3</code> (ratio: 93%)</h4><p><img src="/assets/images/user3.png?style=centerme" width="90%"><br></p><p><code>user 3</code> 의 주 활동 시기는 사드배치, 평창 동계올림픽 및북미회담과 맞물려있었다. 댓글을 자세히 보면 아래와 같다.</p><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>title</th><th>article date</th><th>user top comments</th></tr></thead><tbody><tr class="odd"><td>정상회담 돌발 변수는 '사드'…청 "모든 가능성 준비"</td><td>2017-06-25 20:20:00</td><td>국익과국가안보가최우선입니다~~~~</td></tr><tr class="even"><td>송영무 "사드, 비준 아닌 국회 검증…고액연봉·음주운전 송구"(종합)</td><td>2017-06-28 12:18:00</td><td>이유미녹취록에맛짱구치고놀아난언론은×</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>이유미-이준서 중 한 명은 거짓말…윗선 수사 불가피</td><td>2017-06-28 20:52:00</td><td>이유미녹취록에맛장구치고놀아난언론은~~~~????</td></tr><tr class="odd"><td>軍, 송영무 인사청문회서 공개된 '군사기밀 유출' 조사 착수</td><td>2017-06-29 12:24:00</td><td>자유당놈들답다도둑놈들</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>文대통령, 내일 트럼프와 만난다…취임 후 첫 韓美정상회담</td><td>2017-06-29 13:54:00</td><td>국익과국가안보가최우선입니다부디좋은결과있으시길~</td></tr><tr class="even"><td>[단독] '제보 조작' 수사망 좁혀오자 安 독대한 이준서…왜?</td><td>2017-06-29 20:19:00</td><td>철수야~ 깜빵갈시간이다가오네~~~~</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>트럼프, 文대통령 부부에 백악관 사적공간 '트리티 룸'깜짝공개(종합)</td><td>2017-06-30 13:23:00</td><td>문재인대통령님~ 멋저부러요~♡♡♡</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>한반도 이슈서 '주도권' 확보 성과…한미FTA 재협상 '숙제'(종합)</td><td>2017-07-01 10:05:00</td><td>국익과국가안보가최우선입니다부디좋은결과있으시길~~~~~~~~♡♡</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>남북 "4월말 정상회담 판문점서 개최"…특사단 발표(종합)</td><td>2018-03-06 20:24:00</td><td>이게 실화나ㅡ ㅡㅡㅡㅡ</td></tr><tr class="odd"><td>문 대통령 "서울·평양·판문점 중 北이 판문점 정상회담 선택"</td><td>2018-03-07 15:28:00</td><td>문통 지지 합니다</td></tr><tr class="even"><td>문 대통령 "국외 대북 비밀접촉 없어…저쪽에 놀아나는 것 아냐"</td><td>2018-03-07 16:53:00</td><td>문 대통 령님 지지 합니다</td></tr><tr class="odd"><td>文대통령 "북핵목표는 비핵화…제재완화, 지금은 불가능"(종합)</td><td>2018-03-07 17:16:00</td><td>문대통령님 지지합니다</td></tr><tr class="even"><td>[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표</td><td>2018-03-09 09:11:00</td><td>이게 실화냐ㅡㅡㅡㅡ</td></tr></tbody></table><h2 id="conclusions">Conclusions</h2><p>어뷰저를 <strong>타인의 생각에 영향을 미치고 비정상적인 행태를 보이는사용자</strong>로 정의하였고, 이 기준에 따라 어뷰저로 의심되는 사용자를찾아내고자 하였다.</p><ol type="1"><li>순공감 기준, 댓글이 10위권 내에 들었던 횟수가 많았던 작성자중에서</li><li>작성한 댓글 수 대비 top 10 댓글 수의 비율이 일반적이지 않은작성자</li></ol><p>개인적으로, <strong>1.</strong>과 <strong>2.</strong>의 기준에 드는사용자는 <code>user 1</code>, <code>user 3</code> 라는 생각이다. 작성한전체 댓글 수는 다른 사용자들에 비해 적은 편이었지만 top 댓글에 들었던비율은 가장 높았고, 그 수치가 일반적이지는 않았다.</p><p>어뷰저를 찾고자 시작한 분석이었지만 데이터를 살펴보면서 네이버 뉴스댓글이 가지는 단일하고 공개된 ranking system이 얼마나 위험한지를 오히려인식하게 되었다.</p><blockquote><p>분석한 기간에서 중복 제거한 기사의 수는 총 100,780개 였고, 만약 top10 댓글의 작성자가 모두 다른 사용자였다면 1,007,800명이 각자의 의견을개시했을 것이다. 하지만 실제 그 기간에 집계된 unique한 작성자는 총308,731 명에 불과했다. 게다가 중복 댓글까지 포함하면, 그 다양성은 조금더 떨어진다.</p></blockquote><p>이 같은 면에서 <strong>네이버 뉴스 댓글은 다양성을 충분히 수용하고있지 못하다는 생각이 들었다.</strong> "플랫폼이기 때문에 그럴 수 있지않을까?" 싶지만 페이스북이나 유튜브, 레딧같은 다른 플랫폼에서의 댓글을보면 무작정 호감순으로 정렬하지는 않는다. 이 플랫폼들의 기준이 문제가없다는 것은 아니다. 하지만 네이버 뉴스 보다 다양한 기준으로 댓글을정렬시키고 있으며 (최신순, 오래된 순, 공감을 많이 받은 순, relevance 순,호감 + vote의 크기 등) 이를 통해 다양한 의견이 쉽게 노출될 수 있는환경을 조성하였다는 점에서는 좀 더 높은 점수를 주고 싶다.</p><p>그래서 세 번째 글은, 간단하지만 다른 정렬 기준을 적용했을 때 발견되는새로운 댓글에 대해서 다뤄 볼 예정이다. 프로젝트 초창기에, 같이 작업을진행했던 재명님이 돌려본 결과가 있는데 이 것도 조금 다듬어서 올리면재밌을 것 같다! 3탄은... 휴가(<span class="citation" data-cites="Norway">@Norway</span>) 다녀오고 나서 작업해볼까 싶다. To becontinued...</p><h2 id="appendix-a-gaussian-mixture-model-fitting">Appendix A: GaussianMixture Model Fitting</h2><p>GMM의 <code>n_components</code> 최적 개수를 구하기 위해<code>silhouette score</code>를 계산하였다.</p><p><img src="/assets/images/silhouette_score.png?style=centerme" width="70%"><br></p><p><code>score</code>가 가장 높은 <code>n_components=2</code> 이므로2개의 gaussian을 가정하여 fitting 해보면 아래 그림과 같다.</p><p><img src="/assets/images/gmm.png?style=centerme" width="80%" alt="cluster 1: blue / cluster 2: orange"><br></p><table><thead><tr class="header"><th>userId</th><th>cluster 1 prob</th><th>cluster 2 prob</th><th>ratio (%)</th></tr></thead><tbody><tr class="odd"><td>user 1</td><td>2.27E-16</td><td>1</td><td>95.60</td></tr><tr class="even"><td>user 3</td><td>2.56E-15</td><td>1</td><td>93.08</td></tr><tr class="odd"><td>user 2</td><td>9.31E-14</td><td>1</td><td>89.21</td></tr><tr class="even"><td>user 39</td><td>4.96E-11</td><td>1</td><td>82.00</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>user 6</td><td>8.11E-01</td><td>0.188898</td><td>41.27</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>user 9</td><td>1.00E+00</td><td>0.000219</td><td>15.05</td></tr></tbody></table><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2016%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD" rel="external nofollow noopener noreferrer" target="_blank">https://ko.wikipedia.org/wiki/2016년_대한민국</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2017%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD" rel="external nofollow noopener noreferrer" target="_blank">https://ko.wikipedia.org/wiki/2017년_대한민국</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2018%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD" rel="external nofollow noopener noreferrer" target="_blank">https://ko.wikipedia.org/wiki/2018년_대한민국</a><a href="#fnref:3" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;NOTICE: 앞으로 소개될 내용은 NAVER와
무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은
중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을
알립니다.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를
확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번
이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게
영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.&lt;/li&gt;
&lt;li&gt;발생하기 어려운 패턴을 보여야 한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서
369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고
269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다.
    
    </summary>
    
    
      <category term="ML" scheme="https://inmoonlight.github.io/categories/ML/"/>
    
      <category term="Data Analysis" scheme="https://inmoonlight.github.io/categories/ML/Data-Analysis/"/>
    
    
      <category term="data analysis" scheme="https://inmoonlight.github.io/tags/data-analysis/"/>
    
      <category term="news comments" scheme="https://inmoonlight.github.io/tags/news-comments/"/>
    
      <category term="social good" scheme="https://inmoonlight.github.io/tags/social-good/"/>
    
  </entry>
  
</feed>
