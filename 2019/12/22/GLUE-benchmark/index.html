<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>General Language Understanding Evaluation (GLUE) benchmark - Space Moon</title><meta description="General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, 전자는 설명이 상세하고 후자는 요약되어"><meta property="og:type" content="blog"><meta property="og:title" content="General Language Understanding Evaluation (GLUE) benchmark"><meta property="og:url" content="https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/"><meta property="og:site_name" content="Space Moon"><meta property="og:description" content="General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, 전자는 설명이 상세하고 후자는 요약되어"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://inmoonlight.github.io/assets/images/glue.png"><meta property="article:published_time" content="2019-12-22T13:22:00.000Z"><meta property="article:modified_time" content="2021-12-18T15:25:01.951Z"><meta property="article:author" content="Jihyung Moon"><meta property="article:tag" content="ML"><meta property="article:tag" content="NLP"><meta property="article:tag" content="dataset"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/assets/images/glue.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/"},"headline":"Space Moon","image":["https://inmoonlight.github.io/assets/images/glue.png"],"datePublished":"2019-12-22T13:22:00.000Z","dateModified":"2021-12-18T15:25:01.951Z","author":{"@type":"Person","name":"Jihyung Moon"},"description":"General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, 전자는 설명이 상세하고 후자는 요약되어"}</script><link rel="canonical" href="https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131297969-1" async></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131297969-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="alternate" href="/feed.xml" title="Space Moon" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/inmoonlight"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="/assets/images/glue.png" alt="General Language Understanding Evaluation (GLUE) benchmark"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2019-12-22T13:22:00.000Z" title="2019-12-22T13:22:00.000Z">2019-12-22</time><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a><span> / </span><a class="link-muted" href="/categories/ML/NLP/">NLP</a></span><span class="level-item">21 minutes read (About 3099 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">General Language Understanding Evaluation (GLUE) benchmark</h1><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>General Language Understanding Evaluation benchmark, 줄여서 GLUE benchmark 라고 불리는 이 데이터셋은 NLP 분야에서 Language Model 검증을 위해 사용된다. ICLR 2019와 BlackboxNLP workshop 2018에 모두 publish 되었으며, <a href="https://openreview.net/pdf?id=rJ4km2R5t7" rel="external nofollow noopener noreferrer" target="_blank">전자</a>는 설명이 상세하고 <a href="https://www.aclweb.org/anthology/W18-5446.pdf" rel="external nofollow noopener noreferrer" target="_blank">후자</a>는 요약되어 있다. 이 글은 가장 최근(2019.2.22)에 업데이트된 <a href="https://arxiv.org/pdf/1804.07461.pdf" rel="external nofollow noopener noreferrer" target="_blank">arXiv에 있는 논문</a>을 기반으로 작성되었다.</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><a id="more"></a>
<h2 id="glue-overall">GLUE overall</h2>
<p>GLUE는 총 9개의 task로 구성되었으며 각 task는 언어의 특정한 성질을 평가하기 위한 목적으로 만들어졌고, 최종 점수는 각 task 별 점수의 평균 값을 가져간다. task는 크게 3가지 - Single-Sentence Tasks (CoLA, SST-2), Similarity and Paraphrase Tasks (MRPC, QQP, STS-B), Inference Tasks (MNLI, RTE, QNLI, WNLI) - 로 구분할 수 있다. 세부 task에 대해 살펴보기 전에 전반적인 task의 특징을 아래의 표에 정리했다. 원 논문에 정리되어 있는 것을 바탕으로 재구성하였고 직접 다운로드 받은 데이터를 기준으로 측정했기 때문에 corpus의 size가 다를 수 있다.</p>
<table>
<colgroup>
<col style="width: 37%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 2%">
<col style="width: 4%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>data</th>
<th>train</th>
<th>dev</th>
<th>test</th>
<th>domain</th>
<th>input</th>
<th>task</th>
<th>metrics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="#corpus-of-linguistic-acceptability-cola1">Corpus of Linguistic Acceptability (CoLA)</a></td>
<td>8.5k</td>
<td>1.0k</td>
<td>1.2k</td>
<td>linguistics literature</td>
<td>single-sentence</td>
<td>- grammatical acceptability <br> - binary classification <br> (grammatical / ungrammatical)</td>
<td>Matthews correlation</td>
</tr>
<tr class="even">
<td><a href="#stanford-sentiment-treebank-sst-2">Stanford Sentiment Treebank (SST-2)</a></td>
<td>67k</td>
<td>872</td>
<td>1.8k</td>
<td>movie reviews</td>
<td>single-sentence</td>
<td>- sentiment <br> - binary classification <br> (positive / negative)</td>
<td>acc.</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><a href="#microsoft-research-paraphrase-corpus-mrpc">Microsoft Research Paraphrase Corpus (MRPC)</a></td>
<td>3.7k</td>
<td>408</td>
<td>1.7k</td>
<td>news</td>
<td>two sentences</td>
<td>paraphrase</td>
<td>acc./F1</td>
</tr>
<tr class="odd">
<td><a href="#quora-question-pairs-qqp2">Quora Question Pairs (QQP)</a></td>
<td>364k</td>
<td>40k</td>
<td>391k</td>
<td>social QA questions</td>
<td>two sentences</td>
<td>paraphrase</td>
<td>acc./F1</td>
</tr>
<tr class="even">
<td><a href="#semantic-textual-similarity-benchmark-sts-b">Semantic Textual Similarity Benchmark (STS-B)</a></td>
<td>5.8k</td>
<td>1.5k</td>
<td>1.4k</td>
<td>news <br> caption <br> forum</td>
<td>two sentences</td>
<td>- sentence similarity <br> - regression</td>
<td>Pearson / Spearman corr.</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><a href="#multi-genre-nli-corpus-mnli">Multi-Genre NLI corpus (MNLI)</a></td>
<td>393k</td>
<td>20k</td>
<td>20k</td>
<td>fiction <br> face-to-face <br> government <br> letters <br> 9/11 <br> oxford university press (oup) <br> slate<br> telephone <br> travel <br> verbatim</td>
<td>two sentences</td>
<td>ternary classification <br> (entailment / contradiction / neutral)</td>
<td>matched acc. / mismatched acc.</td>
</tr>
<tr class="odd">
<td><a href="#the-recognizing-textual-entailment-rte">The Recognizing Textual Entailment (RTE)</a></td>
<td>2.5k</td>
<td>276</td>
<td>3.0k</td>
<td>news <br> wikipedia</td>
<td>two sentences</td>
<td>binary classification <br> (entailment / not_entailment)</td>
<td>acc.</td>
</tr>
<tr class="even">
<td><a href="#the-stanford-question-answering-nli-qnli">The Stanford Question Answering NLI (QNLI)</a></td>
<td>105k</td>
<td>5.5k</td>
<td>5.5k</td>
<td>wikipedia</td>
<td>two sentences (question, sentence)</td>
<td>binary classification <br> (entailment / not_entailment)</td>
<td>acc.</td>
</tr>
<tr class="odd">
<td><a href="#the-winograd-schema-challenge-nli-wnli">The Winograd Schema Challenge NLI (WNLI)</a></td>
<td>634</td>
<td>71</td>
<td>146</td>
<td>fiction books</td>
<td>two sentences</td>
<td>binary classification <br> (entailment / not_entailment)</td>
<td>acc.</td>
</tr>
</tbody>
</table>
<h2 id="corpus-of-linguistic-acceptability-cola1">Corpus of Linguistic Acceptability (CoLA)<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://arxiv.org/pdf/1805.12471.pdf&gt;
">[1]</span></a></sup></h2>
<p>CoLA는 공개된 언어학 문헌(publised liguistics literature)에서 추출된 약 21k 문장들로 구성되어 있다. 이 문장들은 문법적으로 옳은지, 그른지가 표기되어 있다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 They drank the pub dry.</span><br><span class="line">0 * They drank the pub.</span><br></pre></td></tr></table></figure>
<p>문법적으로 옳고 그름을 판단하는 기준은 다양하다. 아래의 표는 corpus를 제작하면서 기준에서 포함된 것들과 제외된 것들을 나타낸다.</p>
<p><img src="/assets/images/cola_dataset_description.png?style=centerme" width="80%"></p>
<h3 id="included">Included</h3>
<p><strong>(a) Morphological Violation</strong>: "should leave" 가 올바른 표현이지만 "should leaving"으로 작성되었다. 동사의 형태(verbal inflection)가 맞지 않는 경우에 해당한다. <strong>(b) Syntactic Violation</strong>: "What did Bill buy?" 혹은 "Bill bought potatoes and _" 이 되어야 한다. 통사 구조가 틀린 경우에 해당한다. <strong>(c) Semantic Violation</strong>: 의미적으로 말이 되지 않는 문장에 해당한다.</p>
<h3 id="excluded">Excluded</h3>
<p><strong>(d) Pragmatic Anomalies</strong>: grammar와 상관없는 외부 지식이 필요하므로 제외되었다. <strong>(e) Unavailable Meanings</strong>: 문장만보고는 판단이 애매하므로 제외되었다. <strong>(f) Prescriptive Rules</strong>: 사람도 누군가의 가르침없이는 터득하기 어려운 rule이기 때문에 제외되었다. <strong>(g) Nonce Words</strong>: "arrivable"과 같이 typical word-level NLP 모델의 vocab에는 등장하지 않는 단어가 포함된 경우이다. NLP 모델의 scope이 아니라고 판단되어 제외되었다.</p>
<h3 id="testset-and-metrics">testset and metrics</h3>
<p>testset은 In-Domain과 Out-of-Domain으로 구성되어 있다. In-Domain은 training set이 추출된 source와 같은 source에서, Out-of-Domain은 training set이 추출되지 <em>않은</em> source에서 구성되었다. GLUE는 원래 구분된 두 testset을 하나로 합쳐 단일 testset을 구축하였고 총 1,160 문장이다.</p>
<p><img src="/assets/images/cola_by_source.png?style=centerme" width="40%"></p>
<p>이 task의 평가는 unbalanced binary classification task에서 사용되는 Matthews correlation으로 한다.</p>
<h2 id="stanford-sentiment-treebank-sst-2">Stanford Sentiment Treebank (SST-2)</h2>
<p><code>rottentomatoes.com</code>의 영화 리뷰 corpus로 구성되었으며 AMT(Amazon Mechanical Turk)를 통해 리뷰의 sentiment가 labeling 되었다. 1은 긍정, 0은 부정을 나타낸다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">that loves its characters and communicates something rather beautiful about human nature 1</span><br><span class="line">on the worst revenge-of-the-nerds clichés the filmmakers could dredge up 0</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-1">testset and metrics</h3>
<p>일반적인 binary classification 문제로 accuracy를 통해 평가한다.</p>
<h2 id="microsoft-research-paraphrase-corpus-mrpc">Microsoft Research Paraphrase Corpus (MRPC)</h2>
<p>MRPC는 온라인 뉴스에서 추출된 문장들로 구성되었으며 2개의 문장이 의미적으로 같은지 다른지를 평가하는 task이다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . </span><br><span class="line">On June 10 , the ship &#39;s owners had published an advertisement on the Internet , offering the explosives for sale .</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">Yucaipa owned Dominick &#39;s before selling the chain to Safeway in 1998 for $ 2.5 billion . </span><br><span class="line">Yucaipa bought Dominick &#39;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-2">testset and metrics</h3>
<p>testset이 label이 불균등(68% positive, 32% negative)하므로 accuracy와 F1 score를 metric으로 한다.</p>
<h2 id="quora-question-pairs-qqp2">Quora Question Pairs (QQP)<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs&gt;
">[2]</span></a></sup></h2>
<p>QQP는 <code>https://www.quora.com/</code>의 질문들로 구성되었으며, 두 개의 질문이 의미상 같은지 다른지가 표기되어있다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How do you start a bakery?</span><br><span class="line">How can one start bakery business?</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">What are natural numbers?</span><br><span class="line">What is a least natural number?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-3">testset and metrics</h3>
<p>MRPC와 마찬가지로 불균등(37% positive, 63% negative)하므로 accuracy와 F1 score가 metric으로 활용된다.</p>
<h2 id="semantic-textual-similarity-benchmark-sts-b">Semantic Textual Similarity Benchmark (STS-B)</h2>
<p>문장의 유사도는 번역, 요약, 문장 생성, QA, 대화 모델링 등등 다양한 NLP 분야에서 중요하게 다뤄진다. STS shared task는 모델이 문장들의 유사도를 얼마나 잘 파악하는지를 평가하기 위해 등장하였고, 2012년부터 2017년까지 매년 개최되었으며 그 때마다 다른 dataset이 사용되었다. 이 때문에 각 연도의 데이터셋을 적절히 조합한 common evaluation set으로 STS-B가 소개되었다.</p>
<p>이 전의 task와는 다르게 STS는 regression task이다. human annotator들은 두 문장의 의미적인 유사도를 1~5점으로 평가하였고 모델은 score를 예측해야한다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A plane is taking off.  An air plane is taking off.     5.000</span><br><span class="line">Three men are playing chess.    Two men are playing chess.      2.600</span><br><span class="line">A man is smoking.       A man is skating.       0.500</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-4">testset and metrics</h3>
<p>Regression task이므로 human label과의 Pearson correlation으로 평가된다.</p>
<h2 id="multi-genre-nli-corpus-mnli">Multi-Genre NLI corpus (MNLI)</h2>
<p>MNLI<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="&lt;https://www.aclweb.org/anthology/N18-1101.pdf&gt;">[3]</span></a></sup>는 SNLI(Stanford NLI) dataset의 단점을 개선시킨 데이터셋이다. SNLI는 image caption으로만 구성되었기 때문에 장면을 표현하는 짧고 간단한 문장이 많고 NLU(Natural Language Understanding) task와 무관한 단어들이 많이 등장한다. 그래서 NLU task의 benchmark로 사용되기는 어렵기 때문에 다양한 도메인(논문에서는 genre라고 표현)의 조합인 MNLI benchmark dataset이 등장하였다.</p>
<p><img src="/assets/images/mnli_dataset_description.png?style=centerme" width="80%"></p>
<p>위의 표에서 나와있듯이 MNLI는 총 10개의 Genre로 구성되었다. Fiction을 제외한 9개의 domain은 Open American National Corpus에서 추출되었고 Fiction은 fiction literature에서 가져왔으며 mystery, humor, sci-fi 등 그 안에서도 다양한 장르로 구성되었다.</p>
<blockquote>
<p>OANC data constitutes the following nine genres: transcriptions from the Charlotte Narrative and Conversation Collection of two-sided, in-person conversations that took place in the early 2000s (FACE-TO-FACE); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT); letters from the Indiana Center for Intercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s–early 2000s (LETTERS); the public report from the National Commission on Terrorist Attacks Upon the United States released on July 22, 2004 2 (9/11); five non-fiction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE) written between 1996–2000; transcriptions from University of Pennsylvania’s Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM).</p>
</blockquote>
<blockquote>
<p>For our tenth genre, FICTION, we compile several freely available works of contemporary fiction written between 1912 and 2010, spanning various genres, including mystery (The Mysterious Affair at Styles, 3 Christie, 1921; The Secret Adversary, 4 Christie, 1922; Murder in the Gun Room, 5 Piper, 1953), humor (Password Incorrect, 6 Name, 2008), western (Rebel Spurs, 7 Norton, 1962), science fiction (Seven Swords, 8 Shea, 2008; Living History,9 Essex, 2016; The Sky Is Falling, 10 Del Rey, 1973; Youth, 11 Asimov, May 1952), and adventure (Captain Blood, 12 Sabatini, 1922).</p>
</blockquote>
<p>선별된 문장을 premise로 두고 human annotator들이 premise와 같은 결론을 도출하는 문장(entailment), 반대되는 문장(contradiction), 두 경우 모두 아닌 문장(neutral)을 생성하고 label을 단다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How do you know? All this is their information again.   </span><br><span class="line">This information belongs to them.      </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">Vrenna and I both fought him and he nearly took us.     </span><br><span class="line">Neither Vrenna nor myself have ever fought him.      </span><br><span class="line">contradiction</span><br><span class="line"></span><br><span class="line">There was nothing like that emotion now.        </span><br><span class="line">There are few emotions that come close.      </span><br><span class="line">neutral</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-5">testset and metrics</h3>
<p>testset은 CoLA처럼 matched(in-domain)와 mismatched(cross-domain)로 구성되었다. mismatched에는 9/11, FACE-TO-FACE, LETTERS, OUP, VERBATIM처럼 training set에는 없는 domain이 포함되어 있다. (위의 표 참고) 각각의 경우를 나누어서 accuracy로 평가한다.</p>
<h2 id="the-recognizing-textual-entailment-rte">The Recognizing Textual Entailment (RTE)</h2>
<p>RTE도 STS처럼 RTE1부터 RTE7까지의 데이터셋에서 만들어졌다. 구체적으로는 RTE1, RTE2, RTE3, RTE5로 구성되었고, 나머지 데이터셋 중 RTE4는 공개되지 않아서, RTE6와 7은 NLI task로는 부적합해서 제외했다고 한다. 취합하는 과정에서 일부는 세 개의 class, 일부는 두 개의 class로 labeling이 되어있어 이를 일괄적으로 두 개의 class(entailment, not_entailment)로 구분지었다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Swansea striker Lee Trundle has negotiated a lucrative image-rights deal with the League One club.      </span><br><span class="line">Lee Trundle is in business with the League One club.    </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">No Weapons of Mass Destruction Found in Iraq Yet.       </span><br><span class="line">Weapons of Mass Destruction Found in Iraq.      </span><br><span class="line">not_entailment</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-6">testset and metrics</h3>
<p>일반적인 binary classification 문제이므로 accuracy로 측정한다.</p>
<h2 id="the-stanford-question-answering-nli-qnli">The Stanford Question Answering NLI (QNLI)</h2>
<p>Stanford에서 구축한 Machine Comprehension 목적의 QA Dataset, a.k.a SQuAD,을 NLI task에 맞게 변형한 데이터셋이다. SQuAD는 wikipedia에서 paragraph를 가져와서 annotator들이 적절한 질문을 던지는데 이에 대한 답을 paragraph 내에 있는 문장, 구, 단어로 답할 수 있게 구성되었다. QNLI는 질문과 paragraph 내의 한 문장을 비교하여 이 둘이 entailment되었는지 아닌지를 판단하도록 바뀌었다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">What two things does Popper argue Tarski&#39;s theory involves in an evaluation of truth?   </span><br><span class="line">He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.   </span><br><span class="line">entailment</span><br><span class="line"></span><br><span class="line">Who was elected as the Watch Tower Society&#39;s president in January of 1917?      </span><br><span class="line">His election was disputed, and members of the Board of Directors accused him of acting in an autocratic and secretive manner.       </span><br><span class="line">not_entailment</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-7">testset and metrics</h3>
<p>일반적인 binary classification 문제이므로 accuracy로 측정한다.</p>
<h3 id="the-winograd-schema-challenge-nli-wnli">The Winograd Schema Challenge NLI (WNLI)</h3>
<p>이 데이터셋도 entailment를 평가하는 목적으로 만들어졌다. original sentence와 이 문장에서 대명사를 일반명사로 치환한 문장 사이의 entailment가 있는지 없는지가 label로 달려있다. 아래 예시의 첫 번째 문장에서 "it" had a hole의 it이 "The carrot"으로 바뀐 문장이 두 번째 문장이고 이 두 문장의 관계가 entailment 되어 있으므로 label 1이 달린다.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">I stuck a pin through a carrot. When I pulled the pin out, it had a hole.       </span><br><span class="line">The carrot had a hole.  </span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">John was jogging through the park when he saw a man juggling watermelons. He was very impressive.       </span><br><span class="line">John was very impressive.       </span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<h3 id="testset-and-metrics-8">testset and metrics</h3>
<p><a href="https://gluebenchmark.com/faq" rel="external nofollow noopener noreferrer" target="_blank">GLUE FAQ</a>의 12번 문항에는 WNLI에서 이상한 결과를 얻을 수 있는 이유가 적혀있다. 같은 문장이 포함된 다른 example 끼리는 반대의 label이 달려있는데 이 때문에 training set에 overfit된 모델은 dev set에서 성능이 매우 나쁠 수 있다는 것이다. 실제로 <a href="https://arxiv.org/pdf/1810.04805.pdf" rel="external nofollow noopener noreferrer" target="_blank">BERT</a>는 이 이유로 WNLI의 성능은 report 하지 않았다.</p>
<h2 id="download">Download</h2>
<p><a href="https://github.com/nyu-mll/jiant/blob/master/scripts/download_glue_data.py" rel="external nofollow noopener noreferrer" target="_blank">링크</a>에 있는 python script를 다운로드한 이후 실행시키면 된다. 지정한 dir에 전체 task를 받을 수도 있고 일부 task만 받을 수도 있다.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python download_glue_data.py --data_dir data --tasks all</span><br></pre></td></tr></table></figure>
<h2 id="leaderboard">Leaderboard</h2>
<p>여태까지 제출한 모델의 성능은 <a href="https://gluebenchmark.com/leaderboard" rel="external nofollow noopener noreferrer" target="_blank">GLUE leaderboard</a>에 정리되어있다.</p>
<p><img src="/assets/images/glue_leaderboard.png" alt="2019.12.22 기준 top 3"></p>
<p>Leaderboard에는 순기능과 역기능이 모두 공존하지만, 아직까지는 순기능이 더 많다고 생각한다. 상대적으로 공정하게 비교할 수 있는 데이터셋이고 덕분에 다양한 Language Model이 주목받을 수 있었기 때문이다. 너무 낡아버리기 전에 새로운 데이터셋이 나와야한다고도 생각했는데, Neurips 2019에 "Stickier Benchmark"라는 부제와 함께 <a href="http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems" rel="external nofollow noopener noreferrer" target="_blank">SuperGLUE</a>가 등장했다!!</p>
<p>이로 인해 열릴 새로운 LM들의 등장을 기대해본다 :)</p>
<h2 id="references">References</h2>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1805.12471.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://arxiv.org/pdf/1805.12471.pdf</a><a href="#fnref:1" rev="footnote"> ↩︎</a></span>
</li>
<li id="fn:2">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs" rel="external nofollow noopener noreferrer" target="_blank">https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs</a><a href="#fnref:2" rev="footnote"> ↩︎</a></span>
</li>
<li id="fn:3">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.aclweb.org/anthology/N18-1101.pdf" rel="external nofollow noopener noreferrer" target="_blank">https://www.aclweb.org/anthology/N18-1101.pdf</a><a href="#fnref:3" rev="footnote"> ↩︎</a></span>
</li>
</ol>
</div>
</div>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ML/">ML</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP/">NLP</a><a class="link-muted mr-2" rel="tag" href="/tags/dataset/">dataset</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/01/07/The-past-of-the-light-by-Eun-hee-kyung/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">빛의 과거</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/11/07/Basics-of-Quantum-Computings/"><span class="level-item">수학으로 이해하는 양자컴퓨터의 기초</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://inmoonlight.github.io/2019/12/22/GLUE-benchmark/';
            this.page.identifier = '2019/12/22/GLUE-benchmark/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'inmoonlight' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#glue-overall"><span class="mr-2">1</span><span>GLUE overall</span></a></li><li><a class="is-flex" href="#corpus-of-linguistic-acceptability-cola1"><span class="mr-2">2</span><span>Corpus of Linguistic Acceptability (CoLA)[1]</span></a><ul class="menu-list"><li><a class="is-flex" href="#included"><span class="mr-2">2.1</span><span>Included</span></a></li><li><a class="is-flex" href="#excluded"><span class="mr-2">2.2</span><span>Excluded</span></a></li><li><a class="is-flex" href="#testset-and-metrics"><span class="mr-2">2.3</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#stanford-sentiment-treebank-sst-2"><span class="mr-2">3</span><span>Stanford Sentiment Treebank (SST-2)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-1"><span class="mr-2">3.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#microsoft-research-paraphrase-corpus-mrpc"><span class="mr-2">4</span><span>Microsoft Research Paraphrase Corpus (MRPC)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-2"><span class="mr-2">4.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#quora-question-pairs-qqp2"><span class="mr-2">5</span><span>Quora Question Pairs (QQP)[2]</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-3"><span class="mr-2">5.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#semantic-textual-similarity-benchmark-sts-b"><span class="mr-2">6</span><span>Semantic Textual Similarity Benchmark (STS-B)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-4"><span class="mr-2">6.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#multi-genre-nli-corpus-mnli"><span class="mr-2">7</span><span>Multi-Genre NLI corpus (MNLI)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-5"><span class="mr-2">7.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#the-recognizing-textual-entailment-rte"><span class="mr-2">8</span><span>The Recognizing Textual Entailment (RTE)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-6"><span class="mr-2">8.1</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#the-stanford-question-answering-nli-qnli"><span class="mr-2">9</span><span>The Stanford Question Answering NLI (QNLI)</span></a><ul class="menu-list"><li><a class="is-flex" href="#testset-and-metrics-7"><span class="mr-2">9.1</span><span>testset and metrics</span></a></li><li><a class="is-flex" href="#the-winograd-schema-challenge-nli-wnli"><span class="mr-2">9.2</span><span>The Winograd Schema Challenge NLI (WNLI)</span></a></li><li><a class="is-flex" href="#testset-and-metrics-8"><span class="mr-2">9.3</span><span>testset and metrics</span></a></li></ul></li><li><a class="is-flex" href="#download"><span class="mr-2">10</span><span>Download</span></a></li><li><a class="is-flex" href="#leaderboard"><span class="mr-2">11</span><span>Leaderboard</span></a></li><li><a class="is-flex" href="#references"><span class="mr-2">12</span><span>References</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Book/"><span class="level-start"><span class="level-item">Book</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Essay/"><span class="level-start"><span class="level-item">Essay</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">12</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/ML/Data-Analysis/"><span class="level-start"><span class="level-item">Data Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/PyTorch/"><span class="level-start"><span class="level-item">PyTorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ML/Quantum-Computing/"><span class="level-start"><span class="level-item">Quantum Computing</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Ops/"><span class="level-start"><span class="level-item">Ops</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Ops/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time datetime="2021-07-10T16:03:00.000Z">2021-07-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/11/Git-merge-strategy/">Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Ops/">Ops</a> / <a class="link-muted" href="/categories/Ops/Git/">Git</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-03-02T19:22:00.000Z">2021-03-03</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/03/PyTorch-view-transpose-reshape/">PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-21T14:22:00.000Z">2021-02-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/21/PyTorch-IterableDataset/">PyTorch의 IterableDataset을 사용해서 데이터 불러오기</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a> / <a class="link-muted" href="/categories/ML/PyTorch/">PyTorch</a></p></div></article><article class="media"><div class="media-content size-small"><p><time datetime="2021-02-04T05:22:00.000Z">2021-02-04</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/04/Pandas-Dataframe-iterations/">Pandas Dataframe의 다양한 iteration 방법 비교</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ML/">ML</a></p></div></article><article class="media"><a class="media-left" href="/2021/01/10/Adieu-2020-and-happy-new-year/"><p class="image is-64x64"><img class="thumbnail" src="/assets/images/2020-2021.jpg" alt="2020년을 정리하고, 2021년을 맞이하는 글"></p></a><div class="media-content size-small"><p><time datetime="2021-01-10T12:13:00.000Z">2021-01-10</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/10/Adieu-2020-and-happy-new-year/">2020년을 정리하고, 2021년을 맞이하는 글</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Essay/">Essay</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/space_moon.png" alt="Space Moon" height="28"></a><p class="size-small"><span>&copy; 2022 Jihyung Moon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://inmoonlight.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>